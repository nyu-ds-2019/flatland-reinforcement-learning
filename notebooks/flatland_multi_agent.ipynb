{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flatland-multi-agent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG76_GpYymMP",
        "outputId": "72f2b67e-1b0b-4c98-841c-5ba3313562d7"
      },
      "source": [
        "!pip install flatland-rl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flatland-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/30/e002f8b7d9075c88f2f00e870294e7896c92db8b3c94ae9c442ca0e42bc2/flatland-rl-2.2.2.tar.gz (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 5.6MB/s \n",
            "\u001b[?25hCollecting tox>=3.5.2\n",
            "  Using cached https://files.pythonhosted.org/packages/e0/79/5915b9dad867e89bb6495456acfe5d4e2287e74dfa29c059f7b127d5480e/tox-3.20.1-py2.py3-none-any.whl\n",
            "Collecting pytest<5,>=3.8.2\n",
            "  Using cached https://files.pythonhosted.org/packages/70/c7/e8cb4a537ee4fc497ac80a606a667fd1832f28ad3ddbfa25bf30473eae13/pytest-4.6.11-py2.py3-none-any.whl\n",
            "Collecting pytest-runner>=4.2\n",
            "  Using cached https://files.pythonhosted.org/packages/16/45/81b5262c0efc08882bdf183b788e6d28e3d684863990996d8b60967d48da/pytest_runner-5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (7.1.2)\n",
            "Collecting crowdai-api>=0.1.21\n",
            "  Using cached https://files.pythonhosted.org/packages/0c/ee/55912b05af8994a190280e3281a18720f8d69da02dcb7ff44e1b96974345/crowdai_api-0.1.22.tar.gz\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.18.5)\n",
            "Collecting recordtype>=1.3\n",
            "  Using cached https://files.pythonhosted.org/packages/60/9a/835ba329e31aa471a5597c733f7ca0136b3a0622ce01b9e66b40f5909da4/recordtype-1.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (3.2.2)\n",
            "Requirement already satisfied: Pillow>=5.4.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (7.0.0)\n",
            "Collecting msgpack==0.6.1\n",
            "  Using cached https://files.pythonhosted.org/packages/92/7e/ae9e91c1bb8d846efafd1f353476e3fd7309778b582d2fb4cea4cc15b9a2/msgpack-0.6.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting msgpack-numpy>=0.4.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/19/05/05b8d7c69c6abb36a34325cc3150089bdafc359f0a81fb998d93c5d5c737/msgpack_numpy-0.4.7.1-py2.py3-none-any.whl\n",
            "Collecting svgutils>=0.3.1\n",
            "  Using cached https://files.pythonhosted.org/packages/79/da/4f7a31a55c247e304a338716e75d761f3dc9b50b220fcfaad7398668367e/svgutils-0.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyarrow>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.14.1)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.1.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.17 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (2.0.0)\n",
            "Collecting importlib-resources<2,>=1.0.1\n",
            "  Using cached https://files.pythonhosted.org/packages/7f/2d/88f166bcaadc09d9fdbf1c336ad118e01b7fe1155e15675e125be2ff1899/importlib_resources-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.15.0)\n",
            "Collecting timeout-decorator>=0.4.1\n",
            "  Using cached https://files.pythonhosted.org/packages/80/f8/0802dd14c58b5d3d72bb9caa4315535f58787a1dc50b81bbbcaaa15451be/timeout-decorator-0.5.0.tar.gz\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (20.3.0)\n",
            "Collecting gym==0.14.0\n",
            "  Using cached https://files.pythonhosted.org/packages/61/75/9e841bc2bc75128e0b65c3d5255d0bd16becb9d8f7120b965d41b8e70041/gym-0.14.0.tar.gz\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (2.5)\n",
            "Collecting ipycanvas\n",
            "  Using cached https://files.pythonhosted.org/packages/8d/ec/87e50e2d47b45a7355796f3eca1e1120eee91613e4e0753fb3719035ffe2/ipycanvas-0.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.10.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (2.4.1)\n",
            "Requirement already satisfied: packaging>=14 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (20.4)\n",
            "Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (0.10.2)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (3.0.12)\n",
            "Collecting pluggy>=0.12.0\n",
            "  Using cached https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Collecting virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/79/88/66ac964ab8cf87c8db839c11812292a966825af205411cb67477cb4e73d3/virtualenv-20.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py>=1.4.17 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (1.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (0.2.5)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0; python_version > \"2.7\" in /usr/local/lib/python3.6/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (8.6.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from crowdai-api>=0.1.21->flatland-rl) (2.23.0)\n",
            "Collecting python-gitlab>=1.3.0\n",
            "  Using cached https://files.pythonhosted.org/packages/b0/5f/2ecfd792eced1c43576bfd49dd96ff872775b985d2759bb8e68cf1f26365/python_gitlab-2.5.0-py3-none-any.whl\n",
            "Collecting redis\n",
            "  Using cached https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (2.4.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from svgutils>=0.3.1->flatland-rl) (4.2.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.1->flatland-rl) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.17->flatland-rl) (3.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.14.0->flatland-rl) (1.4.1)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl\n",
            "Collecting cloudpickle~=1.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->flatland-rl) (4.4.2)\n",
            "Requirement already satisfied: ipywidgets>=7.5.0 in /usr/local/lib/python3.6/dist-packages (from ipycanvas->flatland-rl) (7.5.1)\n",
            "Collecting orjson\n",
            "  Using cached https://files.pythonhosted.org/packages/92/46/de838c1a3eec0852007749aa3e3a922df8b3e9a37d2d7c849e5dbc463977/orjson-3.4.4-cp36-cp36m-manylinux2014_x86_64.whl\n",
            "Collecting appdirs<2,>=1.4.3\n",
            "  Using cached https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Using cached https://files.pythonhosted.org/packages/f5/0a/490fa011d699bb5a5f3a0cf57de82237f52a6db9d40f33c53b2736c9a1f9/distlib-0.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.14.0->flatland-rl) (0.16.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.0.8)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (3.5.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.5.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.7.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.2.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.3.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.3.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (50.3.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.8.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.11.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (20.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.1.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (3.2.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.5.1)\n",
            "Building wheels for collected packages: flatland-rl, crowdai-api, timeout-decorator, gym\n",
            "  Building wheel for flatland-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flatland-rl: filename=flatland_rl-2.2.2-py2.py3-none-any.whl size=1690842 sha256=c84b8b2cf7a10c915c9909b8e4482bdc31a14fa2c10da48707c4eee4ef3cecf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/c0/b9/153b0e6135cd9a01d550b5e5735fb1c19fbde1ac39c83e18c0\n",
            "  Building wheel for crowdai-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crowdai-api: filename=crowdai_api-0.1.22-py2.py3-none-any.whl size=10001 sha256=6436c898353d52b000c9013cbeedb41f92c9bc60f795d98848045f7102da5218\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/a2/87/fc28f0db513219afc295b27e829a1f2d74cf2886d0f03aa4ef\n",
            "  Building wheel for timeout-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timeout-decorator: filename=timeout_decorator-0.5.0-cp36-none-any.whl size=5033 sha256=3e191898ed081206387c506b1ceaff0dcdc358e59e382aa949f21fde9ef66c4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/ea/e7/64dd533737dd2b14a718a061e3a0e0baa2ce0ad50b519514ea\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.14.0-cp36-none-any.whl size=1637527 sha256=db8ed80a8ddfcde96d40aff978c7f216a4cafec2def3c5a1fad062b4fcd84a3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/53/f6/c0cd3c9bf953f35c0aee7fa62ea209371e92f5e5cced3245ba\n",
            "Successfully built flatland-rl crowdai-api timeout-decorator gym\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement cloudpickle==1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pluggy, appdirs, importlib-resources, distlib, virtualenv, tox, pytest, pytest-runner, python-gitlab, redis, crowdai-api, recordtype, msgpack, msgpack-numpy, svgutils, timeout-decorator, pyglet, cloudpickle, gym, orjson, ipycanvas, flatland-rl\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: importlib-resources 3.3.0\n",
            "    Uninstalling importlib-resources-3.3.0:\n",
            "      Successfully uninstalled importlib-resources-3.3.0\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Found existing installation: msgpack 1.0.0\n",
            "    Uninstalling msgpack-1.0.0:\n",
            "      Successfully uninstalled msgpack-1.0.0\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed appdirs-1.4.4 cloudpickle-1.2.2 crowdai-api-0.1.22 distlib-0.3.1 flatland-rl-2.2.2 gym-0.14.0 importlib-resources-1.5.0 ipycanvas-0.7.0 msgpack-0.6.1 msgpack-numpy-0.4.7.1 orjson-3.4.4 pluggy-0.13.1 pyglet-1.3.2 pytest-4.6.11 pytest-runner-5.2 python-gitlab-2.5.0 recordtype-1.3 redis-3.5.3 svgutils-0.3.1 timeout-decorator-0.5.0 tox-3.20.1 virtualenv-20.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gErAuZdgytFi"
      },
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import copy\n",
        "\n",
        "from argparse import ArgumentParser, Namespace\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from collections import namedtuple, deque, Iterable\n",
        "\n",
        "import psutil\n",
        "from flatland.utils.rendertools import RenderTool\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from flatland.envs.rail_env import RailEnv, RailEnvActions\n",
        "from flatland.envs.rail_generators import sparse_rail_generator\n",
        "from flatland.envs.schedule_generators import sparse_schedule_generator\n",
        "from flatland.envs.observations import TreeObsForRailEnv\n",
        "\n",
        "from flatland.envs.malfunction_generators import malfunction_from_params, MalfunctionParameters\n",
        "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n",
        "\n",
        "# base_dir = Path(__file__).resolve().parent.parent\n",
        "# sys.path.append(str(base_dir))\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2CR7QeHy29w"
      },
      "source": [
        "\n",
        "def max_lt(seq, val):\n",
        "    \"\"\"\n",
        "    Return greatest item in seq for which item < val applies.\n",
        "    None is returned if seq was empty or all items in seq were >= val.\n",
        "    \"\"\"\n",
        "    max = 0\n",
        "    idx = len(seq) - 1\n",
        "    while idx >= 0:\n",
        "        if seq[idx] < val and seq[idx] >= 0 and seq[idx] > max:\n",
        "            max = seq[idx]\n",
        "        idx -= 1\n",
        "    return max\n",
        "\n",
        "\n",
        "def min_gt(seq, val):\n",
        "    \"\"\"\n",
        "    Return smallest item in seq for which item > val applies.\n",
        "    None is returned if seq was empty or all items in seq were >= val.\n",
        "    \"\"\"\n",
        "    min = np.inf\n",
        "    idx = len(seq) - 1\n",
        "    while idx >= 0:\n",
        "        if seq[idx] >= val and seq[idx] < min:\n",
        "            min = seq[idx]\n",
        "        idx -= 1\n",
        "    return min\n",
        "\n",
        "\n",
        "def norm_obs_clip(obs, clip_min=-1, clip_max=1, fixed_radius=0, normalize_to_range=False):\n",
        "    \"\"\"\n",
        "    This function returns the difference between min and max value of an observation\n",
        "    :param obs: Observation that should be normalized\n",
        "    :param clip_min: min value where observation will be clipped\n",
        "    :param clip_max: max value where observation will be clipped\n",
        "    :return: returnes normalized and clipped observatoin\n",
        "    \"\"\"\n",
        "    if fixed_radius > 0:\n",
        "        max_obs = fixed_radius\n",
        "    else:\n",
        "        max_obs = max(1, max_lt(obs, 1000)) + 1\n",
        "\n",
        "    min_obs = 0  # min(max_obs, min_gt(obs, 0))\n",
        "    if normalize_to_range:\n",
        "        min_obs = min_gt(obs, 0)\n",
        "    if min_obs > max_obs:\n",
        "        min_obs = max_obs\n",
        "    if max_obs == min_obs:\n",
        "        return np.clip(np.array(obs) / max_obs, clip_min, clip_max)\n",
        "    norm = np.abs(max_obs - min_obs)\n",
        "    return np.clip((np.array(obs) - min_obs) / norm, clip_min, clip_max)\n",
        "\n",
        "\n",
        "def _split_node_into_feature_groups(node) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    data = np.zeros(6)\n",
        "    distance = np.zeros(1)\n",
        "    agent_data = np.zeros(4)\n",
        "\n",
        "    data[0] = node.dist_own_target_encountered\n",
        "    data[1] = node.dist_other_target_encountered\n",
        "    data[2] = node.dist_other_agent_encountered\n",
        "    data[3] = node.dist_potential_conflict\n",
        "    data[4] = node.dist_unusable_switch\n",
        "    data[5] = node.dist_to_next_branch\n",
        "\n",
        "    distance[0] = node.dist_min_to_target\n",
        "\n",
        "    agent_data[0] = node.num_agents_same_direction\n",
        "    agent_data[1] = node.num_agents_opposite_direction\n",
        "    agent_data[2] = node.num_agents_malfunctioning\n",
        "    agent_data[3] = node.speed_min_fractional\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def _split_subtree_into_feature_groups(node, current_tree_depth: int, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    if node == -np.inf:\n",
        "        remaining_depth = max_tree_depth - current_tree_depth\n",
        "        # reference: https://stackoverflow.com/questions/515214/total-number-of-nodes-in-a-tree-data-structure\n",
        "        num_remaining_nodes = int((4 ** (remaining_depth + 1) - 1) / (4 - 1))\n",
        "        return [-np.inf] * num_remaining_nodes * 6, [-np.inf] * num_remaining_nodes, [-np.inf] * num_remaining_nodes * 4\n",
        "\n",
        "    data, distance, agent_data = _split_node_into_feature_groups(node)\n",
        "\n",
        "    if not node.childs:\n",
        "        return data, distance, agent_data\n",
        "\n",
        "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
        "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(node.childs[direction], current_tree_depth + 1, max_tree_depth)\n",
        "        data = np.concatenate((data, sub_data))\n",
        "        distance = np.concatenate((distance, sub_distance))\n",
        "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def split_tree_into_feature_groups(tree, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    \"\"\"\n",
        "    This function splits the tree into three difference arrays of values\n",
        "    \"\"\"\n",
        "    data, distance, agent_data = _split_node_into_feature_groups(tree)\n",
        "\n",
        "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
        "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(tree.childs[direction], 1, max_tree_depth)\n",
        "        data = np.concatenate((data, sub_data))\n",
        "        distance = np.concatenate((distance, sub_distance))\n",
        "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def normalize_observation(observation, tree_depth: int, observation_radius=0):\n",
        "    \"\"\"\n",
        "    This function normalizes the observation used by the RL algorithm\n",
        "    \"\"\"\n",
        "    data, distance, agent_data = split_tree_into_feature_groups(observation, tree_depth)\n",
        "\n",
        "    data = norm_obs_clip(data, fixed_radius=observation_radius)\n",
        "    distance = norm_obs_clip(distance, normalize_to_range=True)\n",
        "    agent_data = np.clip(agent_data, -1, 1)\n",
        "    normalized_obs = np.concatenate((np.concatenate((data, distance)), agent_data))\n",
        "    return normalized_obs\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2bq4Yry_Ui"
      },
      "source": [
        "class DuelingQNetwork(nn.Module):\n",
        "    \"\"\"Dueling Q-network (https://arxiv.org/abs/1511.06581)\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, hidsize1=128, hidsize2=128):\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "\n",
        "        # value network\n",
        "        self.fc1_val = nn.Linear(state_size, hidsize1)\n",
        "        self.fc2_val = nn.Linear(hidsize1, hidsize2)\n",
        "        self.fc4_val = nn.Linear(hidsize2, 1)\n",
        "\n",
        "        # advantage network\n",
        "        self.fc1_adv = nn.Linear(state_size, hidsize1)\n",
        "        self.fc2_adv = nn.Linear(hidsize1, hidsize2)\n",
        "        self.fc4_adv = nn.Linear(hidsize2, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        val = F.relu(self.fc1_val(x))\n",
        "        val = F.relu(self.fc2_val(val))\n",
        "        val = self.fc4_val(val)\n",
        "\n",
        "        # advantage calculation\n",
        "        adv = F.relu(self.fc1_adv(x))\n",
        "        adv = F.relu(self.fc2_adv(adv))\n",
        "        adv = self.fc4_adv(adv)\n",
        "\n",
        "        return val + adv - adv.mean()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnrErJgVzCKv"
      },
      "source": [
        "class Policy:\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save(self, filename):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def load(self, filename):\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYmDw0N6zC5v"
      },
      "source": [
        "\n",
        "class DDDQNPolicy(Policy):\n",
        "    \"\"\"Dueling Double DQN policy\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, parameters, evaluation_mode=False):\n",
        "        self.evaluation_mode = evaluation_mode\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.double_dqn = True\n",
        "        self.hidsize = 1\n",
        "\n",
        "        if not evaluation_mode:\n",
        "            self.hidsize = parameters.hidden_size\n",
        "            self.buffer_size = parameters.buffer_size\n",
        "            self.batch_size = parameters.batch_size\n",
        "            self.update_every = parameters.update_every\n",
        "            self.learning_rate = parameters.learning_rate\n",
        "            self.tau = parameters.tau\n",
        "            self.gamma = parameters.gamma\n",
        "            self.buffer_min_size = parameters.buffer_min_size\n",
        "\n",
        "        # Device\n",
        "        if parameters.use_gpu and torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda:0\")\n",
        "            # print(\"🐇 Using GPU\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "            # print(\"🐢 Using CPU\")\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = DuelingQNetwork(state_size, action_size, hidsize1=self.hidsize, hidsize2=self.hidsize).to(self.device)\n",
        "\n",
        "        if not evaluation_mode:\n",
        "            self.qnetwork_target = copy.deepcopy(self.qnetwork_local)\n",
        "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.learning_rate)\n",
        "            self.memory = ReplayBuffer(action_size, self.buffer_size, self.batch_size, self.device)\n",
        "\n",
        "            self.t_step = 0\n",
        "            self.loss = 0.0\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        assert not self.evaluation_mode, \"Policy has been initialized for evaluation only.\"\n",
        "\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > self.buffer_min_size and len(self.memory) > self.batch_size:\n",
        "                self._learn()\n",
        "\n",
        "    def _learn(self):\n",
        "        experiences = self.memory.sample()\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Get expected Q values from local model\n",
        "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        if self.double_dqn:\n",
        "            # Double DQN\n",
        "            q_best_action = self.qnetwork_local(next_states).max(1)[1]\n",
        "            q_targets_next = self.qnetwork_target(next_states).gather(1, q_best_action.unsqueeze(-1))\n",
        "        else:\n",
        "            # DQN\n",
        "            q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(-1)\n",
        "\n",
        "        # Compute Q targets for current states\n",
        "        q_targets = rewards + (self.gamma * q_targets_next * (1 - dones))\n",
        "\n",
        "        # Compute loss\n",
        "        self.loss = F.mse_loss(q_expected, q_targets)\n",
        "\n",
        "        # Minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        self.loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        self._soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
        "\n",
        "    def _soft_update(self, local_model, target_model, tau):\n",
        "        # Soft update model parameters.\n",
        "        # θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "    def save(self, filename):\n",
        "        torch.save(self.qnetwork_local.state_dict(), filename + \".local\")\n",
        "        torch.save(self.qnetwork_target.state_dict(), filename + \".target\")\n",
        "\n",
        "    def load(self, filename):\n",
        "        if os.path.exists(filename + \".local\"):\n",
        "            self.qnetwork_local.load_state_dict(torch.load(filename + \".local\"))\n",
        "        if os.path.exists(filename + \".target\"):\n",
        "            self.qnetwork_target.load_state_dict(torch.load(filename + \".target\"))\n",
        "\n",
        "    def save_replay_buffer(self, filename):\n",
        "        memory = self.memory.memory\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(list(memory)[-500000:], f)\n",
        "\n",
        "    def load_replay_buffer(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.memory.memory = pickle.load(f)\n",
        "\n",
        "    def test(self):\n",
        "        self.act(np.array([[0] * self.state_size]))\n",
        "        self._learn()\n",
        "\n",
        "\n",
        "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, device):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = Experience(np.expand_dims(state, 0), action, reward, np.expand_dims(next_state, 0), done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(self.__v_stack_impr([e.state for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        actions = torch.from_numpy(self.__v_stack_impr([e.action for e in experiences if e is not None])) \\\n",
        "            .long().to(self.device)\n",
        "        rewards = torch.from_numpy(self.__v_stack_impr([e.reward for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        next_states = torch.from_numpy(self.__v_stack_impr([e.next_state for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        dones = torch.from_numpy(self.__v_stack_impr([e.done for e in experiences if e is not None]).astype(np.uint8)) \\\n",
        "            .float().to(self.device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "    def __v_stack_impr(self, states):\n",
        "        sub_dim = len(states[0][0]) if isinstance(states[0], Iterable) else 1\n",
        "        np_states = np.reshape(np.array(states), (len(states), sub_dim))\n",
        "        return np_states\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFIZMfugzVAf"
      },
      "source": [
        "from timeit import default_timer\n",
        "\n",
        "\n",
        "class Timer(object):\n",
        "    \"\"\"\n",
        "    Utility to measure times.\n",
        "\n",
        "    TODO:\n",
        "    - add \"lap\" method to make it easier to measure average time (+std) when measuring the same thing multiple times.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.total_time = 0.0\n",
        "        self.start_time = 0.0\n",
        "        self.end_time = 0.0\n",
        "\n",
        "    def start(self):\n",
        "        self.start_time = default_timer()\n",
        "\n",
        "    def end(self):\n",
        "        self.total_time += default_timer() - self.start_time\n",
        "\n",
        "    def get(self):\n",
        "        return self.total_time\n",
        "\n",
        "    def get_current(self):\n",
        "        return default_timer() - self.start_time\n",
        "\n",
        "    def reset(self):\n",
        "        self.__init__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.get()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpZI9XgOzVo_"
      },
      "source": [
        "\n",
        "def create_rail_env(env_params, tree_observation):\n",
        "    n_agents = env_params.n_agents\n",
        "    x_dim = env_params.x_dim\n",
        "    y_dim = env_params.y_dim\n",
        "    n_cities = env_params.n_cities\n",
        "    max_rails_between_cities = env_params.max_rails_between_cities\n",
        "    max_rails_in_city = env_params.max_rails_in_city\n",
        "    seed = env_params.seed\n",
        "\n",
        "    # Break agents from time to time\n",
        "    malfunction_parameters = MalfunctionParameters(\n",
        "        malfunction_rate=env_params.malfunction_rate,\n",
        "        min_duration=20,\n",
        "        max_duration=50\n",
        "    )\n",
        "\n",
        "    return RailEnv(\n",
        "        width=x_dim, height=y_dim,\n",
        "        rail_generator=sparse_rail_generator(\n",
        "            max_num_cities=n_cities,\n",
        "            grid_mode=False,\n",
        "            max_rails_between_cities=max_rails_between_cities,\n",
        "            max_rails_in_city=max_rails_in_city\n",
        "        ),\n",
        "        schedule_generator=sparse_schedule_generator(),\n",
        "        number_of_agents=n_agents,\n",
        "        malfunction_generator_and_process_data=malfunction_from_params(malfunction_parameters),\n",
        "        obs_builder_object=tree_observation,\n",
        "        random_seed=seed\n",
        "    )\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdjYcyLTzaPS"
      },
      "source": [
        "\n",
        "def train_agent(train_params, train_env_params, eval_env_params, obs_params):\n",
        "    # Environment parameters\n",
        "    n_agents = train_env_params.n_agents\n",
        "    x_dim = train_env_params.x_dim\n",
        "    y_dim = train_env_params.y_dim\n",
        "    n_cities = train_env_params.n_cities\n",
        "    max_rails_between_cities = train_env_params.max_rails_between_cities\n",
        "    max_rails_in_city = train_env_params.max_rails_in_city\n",
        "    seed = train_env_params.seed\n",
        "\n",
        "    # Unique ID for this training\n",
        "    now = datetime.now()\n",
        "    training_id = now.strftime('%y%m%d%H%M%S')\n",
        "\n",
        "    # Observation parameters\n",
        "    observation_tree_depth = obs_params.observation_tree_depth\n",
        "    observation_radius = obs_params.observation_radius\n",
        "    observation_max_path_depth = obs_params.observation_max_path_depth\n",
        "\n",
        "    # Training parameters\n",
        "    eps_start = train_params.eps_start\n",
        "    eps_end = train_params.eps_end\n",
        "    eps_decay = train_params.eps_decay\n",
        "    n_episodes = train_params.n_episodes\n",
        "    checkpoint_interval = train_params.checkpoint_interval\n",
        "    n_eval_episodes = train_params.n_evaluation_episodes\n",
        "    restore_replay_buffer = train_params.restore_replay_buffer\n",
        "    save_replay_buffer = train_params.save_replay_buffer\n",
        "\n",
        "    # Set the seeds\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Observation builder\n",
        "    predictor = ShortestPathPredictorForRailEnv(observation_max_path_depth)\n",
        "    tree_observation = TreeObsForRailEnv(max_depth=observation_tree_depth, predictor=predictor)\n",
        "\n",
        "    # Setup the environments\n",
        "    train_env = create_rail_env(train_env_params, tree_observation)\n",
        "    train_env.reset(regenerate_schedule=True, regenerate_rail=True)\n",
        "    eval_env = create_rail_env(eval_env_params, tree_observation)\n",
        "    eval_env.reset(regenerate_schedule=True, regenerate_rail=True)\n",
        "\n",
        "    # Setup renderer\n",
        "    if train_params.render:\n",
        "        env_renderer = RenderTool(train_env, gl=\"PGL\")\n",
        "\n",
        "    # Calculate the state size given the depth of the tree observation and the number of features\n",
        "    n_features_per_node = train_env.obs_builder.observation_dim\n",
        "    n_nodes = sum([np.power(4, i) for i in range(observation_tree_depth + 1)])\n",
        "    state_size = n_features_per_node * n_nodes\n",
        "\n",
        "    # The action space of flatland is 5 discrete actions\n",
        "    action_size = 5\n",
        "\n",
        "    # Max number of steps per episode\n",
        "    # This is the official formula used during evaluations\n",
        "    # See details in flatland.envs.schedule_generators.sparse_schedule_generator\n",
        "    # max_steps = int(4 * 2 * (env.height + env.width + (n_agents / n_cities)))\n",
        "    max_steps = train_env._max_episode_steps\n",
        "\n",
        "    action_count = [0] * action_size\n",
        "    action_dict = dict()\n",
        "    agent_obs = [None] * n_agents\n",
        "    agent_prev_obs = [None] * n_agents\n",
        "    agent_prev_action = [2] * n_agents\n",
        "    update_values = [False] * n_agents\n",
        "\n",
        "    # Smoothed values used as target for hyperparameter tuning\n",
        "    smoothed_normalized_score = -1.0\n",
        "    smoothed_eval_normalized_score = -1.0\n",
        "    smoothed_completion = 0.0\n",
        "    smoothed_eval_completion = 0.0\n",
        "\n",
        "    # Double Dueling DQN policy\n",
        "    policy = DDDQNPolicy(state_size, action_size, train_params)\n",
        "\n",
        "    # Loads existing replay buffer\n",
        "    if restore_replay_buffer:\n",
        "        try:\n",
        "            policy.load_replay_buffer(restore_replay_buffer)\n",
        "            policy.test()\n",
        "        except RuntimeError as e:\n",
        "            print(\"\\n🛑 Could't load replay buffer, were the experiences generated using the same tree depth?\")\n",
        "            print(e)\n",
        "            exit(1)\n",
        "\n",
        "    print(\"\\n💾 Replay buffer status: {}/{} experiences\".format(len(policy.memory.memory), train_params.buffer_size))\n",
        "\n",
        "    hdd = psutil.disk_usage('/')\n",
        "    if save_replay_buffer and (hdd.free / (2 ** 30)) < 500.0:\n",
        "        print(\"⚠️  Careful! Saving replay buffers will quickly consume a lot of disk space. You have {:.2f}gb left.\".format(hdd.free / (2 ** 30)))\n",
        "\n",
        "    # TensorBoard writer\n",
        "    writer = SummaryWriter()\n",
        "    writer.add_hparams(vars(train_params), {})\n",
        "    writer.add_hparams(vars(train_env_params), {})\n",
        "    writer.add_hparams(vars(obs_params), {})\n",
        "\n",
        "    training_timer = Timer()\n",
        "    training_timer.start()\n",
        "\n",
        "    print(\"\\n🚉 Training {} trains on {}x{} grid for {} episodes, evaluating on {} episodes every {} episodes. Training id '{}'.\\n\".format(\n",
        "        train_env.get_num_agents(),\n",
        "        x_dim, y_dim,\n",
        "        n_episodes,\n",
        "        n_eval_episodes,\n",
        "        checkpoint_interval,\n",
        "        training_id\n",
        "    ))\n",
        "\n",
        "    for episode_idx in range(n_episodes + 1):\n",
        "        step_timer = Timer()\n",
        "        reset_timer = Timer()\n",
        "        learn_timer = Timer()\n",
        "        preproc_timer = Timer()\n",
        "        inference_timer = Timer()\n",
        "\n",
        "        # Reset environment\n",
        "        reset_timer.start()\n",
        "        obs, info = train_env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "        reset_timer.end()\n",
        "\n",
        "        if train_params.render:\n",
        "            env_renderer.set_new_rail()\n",
        "\n",
        "        score = 0\n",
        "        nb_steps = 0\n",
        "        actions_taken = []\n",
        "\n",
        "        # Build initial agent-specific observations\n",
        "        for agent in train_env.get_agent_handles():\n",
        "            if obs[agent]:\n",
        "                agent_obs[agent] = normalize_observation(obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
        "                agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "\n",
        "        # Run episode\n",
        "        for step in range(max_steps - 1):\n",
        "            inference_timer.start()\n",
        "            for agent in train_env.get_agent_handles():\n",
        "                if info['action_required'][agent]:\n",
        "                    update_values[agent] = True\n",
        "                    action = policy.act(agent_obs[agent], eps=eps_start)\n",
        "\n",
        "                    action_count[action] += 1\n",
        "                    actions_taken.append(action)\n",
        "                else:\n",
        "                    # An action is not required if the train hasn't joined the railway network,\n",
        "                    # if it already reached its target, or if is currently malfunctioning.\n",
        "                    update_values[agent] = False\n",
        "                    action = 0\n",
        "                action_dict.update({agent: action})\n",
        "            inference_timer.end()\n",
        "\n",
        "            # Environment step\n",
        "            step_timer.start()\n",
        "            next_obs, all_rewards, done, info = train_env.step(action_dict)\n",
        "            step_timer.end()\n",
        "\n",
        "            # Render an episode at some interval\n",
        "            if train_params.render and episode_idx % checkpoint_interval == 0:\n",
        "                env_renderer.render_env(\n",
        "                    show=True,\n",
        "                    frames=False,\n",
        "                    show_observations=False,\n",
        "                    show_predictions=False\n",
        "                )\n",
        "\n",
        "            # Update replay buffer and train agent\n",
        "            for agent in train_env.get_agent_handles():\n",
        "                if update_values[agent] or done['__all__']:\n",
        "                    # Only learn from timesteps where somethings happened\n",
        "                    learn_timer.start()\n",
        "                    policy.step(agent_prev_obs[agent], agent_prev_action[agent], all_rewards[agent], agent_obs[agent], done[agent])\n",
        "                    learn_timer.end()\n",
        "\n",
        "                    agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "                    agent_prev_action[agent] = action_dict[agent]\n",
        "\n",
        "                # Preprocess the new observations\n",
        "                if next_obs[agent]:\n",
        "                    preproc_timer.start()\n",
        "                    agent_obs[agent] = normalize_observation(next_obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
        "                    preproc_timer.end()\n",
        "\n",
        "                score += all_rewards[agent]\n",
        "\n",
        "            nb_steps = step\n",
        "\n",
        "            if done['__all__']:\n",
        "                break\n",
        "\n",
        "        # Epsilon decay\n",
        "        eps_start = max(eps_end, eps_decay * eps_start)\n",
        "\n",
        "        # Collect information about training\n",
        "        tasks_finished = sum(done[idx] for idx in train_env.get_agent_handles())\n",
        "        completion = tasks_finished / max(1, train_env.get_num_agents())\n",
        "        normalized_score = score / (max_steps * train_env.get_num_agents())\n",
        "        action_probs = action_count / np.sum(action_count)\n",
        "        action_count = [1] * action_size\n",
        "\n",
        "        smoothing = 0.99\n",
        "        smoothed_normalized_score = smoothed_normalized_score * smoothing + normalized_score * (1.0 - smoothing)\n",
        "        smoothed_completion = smoothed_completion * smoothing + completion * (1.0 - smoothing)\n",
        "\n",
        "        # Print logs\n",
        "        if episode_idx % checkpoint_interval == 0:\n",
        "            torch.save(policy.qnetwork_local, './checkpoints/' + training_id + '-' + str(episode_idx) + '.pth')\n",
        "\n",
        "            if save_replay_buffer:\n",
        "                policy.save_replay_buffer('./replay_buffers/' + training_id + '-' + str(episode_idx) + '.pkl')\n",
        "\n",
        "            if train_params.render:\n",
        "                env_renderer.close_window()\n",
        "\n",
        "        print(\n",
        "            '\\r🚂 Episode {}'\n",
        "            '\\t 🏆 Score: {:.3f}'\n",
        "            ' Avg: {:.3f}'\n",
        "            '\\t 💯 Done: {:.2f}%'\n",
        "            ' Avg: {:.2f}%'\n",
        "            '\\t 🎲 Epsilon: {:.3f} '\n",
        "            '\\t 🔀 Action Probs: {}'.format(\n",
        "                episode_idx,\n",
        "                normalized_score,\n",
        "                smoothed_normalized_score,\n",
        "                100 * completion,\n",
        "                100 * smoothed_completion,\n",
        "                eps_start,\n",
        "                format_action_prob(action_probs)\n",
        "            ), end=\" \")\n",
        "\n",
        "        # Evaluate policy and log results at some interval\n",
        "        if episode_idx % checkpoint_interval == 0 and n_eval_episodes > 0:\n",
        "            scores, completions, nb_steps_eval = eval_policy(eval_env, policy, train_params, obs_params)\n",
        "\n",
        "            writer.add_scalar(\"evaluation/scores_min\", np.min(scores), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/scores_max\", np.max(scores), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/scores_mean\", np.mean(scores), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/scores_std\", np.std(scores), episode_idx)\n",
        "            writer.add_histogram(\"evaluation/scores\", np.array(scores), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/completions_min\", np.min(completions), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/completions_max\", np.max(completions), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/completions_mean\", np.mean(completions), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/completions_std\", np.std(completions), episode_idx)\n",
        "            writer.add_histogram(\"evaluation/completions\", np.array(completions), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/nb_steps_min\", np.min(nb_steps_eval), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/nb_steps_max\", np.max(nb_steps_eval), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/nb_steps_mean\", np.mean(nb_steps_eval), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/nb_steps_std\", np.std(nb_steps_eval), episode_idx)\n",
        "            writer.add_histogram(\"evaluation/nb_steps\", np.array(nb_steps_eval), episode_idx)\n",
        "\n",
        "            smoothing = 0.9\n",
        "            smoothed_eval_normalized_score = smoothed_eval_normalized_score * smoothing + np.mean(scores) * (1.0 - smoothing)\n",
        "            smoothed_eval_completion = smoothed_eval_completion * smoothing + np.mean(completions) * (1.0 - smoothing)\n",
        "            writer.add_scalar(\"evaluation/smoothed_score\", smoothed_eval_normalized_score, episode_idx)\n",
        "            writer.add_scalar(\"evaluation/smoothed_completion\", smoothed_eval_completion, episode_idx)\n",
        "\n",
        "        # Save logs to tensorboard\n",
        "        writer.add_scalar(\"training/score\", normalized_score, episode_idx)\n",
        "        writer.add_scalar(\"training/smoothed_score\", smoothed_normalized_score, episode_idx)\n",
        "        writer.add_scalar(\"training/completion\", np.mean(completion), episode_idx)\n",
        "        writer.add_scalar(\"training/smoothed_completion\", np.mean(smoothed_completion), episode_idx)\n",
        "        writer.add_scalar(\"training/nb_steps\", nb_steps, episode_idx)\n",
        "        writer.add_histogram(\"actions/distribution\", np.array(actions_taken), episode_idx)\n",
        "        writer.add_scalar(\"actions/nothing\", action_probs[RailEnvActions.DO_NOTHING], episode_idx)\n",
        "        writer.add_scalar(\"actions/left\", action_probs[RailEnvActions.MOVE_LEFT], episode_idx)\n",
        "        writer.add_scalar(\"actions/forward\", action_probs[RailEnvActions.MOVE_FORWARD], episode_idx)\n",
        "        writer.add_scalar(\"actions/right\", action_probs[RailEnvActions.MOVE_RIGHT], episode_idx)\n",
        "        writer.add_scalar(\"actions/stop\", action_probs[RailEnvActions.STOP_MOVING], episode_idx)\n",
        "        writer.add_scalar(\"training/epsilon\", eps_start, episode_idx)\n",
        "        writer.add_scalar(\"training/buffer_size\", len(policy.memory), episode_idx)\n",
        "        writer.add_scalar(\"training/loss\", policy.loss, episode_idx)\n",
        "        writer.add_scalar(\"timer/reset\", reset_timer.get(), episode_idx)\n",
        "        writer.add_scalar(\"timer/step\", step_timer.get(), episode_idx)\n",
        "        writer.add_scalar(\"timer/learn\", learn_timer.get(), episode_idx)\n",
        "        writer.add_scalar(\"timer/preproc\", preproc_timer.get(), episode_idx)\n",
        "        writer.add_scalar(\"timer/total\", training_timer.get_current(), episode_idx)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXHOaM0mzdZm"
      },
      "source": [
        "\n",
        "def format_action_prob(action_probs):\n",
        "    action_probs = np.round(action_probs, 3)\n",
        "    actions = [\"↻\", \"←\", \"↑\", \"→\", \"◼\"]\n",
        "\n",
        "    buffer = \"\"\n",
        "    for action, action_prob in zip(actions, action_probs):\n",
        "        buffer += action + \" \" + \"{:.3f}\".format(action_prob) + \" \"\n",
        "\n",
        "    return buffer\n",
        "\n",
        "\n",
        "def eval_policy(env, policy, train_params, obs_params):\n",
        "    n_eval_episodes = train_params.n_evaluation_episodes\n",
        "    max_steps = env._max_episode_steps\n",
        "    tree_depth = obs_params.observation_tree_depth\n",
        "    observation_radius = obs_params.observation_radius\n",
        "\n",
        "    action_dict = dict()\n",
        "    scores = []\n",
        "    completions = []\n",
        "    nb_steps = []\n",
        "\n",
        "    for episode_idx in range(n_eval_episodes):\n",
        "        agent_obs = [None] * env.get_num_agents()\n",
        "        score = 0.0\n",
        "\n",
        "        obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "\n",
        "        final_step = 0\n",
        "\n",
        "        for step in range(max_steps - 1):\n",
        "            for agent in env.get_agent_handles():\n",
        "                if obs[agent]:\n",
        "                    agent_obs[agent] = normalize_observation(obs[agent], tree_depth=tree_depth, observation_radius=observation_radius)\n",
        "\n",
        "                action = 0\n",
        "                if info['action_required'][agent]:\n",
        "                    action = policy.act(agent_obs[agent], eps=0.0)\n",
        "                action_dict.update({agent: action})\n",
        "\n",
        "            obs, all_rewards, done, info = env.step(action_dict)\n",
        "\n",
        "            for agent in env.get_agent_handles():\n",
        "                score += all_rewards[agent]\n",
        "\n",
        "            final_step = step\n",
        "\n",
        "            if done['__all__']:\n",
        "                break\n",
        "\n",
        "        normalized_score = score / (max_steps * env.get_num_agents())\n",
        "        scores.append(normalized_score)\n",
        "\n",
        "        tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "        completion = tasks_finished / max(1, env.get_num_agents())\n",
        "        completions.append(completion)\n",
        "\n",
        "        nb_steps.append(final_step)\n",
        "\n",
        "    print(\"\\t✅ Eval: score {:.3f} done {:.1f}%\".format(np.mean(scores), np.mean(completions) * 100.0))\n",
        "\n",
        "    return scores, completions, nb_steps\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPCUX9n7zfeI",
        "outputId": "5b1b2221-b0ea-4aa4-fca3-321498843118"
      },
      "source": [
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "training_params = Object()\n",
        "training_params.n_episodes = 2500\n",
        "training_params.training_env_config = 0\n",
        "training_params.evaluation_env_config = 0\n",
        "training_params.n_evaluation_episodes = 25\n",
        "training_params.checkpoint_interval = 100\n",
        "training_params.eps_start = 1.0\n",
        "training_params.eps_end = 0.01\n",
        "training_params.eps_decay = 0.99\n",
        "training_params.buffer_size = int(1e5)\n",
        "training_params.buffer_min_size = 0\n",
        "training_params.restore_replay_buffer = \"\"\n",
        "training_params.save_replay_buffer = False\n",
        "training_params.batch_size = 128\n",
        "training_params.gamma = 0.99\n",
        "training_params.tau = 1e-3\n",
        "training_params.learning_rate = 0.5e-4\n",
        "training_params.hidden_size = 128\n",
        "training_params.update_every = 8\n",
        "training_params.use_gpu = False\n",
        "training_params.num_threads = 1\n",
        "training_params.render = False\n",
        "\n",
        "\n",
        "env_params = [\n",
        "    {\n",
        "        # Test_0\n",
        "        \"n_agents\": 5,\n",
        "        \"x_dim\": 25,\n",
        "        \"y_dim\": 25,\n",
        "        \"n_cities\": 2,\n",
        "        \"max_rails_between_cities\": 2,\n",
        "        \"max_rails_in_city\": 3,\n",
        "        \"malfunction_rate\": 1 / 50,\n",
        "        \"seed\": 0\n",
        "    },\n",
        "    {\n",
        "        # Test_1\n",
        "        \"n_agents\": 10,\n",
        "        \"x_dim\": 30,\n",
        "        \"y_dim\": 30,\n",
        "        \"n_cities\": 2,\n",
        "        \"max_rails_between_cities\": 2,\n",
        "        \"max_rails_in_city\": 3,\n",
        "        \"malfunction_rate\": 1 / 100,\n",
        "        \"seed\": 0\n",
        "    },\n",
        "    {\n",
        "        # Test_2\n",
        "        \"n_agents\": 20,\n",
        "        \"x_dim\": 30,\n",
        "        \"y_dim\": 30,\n",
        "        \"n_cities\": 3,\n",
        "        \"max_rails_between_cities\": 2,\n",
        "        \"max_rails_in_city\": 3,\n",
        "        \"malfunction_rate\": 1 / 200,\n",
        "        \"seed\": 0\n",
        "    },\n",
        "]\n",
        "\n",
        "obs_params = {\n",
        "    \"observation_tree_depth\": 2,\n",
        "    \"observation_radius\": 10,\n",
        "    \"observation_max_path_depth\": 30\n",
        "}\n",
        "\n",
        "def check_env_config(id):\n",
        "    if id >= len(env_params) or id < 0:\n",
        "        print(\"\\n🛑 Invalid environment configuration, only Test_0 to Test_{} are supported.\".format(len(env_params) - 1))\n",
        "        exit(1)\n",
        "\n",
        "\n",
        "check_env_config(training_params.training_env_config)\n",
        "check_env_config(training_params.evaluation_env_config)\n",
        "\n",
        "training_env_params = env_params[training_params.training_env_config]\n",
        "evaluation_env_params = env_params[training_params.evaluation_env_config]\n",
        "\n",
        "print(\"\\nTraining parameters:\")\n",
        "pprint(vars(training_params))\n",
        "print(\"\\nTraining environment parameters (Test_{}):\".format(training_params.training_env_config))\n",
        "pprint(training_env_params)\n",
        "print(\"\\nEvaluation environment parameters (Test_{}):\".format(training_params.evaluation_env_config))\n",
        "pprint(evaluation_env_params)\n",
        "print(\"\\nObservation parameters:\")\n",
        "pprint(obs_params)\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(training_params.num_threads)\n",
        "train_agent(training_params, Namespace(**training_env_params), Namespace(**evaluation_env_params), Namespace(**obs_params))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training parameters:\n",
            "{'batch_size': 128,\n",
            " 'buffer_min_size': 0,\n",
            " 'buffer_size': 100000,\n",
            " 'checkpoint_interval': 100,\n",
            " 'eps_decay': 0.99,\n",
            " 'eps_end': 0.01,\n",
            " 'eps_start': 1.0,\n",
            " 'evaluation_env_config': 0,\n",
            " 'gamma': 0.99,\n",
            " 'hidden_size': 128,\n",
            " 'learning_rate': 5e-05,\n",
            " 'n_episodes': 2500,\n",
            " 'n_evaluation_episodes': 25,\n",
            " 'num_threads': 1,\n",
            " 'render': False,\n",
            " 'restore_replay_buffer': '',\n",
            " 'save_replay_buffer': False,\n",
            " 'tau': 0.001,\n",
            " 'training_env_config': 0,\n",
            " 'update_every': 8,\n",
            " 'use_gpu': False}\n",
            "\n",
            "Training environment parameters (Test_0):\n",
            "{'malfunction_rate': 0.02,\n",
            " 'max_rails_between_cities': 2,\n",
            " 'max_rails_in_city': 3,\n",
            " 'n_agents': 5,\n",
            " 'n_cities': 2,\n",
            " 'seed': 0,\n",
            " 'x_dim': 25,\n",
            " 'y_dim': 25}\n",
            "\n",
            "Evaluation environment parameters (Test_0):\n",
            "{'malfunction_rate': 0.02,\n",
            " 'max_rails_between_cities': 2,\n",
            " 'max_rails_in_city': 3,\n",
            " 'n_agents': 5,\n",
            " 'n_cities': 2,\n",
            " 'seed': 0,\n",
            " 'x_dim': 25,\n",
            " 'y_dim': 25}\n",
            "\n",
            "Observation parameters:\n",
            "{'observation_max_path_depth': 30,\n",
            " 'observation_radius': 10,\n",
            " 'observation_tree_depth': 2}\n",
            "DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator\n",
            "DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/rail_generators.py:781: UserWarning: Could not set all required cities!\n",
            "  \"Could not set all required cities!\")\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/rail_generators.py:703: UserWarning: [WARNING] Changing to Grid mode to place at least 2 cities.\n",
            "  warnings.warn(\"[WARNING] Changing to Grid mode to place at least 2 cities.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Replay buffer status: 0/100000 experiences\n",
            "\n",
            "🚉 Training 5 trains on 25x25 grid for 2500 episodes, evaluating on 25 episodes every 100 episodes. Training id '201201194822'.\n",
            "\n",
            "🚂 Episode 0\t 🏆 Score: -0.998 Avg: -1.000\t 💯 Done: 0.00% Avg: 0.00%\t 🎲 Epsilon: 0.990 \t 🔀 Action Probs: ↻ 0.164 ← 0.244 ↑ 0.192 → 0.174 ◼ 0.225  \t✅ Eval: score -0.998 done 0.0%\n",
            "🚂 Episode 100\t 🏆 Score: -0.638 Avg: -0.821\t 💯 Done: 60.00% Avg: 23.92%\t 🎲 Epsilon: 0.362 \t 🔀 Action Probs: ↻ 0.078 ← 0.317 ↑ 0.177 → 0.212 ◼ 0.217  \t✅ Eval: score -0.707 done 32.8%\n",
            "🚂 Episode 200\t 🏆 Score: -0.468 Avg: -0.658\t 💯 Done: 60.00% Avg: 44.24%\t 🎲 Epsilon: 0.133 \t 🔀 Action Probs: ↻ 0.028 ← 0.397 ↑ 0.149 → 0.167 ◼ 0.259  \t✅ Eval: score -0.781 done 24.8%\n",
            "🚂 Episode 300\t 🏆 Score: -0.849 Avg: -0.581\t 💯 Done: 20.00% Avg: 53.83%\t 🎲 Epsilon: 0.049 \t 🔀 Action Probs: ↻ 0.027 ← 0.268 ↑ 0.281 → 0.277 ◼ 0.147  \t✅ Eval: score -0.749 done 30.4%\n",
            "🚂 Episode 400\t 🏆 Score: -0.690 Avg: -0.539\t 💯 Done: 40.00% Avg: 57.27%\t 🎲 Epsilon: 0.018 \t 🔀 Action Probs: ↻ 0.015 ← 0.135 ↑ 0.149 → 0.118 ◼ 0.582  \t✅ Eval: score -0.591 done 48.8%\n",
            "🚂 Episode 500\t 🏆 Score: -0.092 Avg: -0.448\t 💯 Done: 100.00% Avg: 66.88%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.005 ← 0.236 ↑ 0.413 → 0.163 ◼ 0.183  \t✅ Eval: score -0.366 done 71.2%\n",
            "🚂 Episode 600\t 🏆 Score: -0.109 Avg: -0.456\t 💯 Done: 100.00% Avg: 64.04%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.015 ← 0.665 ↑ 0.019 → 0.175 ◼ 0.126  \t✅ Eval: score -0.507 done 57.6%\n",
            "🚂 Episode 700\t 🏆 Score: -0.998 Avg: -0.424\t 💯 Done: 0.00% Avg: 67.66%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.333 ← 0.217 ↑ 0.023 → 0.147 ◼ 0.280  \t✅ Eval: score -0.490 done 58.4%\n",
            "🚂 Episode 800\t 🏆 Score: -0.234 Avg: -0.439\t 💯 Done: 100.00% Avg: 65.99%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.003 ← 0.245 ↑ 0.233 → 0.333 ◼ 0.185  \t✅ Eval: score -0.403 done 68.8%\n",
            "🚂 Episode 900\t 🏆 Score: -0.998 Avg: -0.465\t 💯 Done: 0.00% Avg: 62.92%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.015 ← 0.387 ↑ 0.387 → 0.109 ◼ 0.102  \t✅ Eval: score -0.449 done 62.4%\n",
            "🚂 Episode 1000\t 🏆 Score: -0.515 Avg: -0.469\t 💯 Done: 60.00% Avg: 61.89%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.011 ← 0.575 ↑ 0.026 → 0.253 ◼ 0.136  \t✅ Eval: score -0.353 done 72.8%\n",
            "🚂 Episode 1100\t 🏆 Score: -0.998 Avg: -0.454\t 💯 Done: 0.00% Avg: 64.12%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.014 ← 0.361 ↑ 0.042 → 0.375 ◼ 0.208  \t✅ Eval: score -0.397 done 68.8%\n",
            "🚂 Episode 1200\t 🏆 Score: -0.256 Avg: -0.450\t 💯 Done: 100.00% Avg: 64.11%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.031 ← 0.447 ↑ 0.278 → 0.142 ◼ 0.102  \t✅ Eval: score -0.503 done 58.4%\n",
            "🚂 Episode 1300\t 🏆 Score: -0.223 Avg: -0.448\t 💯 Done: 100.00% Avg: 64.68%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.012 ← 0.621 ↑ 0.090 → 0.266 ◼ 0.012  \t✅ Eval: score -0.351 done 73.6%\n",
            "🚂 Episode 1400\t 🏆 Score: -0.207 Avg: -0.420\t 💯 Done: 100.00% Avg: 67.48%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.019 ← 0.365 ↑ 0.337 → 0.276 ◼ 0.003  \t✅ Eval: score -0.517 done 56.8%\n",
            "🚂 Episode 1500\t 🏆 Score: -0.998 Avg: -0.425\t 💯 Done: 0.00% Avg: 66.76%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.059 ← 0.238 ↑ 0.119 → 0.432 ◼ 0.151  \t✅ Eval: score -0.459 done 64.8%\n",
            "🚂 Episode 1600\t 🏆 Score: -0.133 Avg: -0.453\t 💯 Done: 100.00% Avg: 64.34%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.004 ← 0.753 ↑ 0.069 → 0.170 ◼ 0.004  \t✅ Eval: score -0.429 done 68.8%\n",
            "🚂 Episode 1700\t 🏆 Score: -0.491 Avg: -0.413\t 💯 Done: 100.00% Avg: 68.98%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.047 ← 0.314 ↑ 0.013 → 0.249 ◼ 0.377  \t✅ Eval: score -0.348 done 75.2%\n",
            "🚂 Episode 1800\t 🏆 Score: -0.998 Avg: -0.408\t 💯 Done: 0.00% Avg: 69.44%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.141 ← 0.268 ↑ 0.099 → 0.488 ◼ 0.005  \t✅ Eval: score -0.512 done 56.8%\n",
            "🚂 Episode 1900\t 🏆 Score: -0.106 Avg: -0.407\t 💯 Done: 100.00% Avg: 68.59%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.004 ← 0.266 ↑ 0.350 → 0.367 ◼ 0.013  \t✅ Eval: score -0.453 done 64.0%\n",
            "🚂 Episode 2000\t 🏆 Score: -0.648 Avg: -0.416\t 💯 Done: 40.00% Avg: 67.01%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.009 ← 0.453 ↑ 0.089 → 0.391 ◼ 0.058  \t✅ Eval: score -0.419 done 68.0%\n",
            "🚂 Episode 2100\t 🏆 Score: -0.616 Avg: -0.417\t 💯 Done: 40.00% Avg: 67.20%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.014 ← 0.753 ↑ 0.110 → 0.096 ◼ 0.027  \t✅ Eval: score -0.393 done 68.8%\n",
            "🚂 Episode 2200\t 🏆 Score: -0.668 Avg: -0.453\t 💯 Done: 40.00% Avg: 63.89%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.060 ← 0.601 ↑ 0.024 → 0.314 ◼ 0.001  \t✅ Eval: score -0.525 done 56.8%\n",
            "🚂 Episode 2300\t 🏆 Score: -0.235 Avg: -0.445\t 💯 Done: 100.00% Avg: 64.48%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.006 ← 0.181 ↑ 0.211 → 0.313 ◼ 0.289  \t✅ Eval: score -0.443 done 67.2%\n",
            "🚂 Episode 2400\t 🏆 Score: -0.082 Avg: -0.429\t 💯 Done: 100.00% Avg: 66.76%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.006 ← 0.503 ↑ 0.057 → 0.428 ◼ 0.006  \t✅ Eval: score -0.427 done 66.4%\n",
            "🚂 Episode 2500\t 🏆 Score: -0.998 Avg: -0.430\t 💯 Done: 0.00% Avg: 66.38%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.011 ← 0.461 ↑ 0.056 → 0.461 ◼ 0.011  \t✅ Eval: score -0.578 done 50.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWKeYkbxzvo5"
      },
      "source": [
        "\n",
        "x = Object()\n",
        "x.a = 1\n",
        "x.b = False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IkPORV4zz1u",
        "outputId": "3d596a05-f9db-4acf-d27b-83fb683e0f54"
      },
      "source": [
        "x.b"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zBvIk7e0Hat"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}