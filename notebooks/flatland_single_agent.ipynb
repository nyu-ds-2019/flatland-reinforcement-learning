{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSfiuAAsYFGv",
    "outputId": "6e850544-e4b3-4350-d1b5-99140fe0a18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting flatland-rl\n",
      "  Downloading flatland-rl-2.2.2.tar.gz (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 31.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tox>=3.5.2\n",
      "  Using cached tox-3.20.1-py2.py3-none-any.whl (83 kB)\n",
      "Collecting pytest<5,>=3.8.2\n",
      "  Using cached pytest-4.6.11-py2.py3-none-any.whl (231 kB)\n",
      "Collecting pytest-runner>=4.2\n",
      "  Using cached pytest_runner-5.2-py2.py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: Click>=7.0 in /ext3/miniconda3/lib/python3.8/site-packages (from flatland-rl) (7.1.2)\n",
      "Processing /home/ns4486/.cache/pip/wheels/f0/5d/3d/d4e79c8c62f53e9a02333017a158c409f69b2d11b8398c3574/crowdai_api-0.1.22-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.16.2 in /ext3/miniconda3/lib/python3.8/site-packages (from flatland-rl) (1.19.2)\n",
      "Collecting recordtype>=1.3\n",
      "  Using cached recordtype-1.3-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: matplotlib>=3.0.2 in /ext3/miniconda3/lib/python3.8/site-packages (from flatland-rl) (3.3.2)\n",
      "Requirement already satisfied: Pillow>=5.4.1 in /ext3/miniconda3/lib/python3.8/site-packages (from flatland-rl) (7.2.0)\n",
      "Processing /home/ns4486/.cache/pip/wheels/56/2e/45/9ae160fc31c10e4b799d0ebc32ba82b32f1b057e0ebf28ea82/msgpack-0.6.1-cp38-cp38-linux_x86_64.whl\n",
      "Collecting msgpack-numpy>=0.4.4.0\n",
      "  Using cached msgpack_numpy-0.4.7.1-py2.py3-none-any.whl (6.7 kB)\n",
      "Collecting svgutils>=0.3.1\n",
      "  Using cached svgutils-0.3.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pyarrow>=0.13.0\n",
      "  Using cached pyarrow-2.0.0-cp38-cp38-manylinux2014_x86_64.whl (17.8 MB)\n",
      "Requirement already satisfied: pandas>=0.25.1 in /ext3/miniconda3/lib/python3.8/site-packages (from flatland-rl) (1.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.17 in /ext3/miniconda3/lib/python3.8/site-packages (from flatland-rl) (1.7.0)\n",
      "Collecting importlib-resources<2,>=1.0.1\n",
      "  Using cached importlib_resources-1.5.0-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /ext3/miniconda3/lib/python3.8/site-packages (from flatland-rl) (1.15.0)\n",
      "Processing /home/ns4486/.cache/pip/wheels/38/05/4e/161d1463ca145ec1023bd4e5e1f31cbf9239aa8f39a2a2b643/timeout_decorator-0.5.0-py3-none-any.whl\n",
      "Requirement already satisfied: attrs in /ext3/miniconda3/lib/python3.8/site-packages (from flatland-rl) (20.2.0)\n",
      "Processing /home/ns4486/.cache/pip/wheels/57/a8/2b/f61e167ab89671b86bdb5252d7b18075e91416673d9939d40b/gym-0.14.0-py3-none-any.whl\n",
      "Collecting networkx\n",
      "  Using cached networkx-2.5-py3-none-any.whl (1.6 MB)\n",
      "Collecting ipycanvas\n",
      "  Using cached ipycanvas-0.7.0-py2.py3-none-any.whl (247 kB)\n",
      "Collecting graphviz\n",
      "  Using cached graphviz-0.15-py2.py3-none-any.whl (18 kB)\n",
      "Collecting imageio\n",
      "  Using cached imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "Collecting py>=1.4.17\n",
      "  Using cached py-1.9.0-py2.py3-none-any.whl (99 kB)\n",
      "Collecting pluggy>=0.12.0\n",
      "  Using cached pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting filelock>=3.0.0\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0\n",
      "  Using cached virtualenv-20.2.1-py2.py3-none-any.whl (4.9 MB)\n",
      "Collecting toml>=0.9.4\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: packaging>=14 in /ext3/miniconda3/lib/python3.8/site-packages (from tox>=3.5.2->flatland-rl) (20.4)\n",
      "Collecting atomicwrites>=1.0\n",
      "  Using cached atomicwrites-1.4.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: wcwidth in /ext3/miniconda3/lib/python3.8/site-packages (from pytest<5,>=3.8.2->flatland-rl) (0.2.5)\n",
      "Collecting more-itertools>=4.0.0; python_version > \"2.7\"\n",
      "  Using cached more_itertools-8.6.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: requests>=2.18.4 in /ext3/miniconda3/lib/python3.8/site-packages (from crowdai-api>=0.1.21->flatland-rl) (2.23.0)\n",
      "Collecting redis\n",
      "  Using cached redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
      "Collecting python-gitlab>=1.3.0\n",
      "  Using cached python_gitlab-2.5.0-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /ext3/miniconda3/lib/python3.8/site-packages (from matplotlib>=3.0.2->flatland-rl) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /ext3/miniconda3/lib/python3.8/site-packages (from matplotlib>=3.0.2->flatland-rl) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /ext3/miniconda3/lib/python3.8/site-packages (from matplotlib>=3.0.2->flatland-rl) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /ext3/miniconda3/lib/python3.8/site-packages (from matplotlib>=3.0.2->flatland-rl) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /ext3/miniconda3/lib/python3.8/site-packages (from matplotlib>=3.0.2->flatland-rl) (1.2.0)\n",
      "Collecting lxml\n",
      "  Using cached lxml-4.6.2-cp38-cp38-manylinux1_x86_64.whl (5.4 MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /ext3/miniconda3/lib/python3.8/site-packages (from pandas>=0.25.1->flatland-rl) (2020.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=0.17->flatland-rl) (3.1.0)\n",
      "Collecting pyglet<=1.3.2,>=1.2.0\n",
      "  Using cached pyglet-1.3.2-py2.py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: scipy in /ext3/miniconda3/lib/python3.8/site-packages (from gym==0.14.0->flatland-rl) (1.5.2)\n",
      "Collecting cloudpickle~=1.2.0\n",
      "  Using cached cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /ext3/miniconda3/lib/python3.8/site-packages (from networkx->flatland-rl) (4.4.2)\n",
      "Collecting orjson\n",
      "  Using cached orjson-3.4.5-cp38-cp38-manylinux2014_x86_64.whl (232 kB)\n",
      "Requirement already satisfied: ipywidgets>=7.5.0 in /ext3/miniconda3/lib/python3.8/site-packages (from ipycanvas->flatland-rl) (7.5.1)\n",
      "Collecting appdirs<2,>=1.4.3\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting distlib<1,>=0.3.1\n",
      "  Using cached distlib-0.3.1-py2.py3-none-any.whl (335 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /ext3/miniconda3/lib/python3.8/site-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /ext3/miniconda3/lib/python3.8/site-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /ext3/miniconda3/lib/python3.8/site-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (1.25.8)\n",
      "Requirement already satisfied: future in /ext3/miniconda3/lib/python3.8/site-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.14.0->flatland-rl) (0.18.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /ext3/miniconda3/lib/python3.8/site-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.3.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /ext3/miniconda3/lib/python3.8/site-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.3.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /ext3/miniconda3/lib/python3.8/site-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (3.5.1)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /ext3/miniconda3/lib/python3.8/site-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (7.18.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /ext3/miniconda3/lib/python3.8/site-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.0.7)\n",
      "Requirement already satisfied: jupyter-client in /ext3/miniconda3/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (6.1.6)\n",
      "Requirement already satisfied: tornado>=4.2 in /ext3/miniconda3/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (6.0.4)\n",
      "Requirement already satisfied: ipython-genutils in /ext3/miniconda3/lib/python3.8/site-packages (from traitlets>=4.3.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /ext3/miniconda3/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (6.1.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /ext3/miniconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.17.2)\n",
      "Requirement already satisfied: backcall in /ext3/miniconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ns4486/.local/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (50.3.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /ext3/miniconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (3.0.7)\n",
      "Requirement already satisfied: pygments in /ext3/miniconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.7.1)\n",
      "Requirement already satisfied: pickleshare in /ext3/miniconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /ext3/miniconda3/lib/python3.8/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.8.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /ext3/miniconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /ext3/miniconda3/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.6.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyzmq>=13 in /ext3/miniconda3/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (19.0.2)\n",
      "Requirement already satisfied: argon2-cffi in /ext3/miniconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (20.1.0)\n",
      "Requirement already satisfied: nbconvert in /ext3/miniconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (6.0.6)\n",
      "Requirement already satisfied: prometheus-client in /ext3/miniconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /ext3/miniconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.8.3)\n",
      "Requirement already satisfied: jinja2 in /ext3/miniconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.11.2)\n",
      "Requirement already satisfied: Send2Trash in /ext3/miniconda3/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.5.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /ext3/miniconda3/lib/python3.8/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /ext3/miniconda3/lib/python3.8/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.6.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /ext3/miniconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.17.3)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /ext3/miniconda3/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.14.3)\n",
      "Requirement already satisfied: defusedxml in /ext3/miniconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.6.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /ext3/miniconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /ext3/miniconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.4.2)\n",
      "Requirement already satisfied: testpath in /ext3/miniconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.4.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /ext3/miniconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /ext3/miniconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.8.4)\n",
      "Requirement already satisfied: bleach in /ext3/miniconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (3.2.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /ext3/miniconda3/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /ext3/miniconda3/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.1.1)\n",
      "Requirement already satisfied: pycparser in /ext3/miniconda3/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.20)\n",
      "Requirement already satisfied: nest-asyncio in /ext3/miniconda3/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.4.0)\n",
      "Requirement already satisfied: async-generator in /ext3/miniconda3/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.10)\n",
      "Requirement already satisfied: webencodings in /ext3/miniconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.5.1)\n",
      "Building wheels for collected packages: flatland-rl\n",
      "  Building wheel for flatland-rl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flatland-rl: filename=flatland_rl-2.2.2-py2.py3-none-any.whl size=1690841 sha256=e63fb4ae14b82a25c221e1ef28c02bda17ae3b75412ab99952334b8707d56bda\n",
      "  Stored in directory: /home/ns4486/.cache/pip/wheels/05/69/70/46f7acb40750081204437dc30dcc0174eb92c5df064d67349f\n",
      "Successfully built flatland-rl\n",
      "Installing collected packages: py, pluggy, filelock, appdirs, distlib, virtualenv, toml, tox, atomicwrites, more-itertools, pytest, pytest-runner, redis, python-gitlab, crowdai-api, recordtype, msgpack, msgpack-numpy, lxml, svgutils, pyarrow, importlib-resources, timeout-decorator, pyglet, cloudpickle, gym, networkx, orjson, ipycanvas, graphviz, imageio, flatland-rl\n",
      "\u001b[33m  WARNING: The script virtualenv is installed in '/home/ns4486/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts tox and tox-quickstart are installed in '/home/ns4486/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts py.test and pytest are installed in '/home/ns4486/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script gitlab is installed in '/home/ns4486/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script plasma_store is installed in '/home/ns4486/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "  Attempting uninstall: pyglet\n",
      "    Found existing installation: pyglet 1.5.0\n",
      "    Uninstalling pyglet-1.5.0:\n",
      "      Successfully uninstalled pyglet-1.5.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.6.0\n",
      "    Uninstalling cloudpickle-1.6.0:\n",
      "      Successfully uninstalled cloudpickle-1.6.0\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.17.3\n",
      "    Uninstalling gym-0.17.3:\n",
      "      Successfully uninstalled gym-0.17.3\n",
      "\u001b[33m  WARNING: The scripts imageio_download_bin and imageio_remove_bin are installed in '/home/ns4486/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts flatland-demo and flatland-evaluator are installed in '/home/ns4486/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed appdirs-1.4.4 atomicwrites-1.4.0 cloudpickle-1.2.2 crowdai-api-0.1.22 distlib-0.3.1 filelock-3.0.12 flatland-rl-2.2.2 graphviz-0.15 gym-0.14.0 imageio-2.9.0 importlib-resources-1.5.0 ipycanvas-0.7.0 lxml-4.6.2 more-itertools-8.6.0 msgpack-0.6.1 msgpack-numpy-0.4.7.1 networkx-2.5 orjson-3.4.5 pluggy-0.13.1 py-1.9.0 pyarrow-2.0.0 pyglet-1.3.2 pytest-4.6.11 pytest-runner-5.2 python-gitlab-2.5.0 recordtype-1.3 redis-3.5.3 svgutils-0.3.1 timeout-decorator-0.5.0 toml-0.10.2 tox-3.20.1 virtualenv-20.2.1\n"
     ]
    }
   ],
   "source": [
    "# !conda install -c conda-forge cairosvg pycairo\n",
    "# !conda install -c anaconda tk\n",
    "!pip install flatland-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5wH_6CZnYMjC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from collections import namedtuple, deque, Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "# base_dir = Path(__file__).resolve().parent.parent\n",
    "# sys.path.append(str(base_dir))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import sparse_rail_generator, random_rail_generator, complex_rail_generator\n",
    "from flatland.envs.schedule_generators import sparse_schedule_generator\n",
    "from flatland.envs.observations import TreeObsForRailEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6jNItEtJZePx"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DuelingQNetwork(nn.Module):\n",
    "    \"\"\"Dueling Q-network (https://arxiv.org/abs/1511.06581)\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidsize1=128, hidsize2=128):\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "\n",
    "        # value network\n",
    "        self.fc1_val = nn.Linear(state_size, hidsize1)\n",
    "        self.fc2_val = nn.Linear(hidsize1, hidsize2)\n",
    "        self.fc4_val = nn.Linear(hidsize2, 1)\n",
    "\n",
    "        # advantage network\n",
    "        self.fc1_adv = nn.Linear(state_size, hidsize1)\n",
    "        self.fc2_adv = nn.Linear(hidsize1, hidsize2)\n",
    "        self.fc4_adv = nn.Linear(hidsize2, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        val = F.relu(self.fc1_val(x))\n",
    "        val = F.relu(self.fc2_val(val))\n",
    "        val = self.fc4_val(val)\n",
    "\n",
    "        # advantage calculation\n",
    "        adv = F.relu(self.fc1_adv(x))\n",
    "        adv = F.relu(self.fc2_adv(adv))\n",
    "        adv = self.fc4_adv(adv)\n",
    "\n",
    "        return val + adv - adv.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d0RZneMGbSJZ"
   },
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = Experience(np.expand_dims(state, 0), action, reward, np.expand_dims(next_state, 0), done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(self.__v_stack_impr([e.state for e in experiences if e is not None])) \\\n",
    "            .float().to(self.device)\n",
    "        actions = torch.from_numpy(self.__v_stack_impr([e.action for e in experiences if e is not None])) \\\n",
    "            .long().to(self.device)\n",
    "        rewards = torch.from_numpy(self.__v_stack_impr([e.reward for e in experiences if e is not None])) \\\n",
    "            .float().to(self.device)\n",
    "        next_states = torch.from_numpy(self.__v_stack_impr([e.next_state for e in experiences if e is not None])) \\\n",
    "            .float().to(self.device)\n",
    "        dones = torch.from_numpy(self.__v_stack_impr([e.done for e in experiences if e is not None]).astype(np.uint8)) \\\n",
    "            .float().to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __v_stack_impr(self, states):\n",
    "        sub_dim = len(states[0][0]) if isinstance(states[0], Iterable) else 1\n",
    "        np_states = np.reshape(np.array(states), (len(states), sub_dim))\n",
    "        return np_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rZFwHl0MZgJ_"
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, filename):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load(self, filename):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class DDDQNPolicy(Policy):\n",
    "    \"\"\"Dueling Double DQN policy\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, parameters, evaluation_mode=False):\n",
    "        self.evaluation_mode = evaluation_mode\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.double_dqn = True\n",
    "        self.hidsize = 1\n",
    "\n",
    "        if not evaluation_mode:\n",
    "            self.hidsize = parameters.hidden_size\n",
    "            self.buffer_size = parameters.buffer_size\n",
    "            self.batch_size = parameters.batch_size\n",
    "            self.update_every = parameters.update_every\n",
    "            self.learning_rate = parameters.learning_rate\n",
    "            self.tau = parameters.tau\n",
    "            self.gamma = parameters.gamma\n",
    "            self.buffer_min_size = parameters.buffer_min_size\n",
    "\n",
    "        # Device\n",
    "        if parameters.use_gpu and torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda:0\")\n",
    "            # print(\"🐇 Using GPU\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            # print(\"🐢 Using CPU\")\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = DuelingQNetwork(state_size, action_size, hidsize1=self.hidsize, hidsize2=self.hidsize).to(self.device)\n",
    "\n",
    "        if not evaluation_mode:\n",
    "            self.qnetwork_target = copy.deepcopy(self.qnetwork_local)\n",
    "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.learning_rate)\n",
    "            self.memory = ReplayBuffer(action_size, self.buffer_size, self.batch_size, self.device)\n",
    "\n",
    "            self.t_step = 0\n",
    "            self.loss = 0.0\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        assert not self.evaluation_mode, \"Policy has been initialized for evaluation only.\"\n",
    "\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.buffer_min_size and len(self.memory) > self.batch_size:\n",
    "                self._learn()\n",
    "\n",
    "    def _learn(self):\n",
    "        experiences = self.memory.sample()\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        if self.double_dqn:\n",
    "            # Double DQN\n",
    "            q_best_action = self.qnetwork_local(next_states).max(1)[1]\n",
    "            q_targets_next = self.qnetwork_target(next_states).gather(1, q_best_action.unsqueeze(-1))\n",
    "        else:\n",
    "            # DQN\n",
    "            q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(-1)\n",
    "\n",
    "        # Compute Q targets for current states\n",
    "        q_targets = rewards + (self.gamma * q_targets_next * (1 - dones))\n",
    "\n",
    "        # Compute loss\n",
    "        self.loss = F.mse_loss(q_expected, q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        self._soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "\n",
    "    def _soft_update(self, local_model, target_model, tau):\n",
    "        # Soft update model parameters.\n",
    "        # θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.qnetwork_local.state_dict(), filename + \".local\")\n",
    "        torch.save(self.qnetwork_target.state_dict(), filename + \".target\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        if os.path.exists(filename + \".local\"):\n",
    "            self.qnetwork_local.load_state_dict(torch.load(filename + \".local\"))\n",
    "        if os.path.exists(filename + \".target\"):\n",
    "            self.qnetwork_target.load_state_dict(torch.load(filename + \".target\"))\n",
    "\n",
    "    def save_replay_buffer(self, filename):\n",
    "        memory = self.memory.memory\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(list(memory)[-500000:], f)\n",
    "\n",
    "    def load_replay_buffer(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.memory.memory = pickle.load(f)\n",
    "\n",
    "    def test(self):\n",
    "        self.act(np.array([[0] * self.state_size]))\n",
    "        self._learn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Fv2o6uY5Z1vh"
   },
   "outputs": [],
   "source": [
    "\n",
    "def max_lt(seq, val):\n",
    "    \"\"\"\n",
    "    Return greatest item in seq for which item < val applies.\n",
    "    None is returned if seq was empty or all items in seq were >= val.\n",
    "    \"\"\"\n",
    "    max = 0\n",
    "    idx = len(seq) - 1\n",
    "    while idx >= 0:\n",
    "        if seq[idx] < val and seq[idx] >= 0 and seq[idx] > max:\n",
    "            max = seq[idx]\n",
    "        idx -= 1\n",
    "    return max\n",
    "\n",
    "\n",
    "def min_gt(seq, val):\n",
    "    \"\"\"\n",
    "    Return smallest item in seq for which item > val applies.\n",
    "    None is returned if seq was empty or all items in seq were >= val.\n",
    "    \"\"\"\n",
    "    min = np.inf\n",
    "    idx = len(seq) - 1\n",
    "    while idx >= 0:\n",
    "        if seq[idx] >= val and seq[idx] < min:\n",
    "            min = seq[idx]\n",
    "        idx -= 1\n",
    "    return min\n",
    "\n",
    "\n",
    "def norm_obs_clip(obs, clip_min=-1, clip_max=1, fixed_radius=0, normalize_to_range=False):\n",
    "    \"\"\"\n",
    "    This function returns the difference between min and max value of an observation\n",
    "    :param obs: Observation that should be normalized\n",
    "    :param clip_min: min value where observation will be clipped\n",
    "    :param clip_max: max value where observation will be clipped\n",
    "    :return: returnes normalized and clipped observatoin\n",
    "    \"\"\"\n",
    "    if fixed_radius > 0:\n",
    "        max_obs = fixed_radius\n",
    "    else:\n",
    "        max_obs = max(1, max_lt(obs, 1000)) + 1\n",
    "\n",
    "    min_obs = 0  # min(max_obs, min_gt(obs, 0))\n",
    "    if normalize_to_range:\n",
    "        min_obs = min_gt(obs, 0)\n",
    "    if min_obs > max_obs:\n",
    "        min_obs = max_obs\n",
    "    if max_obs == min_obs:\n",
    "        return np.clip(np.array(obs) / max_obs, clip_min, clip_max)\n",
    "    norm = np.abs(max_obs - min_obs)\n",
    "    return np.clip((np.array(obs) - min_obs) / norm, clip_min, clip_max)\n",
    "\n",
    "\n",
    "def _split_node_into_feature_groups(node) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    data = np.zeros(6)\n",
    "    distance = np.zeros(1)\n",
    "    agent_data = np.zeros(4)\n",
    "\n",
    "    data[0] = node.dist_own_target_encountered\n",
    "    data[1] = node.dist_other_target_encountered\n",
    "    data[2] = node.dist_other_agent_encountered\n",
    "    data[3] = node.dist_potential_conflict\n",
    "    data[4] = node.dist_unusable_switch\n",
    "    data[5] = node.dist_to_next_branch\n",
    "\n",
    "    distance[0] = node.dist_min_to_target\n",
    "\n",
    "    agent_data[0] = node.num_agents_same_direction\n",
    "    agent_data[1] = node.num_agents_opposite_direction\n",
    "    agent_data[2] = node.num_agents_malfunctioning\n",
    "    agent_data[3] = node.speed_min_fractional\n",
    "\n",
    "    return data, distance, agent_data\n",
    "\n",
    "\n",
    "def _split_subtree_into_feature_groups(node, current_tree_depth: int, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    if node == -np.inf:\n",
    "        remaining_depth = max_tree_depth - current_tree_depth\n",
    "        # reference: https://stackoverflow.com/questions/515214/total-number-of-nodes-in-a-tree-data-structure\n",
    "        num_remaining_nodes = int((4 ** (remaining_depth + 1) - 1) / (4 - 1))\n",
    "        return [-np.inf] * num_remaining_nodes * 6, [-np.inf] * num_remaining_nodes, [-np.inf] * num_remaining_nodes * 4\n",
    "\n",
    "    data, distance, agent_data = _split_node_into_feature_groups(node)\n",
    "\n",
    "    if not node.childs:\n",
    "        return data, distance, agent_data\n",
    "\n",
    "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
    "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(node.childs[direction], current_tree_depth + 1, max_tree_depth)\n",
    "        data = np.concatenate((data, sub_data))\n",
    "        distance = np.concatenate((distance, sub_distance))\n",
    "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
    "\n",
    "    return data, distance, agent_data\n",
    "\n",
    "\n",
    "def split_tree_into_feature_groups(tree, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    This function splits the tree into three difference arrays of values\n",
    "    \"\"\"\n",
    "    data, distance, agent_data = _split_node_into_feature_groups(tree)\n",
    "\n",
    "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
    "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(tree.childs[direction], 1, max_tree_depth)\n",
    "        data = np.concatenate((data, sub_data))\n",
    "        distance = np.concatenate((distance, sub_distance))\n",
    "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
    "\n",
    "    return data, distance, agent_data\n",
    "\n",
    "\n",
    "def normalize_observation(observation, tree_depth: int, observation_radius=0):\n",
    "    \"\"\"\n",
    "    This function normalizes the observation used by the RL algorithm\n",
    "    \"\"\"\n",
    "    data, distance, agent_data = split_tree_into_feature_groups(observation, tree_depth)\n",
    "\n",
    "    data = norm_obs_clip(data, fixed_radius=observation_radius)\n",
    "    distance = norm_obs_clip(distance, normalize_to_range=True)\n",
    "    agent_data = np.clip(agent_data, -1, 1)\n",
    "    normalized_obs = np.concatenate((np.concatenate((data, distance)), agent_data))\n",
    "    return normalized_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OdzAav6yZ2nl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_agent(n_episodes):\n",
    "    # Environment parameters\n",
    "    n_agents = 1\n",
    "    x_dim = 50\n",
    "    y_dim = 50\n",
    "    n_cities = 2\n",
    "    max_rails_between_cities = 2\n",
    "    max_rails_in_city = 3\n",
    "    seed = 42\n",
    "\n",
    "    # Observation parameters\n",
    "    observation_tree_depth = 5\n",
    "    observation_radius = 10\n",
    "\n",
    "    # Exploration parameters\n",
    "    eps_start = 1.0\n",
    "    eps_end = 0.01\n",
    "    eps_decay = 0.997  # for 2500ts\n",
    "\n",
    "    # Set the seeds\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Observation builder\n",
    "    tree_observation = TreeObsForRailEnv(max_depth=observation_tree_depth)\n",
    "\n",
    "    # Setup the environment\n",
    "    env = RailEnv(\n",
    "        width=x_dim,\n",
    "        height=y_dim,\n",
    "#         rail_generator=sparse_rail_generator(\n",
    "#             max_num_cities=n_cities,\n",
    "#             seed=seed,\n",
    "#             grid_mode=False,\n",
    "#             max_rails_between_cities=max_rails_between_cities,\n",
    "#             max_rails_in_city=max_rails_in_city\n",
    "#         ),\n",
    "#         schedule_generator=sparse_schedule_generator(),\n",
    "        rail_generator=random_rail_generator(),\n",
    "        number_of_agents=n_agents,\n",
    "        obs_builder_object=tree_observation\n",
    "    )\n",
    "\n",
    "    env.reset(True, True)\n",
    "\n",
    "    # Calculate the state size given the depth of the tree observation and the number of features\n",
    "    n_features_per_node = env.obs_builder.observation_dim\n",
    "    n_nodes = 0\n",
    "    for i in range(observation_tree_depth + 1):\n",
    "        n_nodes += np.power(4, i)\n",
    "    state_size = n_features_per_node * n_nodes\n",
    "\n",
    "    # The action space of flatland is 5 discrete actions\n",
    "    action_size = 5\n",
    "\n",
    "    # Max number of steps per episode\n",
    "    # This is the official formula used during evaluations\n",
    "    max_steps = int(4 * 2 * (env.height + env.width + (n_agents / n_cities)))\n",
    "\n",
    "    action_dict = dict()\n",
    "\n",
    "    # And some variables to keep track of the progress\n",
    "    scores_window = deque(maxlen=100)  # todo smooth when rendering instead\n",
    "    completion_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    completion = []\n",
    "    action_count = [0] * action_size\n",
    "    agent_obs = [None] * env.get_num_agents()\n",
    "    agent_prev_obs = [None] * env.get_num_agents()\n",
    "    agent_prev_action = [2] * env.get_num_agents()\n",
    "    update_values = False\n",
    "\n",
    "    # Training parameters\n",
    "    training_parameters = {\n",
    "        'buffer_size': int(1e6),\n",
    "        'batch_size': 128,\n",
    "        'update_every': 8,\n",
    "        'learning_rate': 0.5e-4,\n",
    "        'tau': 1e-3,\n",
    "        'gamma': 0.99,\n",
    "        'buffer_min_size': 0,\n",
    "        'hidden_size': 512,\n",
    "        'use_gpu': True\n",
    "    }\n",
    "    \n",
    "    make_dir(CHECKPOINT_DIR)\n",
    "    \n",
    "    # write params to checkpoint dir\n",
    "    params_file = os.path.join(CHECKPOINT_DIR, 'params.txt')\n",
    "    with open(params_file, \"w\") as file1: \n",
    "        file1.write(f'n_agents={n_agents}' + '\\n')\n",
    "        file1.write(f'x_dim={x_dim}' + '\\n')\n",
    "        file1.write(f'y_dim={y_dim}' + '\\n')\n",
    "        file1.write(f'n_cities={n_cities}' + '\\n')\n",
    "        file1.write(f'max_rails_between_cities={max_rails_between_cities}' + '\\n')\n",
    "        file1.write(f'max_rails_in_city={max_rails_in_city}' + '\\n')\n",
    "        file1.write(f'seed={seed}' + '\\n')\n",
    "        file1.write(f'observation_tree_depth={observation_tree_depth}' + '\\n')\n",
    "        file1.write(f'observation_radius={observation_radius}' + '\\n')\n",
    "        file1.write(f'buffer_size={training_parameters[\"buffer_size\"]}' + '\\n')\n",
    "        file1.write(f'batch_size={training_parameters[\"batch_size\"]}' + '\\n')\n",
    "        file1.write(f'update_every={training_parameters[\"update_every\"]}' + '\\n')\n",
    "        file1.write(f'learning_rate={training_parameters[\"learning_rate\"]}' + '\\n')\n",
    "        file1.write(f'tau={training_parameters[\"tau\"]}' + '\\n')\n",
    "        file1.write(f'gamma={training_parameters[\"gamma\"]}' + '\\n')\n",
    "        file1.write(f'buffer_min_size={training_parameters[\"buffer_min_size\"]}' + '\\n')\n",
    "        file1.write(f'hidden_size={training_parameters[\"hidden_size\"]}' + '\\n')\n",
    "        file1.write(f'use_gpu={training_parameters[\"use_gpu\"]}' + '\\n')\n",
    "\n",
    "    # Double Dueling DQN policy\n",
    "    policy = DDDQNPolicy(state_size, action_size, Namespace(**training_parameters))\n",
    "\n",
    "    for episode_idx in range(1, n_episodes + 1):\n",
    "        score = 0\n",
    "\n",
    "        # Reset environment\n",
    "        obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
    "\n",
    "        # Build agent specific observations\n",
    "        for agent in env.get_agent_handles():\n",
    "            if obs[agent]:\n",
    "                agent_obs[agent] = normalize_observation(obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
    "                agent_prev_obs[agent] = agent_obs[agent].copy()\n",
    "\n",
    "        # Run episode\n",
    "        for step in range(max_steps - 1):\n",
    "            for agent in env.get_agent_handles():\n",
    "                if info['action_required'][agent]:\n",
    "                    # If an action is required, we want to store the obs at that step as well as the action\n",
    "                    update_values = True\n",
    "                    action = policy.act(agent_obs[agent], eps=eps_start)\n",
    "                    action_count[action] += 1\n",
    "                else:\n",
    "                    update_values = False\n",
    "                    action = 0\n",
    "                action_dict.update({agent: action})\n",
    "\n",
    "            # Environment step\n",
    "            next_obs, all_rewards, done, info = env.step(action_dict)\n",
    "\n",
    "            # Update replay buffer and train agent\n",
    "            for agent in range(env.get_num_agents()):\n",
    "                # Only update the values when we are done or when an action was taken and thus relevant information is present\n",
    "                if update_values or done[agent]:\n",
    "                    policy.step(agent_prev_obs[agent], agent_prev_action[agent], all_rewards[agent], agent_obs[agent], done[agent])\n",
    "\n",
    "                    agent_prev_obs[agent] = agent_obs[agent].copy()\n",
    "                    agent_prev_action[agent] = action_dict[agent]\n",
    "\n",
    "                if next_obs[agent]:\n",
    "                    agent_obs[agent] = normalize_observation(next_obs[agent], observation_tree_depth, observation_radius=10)\n",
    "\n",
    "                score += all_rewards[agent]\n",
    "\n",
    "            if done['__all__']:\n",
    "                break\n",
    "\n",
    "        # Epsilon decay\n",
    "        eps_start = max(eps_end, eps_decay * eps_start)\n",
    "\n",
    "        # Collection information about training\n",
    "        tasks_finished = np.sum([int(done[idx]) for idx in env.get_agent_handles()])\n",
    "        completion_window.append(tasks_finished / max(1, env.get_num_agents()))\n",
    "        scores_window.append(score / (max_steps * env.get_num_agents()))\n",
    "        completion.append((np.mean(completion_window)))\n",
    "        scores.append(np.mean(scores_window))\n",
    "        action_probs = action_count / np.sum(action_count)\n",
    "\n",
    "        if episode_idx % 100 == 0:\n",
    "            end = \"\\n\"\n",
    "            torch.save(policy.qnetwork_local, os.path.join(CHECKPOINT_DIR, str(episode_idx) + '.pth'))\n",
    "            action_count = [1] * action_size\n",
    "        else:\n",
    "            end = \" \"\n",
    "\n",
    "        print('\\rTraining {} agents on {}x{}\\t Episode {}\\t Average Score: {:.3f}\\tDones: {:.2f}%\\tEpsilon: {:.2f} \\t Action Probabilities: \\t {}'.format(\n",
    "            env.get_num_agents(),\n",
    "            x_dim, y_dim,\n",
    "            episode_idx,\n",
    "            np.mean(scores_window),\n",
    "            100 * np.mean(completion_window),\n",
    "            eps_start,\n",
    "            action_probs\n",
    "        ), end=end)\n",
    "\n",
    "    pickle_list(scores, os.path.join(CHECKPOINT_DIR, 'scores.pkl'))\n",
    "    pickle_list(completion, os.path.join(CHECKPOINT_DIR, 'completion.pkl'))\n",
    "\n",
    "    # Plot overall training progress at the end\n",
    "    plt.plot(scores)\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'scores.png'))\n",
    "    \n",
    "    plt.plot(completion)\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'completion.png'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "def get_timestamp():\n",
    "    ct = datetime.datetime.now()\n",
    "    return str(ct).split('.')[0].replace(' ', '').replace('-', '').replace(':', '')\n",
    "\n",
    "def pickle_list(l, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(l, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "II-EzccEYq1y",
    "outputId": "a3e0e8ee-df05-41fc-c180-c18b4bdbe471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1 agents on 50x50\t Episode 56\t Average Score: -0.906\tDones: 14.29%\tEpsilon: 0.85 \t Action Probabilities: \t [0.19758262 0.19694518 0.20513386 0.19893106 0.20140728] "
     ]
    }
   ],
   "source": [
    "n_episodes = 1000\n",
    "\n",
    "CHECKPOINT_DIR = '/scratch/ns4486/flatland-reinforcement-learning/single-agent/checkpoints'\n",
    "CHECKPOINT_DIR = os.path.join(CHECKPOINT_DIR, get_timestamp())\n",
    "\n",
    "\n",
    "train_agent(n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVuIiUAxa7nR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "flatland-single-agent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
