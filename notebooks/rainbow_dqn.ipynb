{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from collections import namedtuple, deque, Iterable\n",
    "\n",
    "import psutil\n",
    "from flatland.utils.rendertools import RenderTool\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from flatland.envs.rail_env import RailEnv, RailEnvActions\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.schedule_generators import sparse_schedule_generator\n",
    "from flatland.envs.observations import TreeObsForRailEnv\n",
    "\n",
    "from flatland.envs.malfunction_generators import malfunction_from_params, MalfunctionParameters\n",
    "from flatland.envs.predictions import ShortestPathPredictorForRailEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_lt(seq, val):\n",
    "    \"\"\"\n",
    "    Return greatest item in seq for which item < val applies.\n",
    "    None is returned if seq was empty or all items in seq were >= val.\n",
    "    \"\"\"\n",
    "    max = 0\n",
    "    idx = len(seq) - 1\n",
    "    while idx >= 0:\n",
    "        if seq[idx] < val and seq[idx] >= 0 and seq[idx] > max:\n",
    "            max = seq[idx]\n",
    "        idx -= 1\n",
    "    return max\n",
    "\n",
    "\n",
    "def min_gt(seq, val):\n",
    "    \"\"\"\n",
    "    Return smallest item in seq for which item > val applies.\n",
    "    None is returned if seq was empty or all items in seq were >= val.\n",
    "    \"\"\"\n",
    "    min = np.inf\n",
    "    idx = len(seq) - 1\n",
    "    while idx >= 0:\n",
    "        if seq[idx] >= val and seq[idx] < min:\n",
    "            min = seq[idx]\n",
    "        idx -= 1\n",
    "    return min\n",
    "\n",
    "\n",
    "def norm_obs_clip(obs, clip_min=-1, clip_max=1, fixed_radius=0, normalize_to_range=False):\n",
    "    \"\"\"\n",
    "    This function returns the difference between min and max value of an observation\n",
    "    :param obs: Observation that should be normalized\n",
    "    :param clip_min: min value where observation will be clipped\n",
    "    :param clip_max: max value where observation will be clipped\n",
    "    :return: returnes normalized and clipped observatoin\n",
    "    \"\"\"\n",
    "    if fixed_radius > 0:\n",
    "        max_obs = fixed_radius\n",
    "    else:\n",
    "        max_obs = max(1, max_lt(obs, 1000)) + 1\n",
    "\n",
    "    min_obs = 0  # min(max_obs, min_gt(obs, 0))\n",
    "    if normalize_to_range:\n",
    "        min_obs = min_gt(obs, 0)\n",
    "    if min_obs > max_obs:\n",
    "        min_obs = max_obs\n",
    "    if max_obs == min_obs:\n",
    "        return np.clip(np.array(obs) / max_obs, clip_min, clip_max)\n",
    "    norm = np.abs(max_obs - min_obs)\n",
    "    return np.clip((np.array(obs) - min_obs) / norm, clip_min, clip_max)\n",
    "\n",
    "\n",
    "def _split_node_into_feature_groups(node) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    data = np.zeros(6)\n",
    "    distance = np.zeros(1)\n",
    "    agent_data = np.zeros(4)\n",
    "\n",
    "    data[0] = node.dist_own_target_encountered\n",
    "    data[1] = node.dist_other_target_encountered\n",
    "    data[2] = node.dist_other_agent_encountered\n",
    "    data[3] = node.dist_potential_conflict\n",
    "    data[4] = node.dist_unusable_switch\n",
    "    data[5] = node.dist_to_next_branch\n",
    "\n",
    "    distance[0] = node.dist_min_to_target\n",
    "\n",
    "    agent_data[0] = node.num_agents_same_direction\n",
    "    agent_data[1] = node.num_agents_opposite_direction\n",
    "    agent_data[2] = node.num_agents_malfunctioning\n",
    "    agent_data[3] = node.speed_min_fractional\n",
    "\n",
    "    return data, distance, agent_data\n",
    "\n",
    "\n",
    "def _split_subtree_into_feature_groups(node, current_tree_depth: int, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    if node == -np.inf:\n",
    "        remaining_depth = max_tree_depth - current_tree_depth\n",
    "        # reference: https://stackoverflow.com/questions/515214/total-number-of-nodes-in-a-tree-data-structure\n",
    "        num_remaining_nodes = int((4 ** (remaining_depth + 1) - 1) / (4 - 1))\n",
    "        return [-np.inf] * num_remaining_nodes * 6, [-np.inf] * num_remaining_nodes, [-np.inf] * num_remaining_nodes * 4\n",
    "\n",
    "    data, distance, agent_data = _split_node_into_feature_groups(node)\n",
    "\n",
    "    if not node.childs:\n",
    "        return data, distance, agent_data\n",
    "\n",
    "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
    "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(node.childs[direction], current_tree_depth + 1, max_tree_depth)\n",
    "        data = np.concatenate((data, sub_data))\n",
    "        distance = np.concatenate((distance, sub_distance))\n",
    "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
    "\n",
    "    return data, distance, agent_data\n",
    "\n",
    "\n",
    "def split_tree_into_feature_groups(tree, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    This function splits the tree into three difference arrays of values\n",
    "    \"\"\"\n",
    "    data, distance, agent_data = _split_node_into_feature_groups(tree)\n",
    "\n",
    "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
    "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(tree.childs[direction], 1, max_tree_depth)\n",
    "        data = np.concatenate((data, sub_data))\n",
    "        distance = np.concatenate((distance, sub_distance))\n",
    "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
    "\n",
    "    return data, distance, agent_data\n",
    "\n",
    "\n",
    "def normalize_observation(observation, tree_depth: int, observation_radius=0):\n",
    "    \"\"\"\n",
    "    This function normalizes the observation used by the RL algorithm\n",
    "    \"\"\"\n",
    "    data, distance, agent_data = split_tree_into_feature_groups(observation, tree_depth)\n",
    "\n",
    "    data = norm_obs_clip(data, fixed_radius=observation_radius)\n",
    "    distance = norm_obs_clip(distance, normalize_to_range=True)\n",
    "    agent_data = np.clip(agent_data, -1, 1)\n",
    "    normalized_obs = np.concatenate((np.concatenate((data, distance)), agent_data))\n",
    "    return normalized_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, use_cuda, std_init=0.4):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.use_cuda     = use_cuda\n",
    "        self.in_features  = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init     = std_init\n",
    "        \n",
    "        self.weight_mu    = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
    "        \n",
    "        self.bias_mu    = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_cuda:\n",
    "            weight_epsilon = self.weight_epsilon.cuda()\n",
    "            bias_epsilon   = self.bias_epsilon.cuda()\n",
    "        else:\n",
    "            weight_epsilon = self.weight_epsilon\n",
    "            bias_epsilon   = self.bias_epsilon\n",
    "            \n",
    "        if self.training: \n",
    "            weight = self.weight_mu + self.weight_sigma.mul(Variable(weight_epsilon))\n",
    "            bias   = self.bias_mu   + self.bias_sigma.mul(Variable(bias_epsilon))\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias   = self.bias_mu\n",
    "        \n",
    "        return F.linear(x, weight, bias)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.weight_mu.size(1))\n",
    "        \n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.weight_sigma.size(1)))\n",
    "        \n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.bias_sigma.size(0)))\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        epsilon_in  = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        \n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n",
    "    \n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowDQN(nn.Module):\n",
    "    \"\"\"Dueling Q-network (https://arxiv.org/abs/1511.06581)\"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_actions, num_atoms, Vmin, Vmax):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "\n",
    "        self.num_inputs   = num_inputs\n",
    "        self.num_actions  = num_actions\n",
    "        self.num_atoms    = num_atoms\n",
    "        self.Vmin         = Vmin\n",
    "        self.Vmax         = Vmax\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, 256)\n",
    "        self.linear2 = nn.Linear(256, 512)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            USE_CUDA = True\n",
    "        else:\n",
    "            USE_CUDA = False\n",
    "        \n",
    "        self.noisy_value1 = NoisyLinear(512, 512, use_cuda=USE_CUDA)\n",
    "        self.noisy_value2 = NoisyLinear(512, self.num_atoms, use_cuda=USE_CUDA)\n",
    "        \n",
    "        self.noisy_advantage1 = NoisyLinear(512, 512, use_cuda=USE_CUDA)\n",
    "        self.noisy_advantage2 = NoisyLinear(512, self.num_atoms * self.num_actions, use_cuda=USE_CUDA)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        value = F.relu(self.noisy_value1(x))\n",
    "        value = self.noisy_value2(value)\n",
    "        \n",
    "        advantage = F.relu(self.noisy_advantage1(x))\n",
    "        advantage = self.noisy_advantage2(advantage)\n",
    "        \n",
    "        value     = value.view(batch_size, 1, self.num_atoms)\n",
    "        advantage = advantage.view(batch_size, self.num_actions, self.num_atoms)\n",
    "        \n",
    "        x = value + advantage - advantage.mean(1, keepdim=True)\n",
    "        x = F.softmax(x.view(-1, self.num_atoms)).view(-1, self.num_actions, self.num_atoms)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.noisy_value1.reset_noise()\n",
    "        self.noisy_value2.reset_noise()\n",
    "        self.noisy_advantage1.reset_noise()\n",
    "        self.noisy_advantage2.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, filename):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load(self, filename):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNPolicy(Policy):\n",
    "    \"\"\"Dueling Double DQN policy\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, parameters, evaluation_mode=False):\n",
    "        self.evaluation_mode = evaluation_mode\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.double_dqn = True\n",
    "        self.hidsize = 64\n",
    "\n",
    "        if not evaluation_mode:\n",
    "            self.hidsize = parameters.hidden_size\n",
    "            self.buffer_size = parameters.buffer_size\n",
    "            self.batch_size = parameters.batch_size\n",
    "            self.update_every = parameters.update_every\n",
    "            self.learning_rate = parameters.learning_rate\n",
    "            self.tau = parameters.tau\n",
    "            self.gamma = parameters.gamma\n",
    "            self.buffer_min_size = parameters.buffer_min_size\n",
    "            self.Vmin = parameters.Vmin\n",
    "            self.Vmax = parameters.Vmax\n",
    "\n",
    "        # Device\n",
    "        if parameters.use_gpu and torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda:0\")\n",
    "            # print(\"🐇 Using GPU\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            # print(\"🐢 Using CPU\")\n",
    "\n",
    "        # Q-Network\n",
    "        self.current_model = RainbowDQN(num_inputs = self.state_size, num_actions = self.action_size, num_atoms = self.hidsize, Vmin=self.Vmin, Vmax=self.Vmax).to(self.device)\n",
    "\n",
    "\n",
    "        if not evaluation_mode:\n",
    "            self.target_model = RainbowDQN(num_inputs = self.state_size, num_actions = self.action_size, num_atoms = self.hidsize, Vmin=self.Vmin, Vmax=self.Vmax).to(self.device)\n",
    "            self.update_target()\n",
    "            self.optimizer = optim.Adam(self.current_model.parameters(), lr=self.learning_rate)\n",
    "            self.memory = ReplayBuffer(action_size, self.buffer_size, self.batch_size, self.device)\n",
    "\n",
    "            self.t_step = 0\n",
    "            self.loss = 0.0\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.current_model.state_dict())\n",
    "\n",
    "    def projection_distribution(self, next_state, rewards, dones):\n",
    "        batch_size  = next_state.size(0)\n",
    "\n",
    "        rewards = rewards.squeeze(1)\n",
    "        dones = dones.squeeze(1)\n",
    "        \n",
    "        delta_z = float(self.Vmax - self.Vmin) / (self.hidsize - 1)\n",
    "        support = torch.linspace(self.Vmin, self.Vmax, self.hidsize)\n",
    "        \n",
    "        next_dist   = self.target_model(next_state).data.cpu() * support\n",
    "        next_action = next_dist.sum(2).max(1)[1]\n",
    "        next_action = next_action.unsqueeze(1).unsqueeze(1).expand(next_dist.size(0), 1, next_dist.size(2))\n",
    "        next_dist   = next_dist.gather(1, next_action).squeeze(1).to(self.device)\n",
    "\n",
    "        # print(next_state.shape)\n",
    "        # print(next_action.shape)\n",
    "        # print(next_dist.shape)\n",
    "        # print(rewards.shape)\n",
    "            \n",
    "        rewards = rewards.unsqueeze(1).expand_as(next_dist).to(self.device)\n",
    "        dones   = dones.unsqueeze(1).expand_as(next_dist).to(self.device)\n",
    "        support = support.unsqueeze(0).expand_as(next_dist).to(self.device)\n",
    "        \n",
    "        Tz = rewards + (1 - dones) * 0.99 * support\n",
    "        Tz = Tz.clamp(min=self.Vmin, max=self.Vmax)\n",
    "        b  = (Tz - self.Vmin) / delta_z\n",
    "        l  = b.floor().long()\n",
    "        u  = b.ceil().long()\n",
    "            \n",
    "        offset = torch.linspace(0, (batch_size - 1) * self.hidsize, batch_size).long()\\\n",
    "                        .unsqueeze(1).expand(batch_size, self.hidsize).to(self.device)\n",
    "\n",
    "        proj_dist = torch.zeros(next_dist.size()).to(self.device)   \n",
    "        proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n",
    "        proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n",
    "            \n",
    "        return proj_dist\n",
    "\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        # self.qnetwork_local.eval()\n",
    "        # with torch.no_grad():\n",
    "\n",
    "        dist = self.current_model(state).data.cpu()\n",
    "        dist = dist * torch.linspace(self.Vmin, self.Vmax, self.hidsize)\n",
    "        action = dist.sum(2).max(1)[1].numpy()[0]\n",
    "        # self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return action\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        assert not self.evaluation_mode, \"Policy has been initialized for evaluation only.\"\n",
    "\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.buffer_min_size and len(self.memory) > self.batch_size:\n",
    "                self._learn()\n",
    "\n",
    "    def _learn(self):\n",
    "        experiences = self.memory.sample()\n",
    "        state, action, reward, next_state, done = experiences\n",
    "\n",
    "        proj_dist = self.projection_distribution(next_state, reward, done)\n",
    "        \n",
    "        dist = self.current_model(state)\n",
    "        # print(dist.shape)\n",
    "        # print(action.shape)\n",
    "        action = action.unsqueeze(1).expand(self.batch_size, 1, self.hidsize)\n",
    "        dist = dist.gather(1, action).squeeze(1)\n",
    "        dist.data.clamp_(0.01, 0.99)\n",
    "        loss = -(Variable(proj_dist) * dist.log()).sum(1)\n",
    "        self.loss  = loss.mean()\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.current_model.reset_noise()\n",
    "        self.target_model.reset_noise()\n",
    "\n",
    "        # Update target network\n",
    "        self._soft_update(self.current_model, self.target_model, self.tau)\n",
    "\n",
    "    def _soft_update(self, local_model, target_model, tau):\n",
    "        # Soft update model parameters.\n",
    "        # θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.current_model.state_dict(), filename + \".local\")\n",
    "        torch.save(self.target_model.state_dict(), filename + \".target\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        if os.path.exists(filename + \".local\"):\n",
    "            self.current_model.load_state_dict(torch.load(filename + \".local\"))\n",
    "        if os.path.exists(filename + \".target\"):\n",
    "            self.target_model.load_state_dict(torch.load(filename + \".target\"))\n",
    "\n",
    "    def save_replay_buffer(self, filename):\n",
    "        memory = self.memory.memory\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(list(memory)[-500000:], f)\n",
    "\n",
    "    def load_replay_buffer(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.memory.memory = pickle.load(f)\n",
    "\n",
    "    def test(self):\n",
    "        self.act(np.array([[0] * self.state_size]))\n",
    "        self._learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, device):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = Experience(np.expand_dims(state, 0), action, reward, np.expand_dims(next_state, 0), done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(self.__v_stack_impr([e.state for e in experiences if e is not None])) \\\n",
    "            .float().to(self.device)\n",
    "        actions = torch.from_numpy(self.__v_stack_impr([e.action for e in experiences if e is not None])) \\\n",
    "            .long().to(self.device)\n",
    "        rewards = torch.from_numpy(self.__v_stack_impr([e.reward for e in experiences if e is not None])) \\\n",
    "            .float().to(self.device)\n",
    "        next_states = torch.from_numpy(self.__v_stack_impr([e.next_state for e in experiences if e is not None])) \\\n",
    "            .float().to(self.device)\n",
    "        dones = torch.from_numpy(self.__v_stack_impr([e.done for e in experiences if e is not None]).astype(np.uint8)) \\\n",
    "            .float().to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __v_stack_impr(self, states):\n",
    "        sub_dim = len(states[0][0]) if isinstance(states[0], Iterable) else 1\n",
    "        np_states = np.reshape(np.array(states), (len(states), sub_dim))\n",
    "        return np_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "\n",
    "\n",
    "class Timer(object):\n",
    "    \"\"\"\n",
    "    Utility to measure times.\n",
    "\n",
    "    TODO:\n",
    "    - add \"lap\" method to make it easier to measure average time (+std) when measuring the same thing multiple times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.total_time = 0.0\n",
    "        self.start_time = 0.0\n",
    "        self.end_time = 0.0\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = default_timer()\n",
    "\n",
    "    def end(self):\n",
    "        self.total_time += default_timer() - self.start_time\n",
    "\n",
    "    def get(self):\n",
    "        return self.total_time\n",
    "\n",
    "    def get_current(self):\n",
    "        return default_timer() - self.start_time\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rail_env(env_params, tree_observation):\n",
    "    n_agents = env_params.n_agents\n",
    "    x_dim = env_params.x_dim\n",
    "    y_dim = env_params.y_dim\n",
    "    n_cities = env_params.n_cities\n",
    "    max_rails_between_cities = env_params.max_rails_between_cities\n",
    "    max_rails_in_city = env_params.max_rails_in_city\n",
    "    seed = env_params.seed\n",
    "\n",
    "    # Break agents from time to time\n",
    "    malfunction_parameters = MalfunctionParameters(\n",
    "        malfunction_rate=env_params.malfunction_rate,\n",
    "        min_duration=20,\n",
    "        max_duration=50\n",
    "    )\n",
    "\n",
    "    return RailEnv(\n",
    "        width=x_dim, height=y_dim,\n",
    "        rail_generator=sparse_rail_generator(\n",
    "            max_num_cities=n_cities,\n",
    "            grid_mode=False,\n",
    "            max_rails_between_cities=max_rails_between_cities,\n",
    "            max_rails_in_city=max_rails_in_city\n",
    "        ),\n",
    "        schedule_generator=sparse_schedule_generator(),\n",
    "        number_of_agents=n_agents,\n",
    "        malfunction_generator_and_process_data=malfunction_from_params(malfunction_parameters),\n",
    "        obs_builder_object=tree_observation,\n",
    "        random_seed=seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(train_params, train_env_params, eval_env_params, obs_params):\n",
    "    # Environment parameters\n",
    "    n_agents = train_env_params.n_agents\n",
    "    x_dim = train_env_params.x_dim\n",
    "    y_dim = train_env_params.y_dim\n",
    "    n_cities = train_env_params.n_cities\n",
    "    max_rails_between_cities = train_env_params.max_rails_between_cities\n",
    "    max_rails_in_city = train_env_params.max_rails_in_city\n",
    "    seed = train_env_params.seed\n",
    "\n",
    "    # Unique ID for this training\n",
    "    now = datetime.datetime.now()\n",
    "    training_id = now.strftime('%y%m%d%H%M%S')\n",
    "\n",
    "    # Observation parameters\n",
    "    observation_tree_depth = obs_params.observation_tree_depth\n",
    "    observation_radius = obs_params.observation_radius\n",
    "    observation_max_path_depth = obs_params.observation_max_path_depth\n",
    "\n",
    "    # Training parameters\n",
    "    eps_start = train_params.eps_start\n",
    "    eps_end = train_params.eps_end\n",
    "    eps_decay = train_params.eps_decay\n",
    "    n_episodes = train_params.n_episodes\n",
    "    checkpoint_interval = train_params.checkpoint_interval\n",
    "    n_eval_episodes = train_params.n_evaluation_episodes\n",
    "    restore_replay_buffer = train_params.restore_replay_buffer\n",
    "    save_replay_buffer = train_params.save_replay_buffer\n",
    "\n",
    "    # Set the seeds\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Observation builder\n",
    "    predictor = ShortestPathPredictorForRailEnv(observation_max_path_depth)\n",
    "    tree_observation = TreeObsForRailEnv(max_depth=observation_tree_depth, predictor=predictor)\n",
    "\n",
    "    # Setup the environments\n",
    "    train_env = create_rail_env(train_env_params, tree_observation)\n",
    "    train_env.reset(regenerate_schedule=True, regenerate_rail=True)\n",
    "    eval_env = create_rail_env(eval_env_params, tree_observation)\n",
    "    eval_env.reset(regenerate_schedule=True, regenerate_rail=True)\n",
    "\n",
    "    # Setup renderer\n",
    "    if train_params.render:\n",
    "        env_renderer = RenderTool(train_env, gl=\"PGL\")\n",
    "\n",
    "    # Calculate the state size given the depth of the tree observation and the number of features\n",
    "    n_features_per_node = train_env.obs_builder.observation_dim\n",
    "    n_nodes = sum([np.power(4, i) for i in range(observation_tree_depth + 1)])\n",
    "    state_size = n_features_per_node * n_nodes\n",
    "\n",
    "    # The action space of flatland is 5 discrete actions\n",
    "    action_size = 5\n",
    "\n",
    "    # Max number of steps per episode\n",
    "    # This is the official formula used during evaluations\n",
    "    # See details in flatland.envs.schedule_generators.sparse_schedule_generator\n",
    "    # max_steps = int(4 * 2 * (env.height + env.width + (n_agents / n_cities)))\n",
    "    max_steps = train_env._max_episode_steps\n",
    "\n",
    "    action_count = [0] * action_size\n",
    "    action_dict = dict()\n",
    "    agent_obs = [None] * n_agents\n",
    "    agent_prev_obs = [None] * n_agents\n",
    "    agent_prev_action = [2] * n_agents\n",
    "    update_values = [False] * n_agents\n",
    "\n",
    "    # Smoothed values used as target for hyperparameter tuning\n",
    "    smoothed_normalized_score = -1.0\n",
    "    smoothed_eval_normalized_score = -1.0\n",
    "    smoothed_completion = 0.0\n",
    "    smoothed_eval_completion = 0.0\n",
    "\n",
    "    # Double Dueling DQN policy\n",
    "    policy = DDDQNPolicy(state_size, action_size, train_params)\n",
    "\n",
    "    # Loads existing replay buffer\n",
    "    if restore_replay_buffer:\n",
    "        try:\n",
    "            policy.load_replay_buffer(restore_replay_buffer)\n",
    "            policy.test()\n",
    "        except RuntimeError as e:\n",
    "            print(\"\\n🛑 Could't load replay buffer, were the experiences generated using the same tree depth?\")\n",
    "            print(e)\n",
    "            exit(1)\n",
    "\n",
    "    print(\"\\n💾 Replay buffer status: {}/{} experiences\".format(len(policy.memory.memory), train_params.buffer_size))\n",
    "\n",
    "    hdd = psutil.disk_usage('/')\n",
    "    if save_replay_buffer and (hdd.free / (2 ** 30)) < 500.0:\n",
    "        print(\"⚠️  Careful! Saving replay buffers will quickly consume a lot of disk space. You have {:.2f}gb left.\".format(hdd.free / (2 ** 30)))\n",
    "\n",
    "    # TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "    writer.add_hparams(vars(train_params), {})\n",
    "    writer.add_hparams(vars(train_env_params), {})\n",
    "    writer.add_hparams(vars(obs_params), {})\n",
    "\n",
    "    training_timer = Timer()\n",
    "    training_timer.start()\n",
    "\n",
    "    print(\"\\n🚉 Training {} trains on {}x{} grid for {} episodes, evaluating on {} episodes every {} episodes. Training id '{}'.\\n\".format(\n",
    "        train_env.get_num_agents(),\n",
    "        x_dim, y_dim,\n",
    "        n_episodes,\n",
    "        n_eval_episodes,\n",
    "        checkpoint_interval,\n",
    "        training_id\n",
    "    ))\n",
    "\n",
    "    make_dir(CHECKPOINT_DIR)\n",
    "    params_file = os.path.join(CHECKPOINT_DIR, 'params.txt')\n",
    "    write_params_to_file(train_params, train_env_params, obs_params, params_file)\n",
    "\n",
    "    score_list = []\n",
    "    completion_list = []\n",
    "\n",
    "    for episode_idx in range(n_episodes + 1):\n",
    "        step_timer = Timer()\n",
    "        reset_timer = Timer()\n",
    "        learn_timer = Timer()\n",
    "        preproc_timer = Timer()\n",
    "        inference_timer = Timer()\n",
    "\n",
    "        # Reset environment\n",
    "        reset_timer.start()\n",
    "        obs, info = train_env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
    "        reset_timer.end()\n",
    "\n",
    "        if train_params.render:\n",
    "            env_renderer.set_new_rail()\n",
    "\n",
    "        score = 0\n",
    "        nb_steps = 0\n",
    "        actions_taken = []\n",
    "\n",
    "        # Build initial agent-specific observations\n",
    "        for agent in train_env.get_agent_handles():\n",
    "            if obs[agent]:\n",
    "                agent_obs[agent] = normalize_observation(obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
    "                agent_prev_obs[agent] = agent_obs[agent].copy()\n",
    "\n",
    "        # Run episode\n",
    "        for step in range(max_steps - 1):\n",
    "            inference_timer.start()\n",
    "            for agent in train_env.get_agent_handles():\n",
    "                if info['action_required'][agent]:\n",
    "                    update_values[agent] = True\n",
    "                    action = policy.act(agent_obs[agent], eps=eps_start)\n",
    "\n",
    "                    action_count[action] += 1\n",
    "                    actions_taken.append(action)\n",
    "                else:\n",
    "                    # An action is not required if the train hasn't joined the railway network,\n",
    "                    # if it already reached its target, or if is currently malfunctioning.\n",
    "                    update_values[agent] = False\n",
    "                    action = 0\n",
    "                action_dict.update({agent: action})\n",
    "            inference_timer.end()\n",
    "\n",
    "            # Environment step\n",
    "            step_timer.start()\n",
    "            next_obs, all_rewards, done, info = train_env.step(action_dict)\n",
    "            step_timer.end()\n",
    "\n",
    "            # Render an episode at some interval\n",
    "            if train_params.render and episode_idx % checkpoint_interval == 0:\n",
    "                env_renderer.render_env(\n",
    "                    show=True,\n",
    "                    frames=False,\n",
    "                    show_observations=False,\n",
    "                    show_predictions=False\n",
    "                )\n",
    "\n",
    "            # Update replay buffer and train agent\n",
    "            for agent in train_env.get_agent_handles():\n",
    "                if update_values[agent] or done['__all__']:\n",
    "                    # Only learn from timesteps where somethings happened\n",
    "                    learn_timer.start()\n",
    "                    policy.step(agent_prev_obs[agent], agent_prev_action[agent], all_rewards[agent], agent_obs[agent], done[agent])\n",
    "                    learn_timer.end()\n",
    "\n",
    "                    agent_prev_obs[agent] = agent_obs[agent].copy()\n",
    "                    agent_prev_action[agent] = action_dict[agent]\n",
    "\n",
    "                # Preprocess the new observations\n",
    "                if next_obs[agent]:\n",
    "                    preproc_timer.start()\n",
    "                    agent_obs[agent] = normalize_observation(next_obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
    "                    preproc_timer.end()\n",
    "\n",
    "                score += all_rewards[agent]\n",
    "\n",
    "            nb_steps = step\n",
    "\n",
    "            if done['__all__']:\n",
    "                break\n",
    "\n",
    "        # Epsilon decay\n",
    "        eps_start = max(eps_end, eps_decay * eps_start)\n",
    "\n",
    "        # Collect information about training\n",
    "        tasks_finished = sum(done[idx] for idx in train_env.get_agent_handles())\n",
    "        completion = tasks_finished / max(1, train_env.get_num_agents())\n",
    "        normalized_score = score / (max_steps * train_env.get_num_agents())\n",
    "        action_probs = action_count / np.sum(action_count)\n",
    "        action_count = [1] * action_size\n",
    "\n",
    "        smoothing = 0.99\n",
    "        smoothed_normalized_score = smoothed_normalized_score * smoothing + normalized_score * (1.0 - smoothing)\n",
    "        smoothed_completion = smoothed_completion * smoothing + completion * (1.0 - smoothing)\n",
    "\n",
    "        score_list.append(smoothed_normalized_score)\n",
    "        completion_list.append(smoothed_completion)\n",
    "        \n",
    "        score_path = os.path.join(CHECKPOINT_DIR, 'score.csv')\n",
    "        completion_path = os.path.join(CHECKPOINT_DIR, 'completion.csv')\n",
    "        normalized_score_path = os.path.join(CHECKPOINT_DIR, 'normalized_score.csv')\n",
    "        smoothed_normalized_score_path = os.path.join(CHECKPOINT_DIR, 'smoothed_normalized_score.csv')\n",
    "        smoothed_completion_path = os.path.join(CHECKPOINT_DIR, 'smoothed_completion.csv')\n",
    "        \n",
    "        with open(score_path, 'a+') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([score])\n",
    "        with open(completion_path, 'a+') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([completion])\n",
    "        with open(normalized_score_path, 'a+') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([normalized_score])\n",
    "        with open(smoothed_normalized_score_path, 'a+') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([smoothed_normalized_score])\n",
    "        with open(smoothed_completion_path, 'a+') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([smoothed_completion])\n",
    "            \n",
    "\n",
    "        # Print logs\n",
    "        if episode_idx % checkpoint_interval == 0:\n",
    "            torch.save(policy.current_model, os.path.join(CHECKPOINT_DIR, str(episode_idx) + '.pth'))\n",
    "\n",
    "            if save_replay_buffer:\n",
    "                policy.save_replay_buffer('replay_buffers/' + training_id + '-' + str(episode_idx) + '.pkl')\n",
    "\n",
    "            if train_params.render:\n",
    "                env_renderer.close_window()\n",
    "\n",
    "        print(\n",
    "            '\\r🚂 Episode {}'\n",
    "            '\\t 🏆 Score: {:.3f}'\n",
    "            ' Avg: {:.3f}'\n",
    "            '\\t 💯 Done: {:.2f}%'\n",
    "            ' Avg: {:.2f}%'\n",
    "            '\\t 🎲 Epsilon: {:.3f} '\n",
    "            '\\t 🔀 Action Probs: {}'.format(\n",
    "                episode_idx,\n",
    "                normalized_score,\n",
    "                smoothed_normalized_score,\n",
    "                100 * completion,\n",
    "                100 * smoothed_completion,\n",
    "                eps_start,\n",
    "                format_action_prob(action_probs)\n",
    "            ), end=\" \")\n",
    "\n",
    "        # Evaluate policy and log results at some interval\n",
    "        if episode_idx % checkpoint_interval == 0 and n_eval_episodes > 0:\n",
    "            scores, completions, nb_steps_eval = eval_policy(eval_env, policy, train_params, obs_params)\n",
    "\n",
    "            writer.add_scalar(\"evaluation/scores_min\", np.min(scores), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/scores_max\", np.max(scores), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/scores_mean\", np.mean(scores), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/scores_std\", np.std(scores), episode_idx)\n",
    "            writer.add_histogram(\"evaluation/scores\", np.array(scores), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/completions_min\", np.min(completions), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/completions_max\", np.max(completions), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/completions_mean\", np.mean(completions), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/completions_std\", np.std(completions), episode_idx)\n",
    "            writer.add_histogram(\"evaluation/completions\", np.array(completions), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/nb_steps_min\", np.min(nb_steps_eval), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/nb_steps_max\", np.max(nb_steps_eval), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/nb_steps_mean\", np.mean(nb_steps_eval), episode_idx)\n",
    "            writer.add_scalar(\"evaluation/nb_steps_std\", np.std(nb_steps_eval), episode_idx)\n",
    "            writer.add_histogram(\"evaluation/nb_steps\", np.array(nb_steps_eval), episode_idx)\n",
    "\n",
    "            smoothing = 0.9\n",
    "            smoothed_eval_normalized_score = smoothed_eval_normalized_score * smoothing + np.mean(scores) * (1.0 - smoothing)\n",
    "            smoothed_eval_completion = smoothed_eval_completion * smoothing + np.mean(completions) * (1.0 - smoothing)\n",
    "            writer.add_scalar(\"evaluation/smoothed_score\", smoothed_eval_normalized_score, episode_idx)\n",
    "            writer.add_scalar(\"evaluation/smoothed_completion\", smoothed_eval_completion, episode_idx)\n",
    "\n",
    "        # Save logs to tensorboard\n",
    "        writer.add_scalar(\"training/score\", normalized_score, episode_idx)\n",
    "        writer.add_scalar(\"training/smoothed_score\", smoothed_normalized_score, episode_idx)\n",
    "        writer.add_scalar(\"training/completion\", np.mean(completion), episode_idx)\n",
    "        writer.add_scalar(\"training/smoothed_completion\", np.mean(smoothed_completion), episode_idx)\n",
    "        writer.add_scalar(\"training/nb_steps\", nb_steps, episode_idx)\n",
    "        writer.add_histogram(\"actions/distribution\", np.array(actions_taken), episode_idx)\n",
    "        writer.add_scalar(\"actions/nothing\", action_probs[RailEnvActions.DO_NOTHING], episode_idx)\n",
    "        writer.add_scalar(\"actions/left\", action_probs[RailEnvActions.MOVE_LEFT], episode_idx)\n",
    "        writer.add_scalar(\"actions/forward\", action_probs[RailEnvActions.MOVE_FORWARD], episode_idx)\n",
    "        writer.add_scalar(\"actions/right\", action_probs[RailEnvActions.MOVE_RIGHT], episode_idx)\n",
    "        writer.add_scalar(\"actions/stop\", action_probs[RailEnvActions.STOP_MOVING], episode_idx)\n",
    "        writer.add_scalar(\"training/epsilon\", eps_start, episode_idx)\n",
    "        writer.add_scalar(\"training/buffer_size\", len(policy.memory), episode_idx)\n",
    "        writer.add_scalar(\"training/loss\", policy.loss, episode_idx)\n",
    "        writer.add_scalar(\"timer/reset\", reset_timer.get(), episode_idx)\n",
    "        writer.add_scalar(\"timer/step\", step_timer.get(), episode_idx)\n",
    "        writer.add_scalar(\"timer/learn\", learn_timer.get(), episode_idx)\n",
    "        writer.add_scalar(\"timer/preproc\", preproc_timer.get(), episode_idx)\n",
    "        writer.add_scalar(\"timer/total\", training_timer.get_current(), episode_idx)\n",
    "\n",
    "    pickle_list(score_list, os.path.join(CHECKPOINT_DIR, 'scores.pkl'))\n",
    "    pickle_list(completion_list, os.path.join(CHECKPOINT_DIR, 'completion.pkl'))\n",
    "\n",
    "    plt.plot(score_list)\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'scores.png'))\n",
    "    \n",
    "    plt.plot(completion_list)\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'completion.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_action_prob(action_probs):\n",
    "    action_probs = np.round(action_probs, 3)\n",
    "    actions = [\"↻\", \"←\", \"↑\", \"→\", \"◼\"]\n",
    "\n",
    "    buffer = \"\"\n",
    "    for action, action_prob in zip(actions, action_probs):\n",
    "        buffer += action + \" \" + \"{:.3f}\".format(action_prob) + \" \"\n",
    "\n",
    "    return buffer\n",
    "\n",
    "\n",
    "def eval_policy(env, policy, train_params, obs_params):\n",
    "    n_eval_episodes = train_params.n_evaluation_episodes\n",
    "    max_steps = env._max_episode_steps\n",
    "    tree_depth = obs_params.observation_tree_depth\n",
    "    observation_radius = obs_params.observation_radius\n",
    "\n",
    "    action_dict = dict()\n",
    "    scores = []\n",
    "    completions = []\n",
    "    nb_steps = []\n",
    "\n",
    "    for episode_idx in range(n_eval_episodes):\n",
    "        agent_obs = [None] * env.get_num_agents()\n",
    "        score = 0.0\n",
    "\n",
    "        obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
    "\n",
    "        final_step = 0\n",
    "\n",
    "        for step in range(max_steps - 1):\n",
    "            for agent in env.get_agent_handles():\n",
    "                if obs[agent]:\n",
    "                    agent_obs[agent] = normalize_observation(obs[agent], tree_depth=tree_depth, observation_radius=observation_radius)\n",
    "\n",
    "                action = 0\n",
    "                if info['action_required'][agent]:\n",
    "                    action = policy.act(agent_obs[agent], eps=0.0)\n",
    "                action_dict.update({agent: action})\n",
    "\n",
    "            obs, all_rewards, done, info = env.step(action_dict)\n",
    "\n",
    "            for agent in env.get_agent_handles():\n",
    "                score += all_rewards[agent]\n",
    "\n",
    "            final_step = step\n",
    "\n",
    "            if done['__all__']:\n",
    "                break\n",
    "\n",
    "        normalized_score = score / (max_steps * env.get_num_agents())\n",
    "        scores.append(normalized_score)\n",
    "\n",
    "        tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
    "        completion = tasks_finished / max(1, env.get_num_agents())\n",
    "        completions.append(completion)\n",
    "\n",
    "        nb_steps.append(final_step)\n",
    "\n",
    "    print(\"\\t✅ Eval: score {:.3f} done {:.1f}%\".format(np.mean(scores), np.mean(completions) * 100.0))\n",
    "\n",
    "    return scores, completions, nb_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "def get_timestamp():\n",
    "    ct = datetime.datetime.now()\n",
    "    return str(ct).split('.')[0].replace(' ', '').replace('-', '').replace(':', '')\n",
    "\n",
    "def pickle_list(l, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(l, f)\n",
    "\n",
    "def write_params_to_file(train_params, train_env_params, obs_params, params_file):\n",
    "    with open(params_file, \"w\") as file1:\n",
    "        file1.write(f'n_episodes={train_params.n_episodes}' + '\\n')\n",
    "        file1.write(f'training_env_config={train_params.training_env_config}' + '\\n')\n",
    "        file1.write(f'evaluation_env_config={train_params.evaluation_env_config}' + '\\n')\n",
    "        file1.write(f'n_evaluation_episodes={train_params.n_evaluation_episodes}' + '\\n')\n",
    "        file1.write(f'checkpoint_interval={train_params.checkpoint_interval}' + '\\n')\n",
    "        file1.write(f'eps_start={train_params.eps_start}' + '\\n')\n",
    "        file1.write(f'eps_end={train_params.eps_end}' + '\\n')\n",
    "        file1.write(f'eps_decay={train_params.eps_decay}' + '\\n')\n",
    "        file1.write(f'buffer_size={train_params.buffer_size}' + '\\n')\n",
    "        file1.write(f'buffer_min_size={train_params.buffer_min_size}' + '\\n')\n",
    "        file1.write(f'restore_replay_buffer={train_params.restore_replay_buffer}' + '\\n')\n",
    "        file1.write(f'save_replay_buffer={train_params.save_replay_buffer}' + '\\n')\n",
    "        file1.write(f'batch_size={train_params.batch_size}' + '\\n')\n",
    "        file1.write(f'gamma={train_params.gamma}' + '\\n')\n",
    "        file1.write(f'tau={train_params.tau}' + '\\n')\n",
    "        file1.write(f'learning_rate={train_params.learning_rate}' + '\\n')\n",
    "        file1.write(f'hidden_size={train_params.hidden_size}' + '\\n')\n",
    "        file1.write(f'update_every={train_params.update_every}' + '\\n')\n",
    "        file1.write(f'use_gpu={train_params.use_gpu}' + '\\n')\n",
    "        file1.write(f'num_threads={train_params.num_threads}' + '\\n')\n",
    "        file1.write(f'render={train_params.render}' + '\\n')\n",
    "        file1.write(f'n_agents={train_env_params.n_agents}' + '\\n')\n",
    "        file1.write(f'x_dim={train_env_params.x_dim}' + '\\n')\n",
    "        file1.write(f'y_dim={train_env_params.y_dim}' + '\\n')\n",
    "        file1.write(f'n_cities={train_env_params.n_cities}' + '\\n')\n",
    "        file1.write(f'max_rails_between_cities={train_env_params.max_rails_between_cities}' + '\\n')\n",
    "        file1.write(f'max_rails_in_city={train_env_params.max_rails_in_city}' + '\\n')\n",
    "        file1.write(f'malfunction_rate={train_env_params.malfunction_rate}' + '\\n')\n",
    "        file1.write(f'seed={train_env_params.seed}' + '\\n')\n",
    "        file1.write(f'observation_tree_depth={obs_params.observation_tree_depth}' + '\\n')\n",
    "        file1.write(f'observation_radius={obs_params.observation_radius}' + '\\n')\n",
    "        file1.write(f'observation_max_path_depth={obs_params.observation_max_path_depth}' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training parameters:\n",
      "{'Vmax': 10,\n",
      " 'Vmin': -10,\n",
      " 'batch_size': 128,\n",
      " 'buffer_min_size': 0,\n",
      " 'buffer_size': 100000,\n",
      " 'checkpoint_interval': 100,\n",
      " 'eps_decay': 0.99,\n",
      " 'eps_end': 0.01,\n",
      " 'eps_start': 1.0,\n",
      " 'evaluation_env_config': 0,\n",
      " 'gamma': 0.99,\n",
      " 'hidden_size': 128,\n",
      " 'learning_rate': 5e-05,\n",
      " 'n_episodes': 2500,\n",
      " 'n_evaluation_episodes': 25,\n",
      " 'num_threads': 1,\n",
      " 'render': False,\n",
      " 'restore_replay_buffer': '',\n",
      " 'save_replay_buffer': False,\n",
      " 'tau': 0.001,\n",
      " 'training_env_config': 0,\n",
      " 'update_every': 8,\n",
      " 'use_gpu': True}\n",
      "\n",
      "Training environment parameters (Test_0):\n",
      "{'malfunction_rate': 0.02,\n",
      " 'max_rails_between_cities': 2,\n",
      " 'max_rails_in_city': 3,\n",
      " 'n_agents': 2,\n",
      " 'n_cities': 2,\n",
      " 'seed': 0,\n",
      " 'x_dim': 25,\n",
      " 'y_dim': 25}\n",
      "\n",
      "Evaluation environment parameters (Test_0):\n",
      "{'malfunction_rate': 0.02,\n",
      " 'max_rails_between_cities': 2,\n",
      " 'max_rails_in_city': 3,\n",
      " 'n_agents': 2,\n",
      " 'n_cities': 2,\n",
      " 'seed': 0,\n",
      " 'x_dim': 25,\n",
      " 'y_dim': 25}\n",
      "\n",
      "Observation parameters:\n",
      "{'observation_max_path_depth': 30,\n",
      " 'observation_radius': 30,\n",
      " 'observation_tree_depth': 3}\n",
      "DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator\n",
      "DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator\n",
      "\n",
      "💾 Replay buffer status: 0/100000 experiences\n",
      "\n",
      "🚉 Training 2 trains on 25x25 grid for 2500 episodes, evaluating on 25 episodes every 100 episodes. Training id '201211065951'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-b93f632ec4c0>:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(x.view(-1, self.num_atoms)).view(-1, self.num_actions, self.num_atoms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚂 Episode 0\t 🏆 Score: -0.472 Avg: -0.995\t 💯 Done: 100.00% Avg: 1.00%\t 🎲 Epsilon: 0.990 \t 🔀 Action Probs: ↻ 0.213 ← 0.228 ↑ 0.191 → 0.194 ◼ 0.174  \t✅ Eval: score -0.910 done 12.0%\n",
      "🚂 Episode 100\t 🏆 Score: -0.630 Avg: -0.819\t 💯 Done: 50.00% Avg: 28.82%\t 🎲 Epsilon: 0.362 \t 🔀 Action Probs: ↻ 0.073 ← 0.096 ↑ 0.083 → 0.079 ◼ 0.669  \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 200\t 🏆 Score: -0.613 Avg: -0.812\t 💯 Done: 50.00% Avg: 30.59%\t 🎲 Epsilon: 0.133 \t 🔀 Action Probs: ↻ 0.721 ← 0.038 ↑ 0.190 → 0.026 ◼ 0.026   \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 300\t 🏆 Score: -0.998 Avg: -0.774\t 💯 Done: 0.00% Avg: 34.16%\t 🎲 Epsilon: 0.049 \t 🔀 Action Probs: ↻ 0.168 ← 0.047 ↑ 0.054 → 0.042 ◼ 0.689    \t✅ Eval: score -0.960 done 4.0%\n",
      "🚂 Episode 400\t 🏆 Score: -0.654 Avg: -0.843\t 💯 Done: 50.00% Avg: 26.25%\t 🎲 Epsilon: 0.018 \t 🔀 Action Probs: ↻ 0.239 ← 0.059 ↑ 0.222 → 0.102 ◼ 0.378   \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 500\t 🏆 Score: -0.998 Avg: -0.859\t 💯 Done: 0.00% Avg: 25.14%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.114 ← 0.032 ↑ 0.082 → 0.011 ◼ 0.762    \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 600\t 🏆 Score: -0.998 Avg: -0.863\t 💯 Done: 0.00% Avg: 26.92%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.252 ← 0.063 ↑ 0.167 → 0.041 ◼ 0.477    \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 700\t 🏆 Score: -0.998 Avg: -0.894\t 💯 Done: 0.00% Avg: 24.50%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.076 ← 0.008 ↑ 0.114 → 0.068 ◼ 0.735    \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 800\t 🏆 Score: -0.816 Avg: -0.912\t 💯 Done: 50.00% Avg: 21.55%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.092 ← 0.057 ↑ 0.027 → 0.027 ◼ 0.798   \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 900\t 🏆 Score: -0.724 Avg: -0.898\t 💯 Done: 50.00% Avg: 24.58%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.141 ← 0.028 ↑ 0.030 → 0.042 ◼ 0.759   \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 1000\t 🏆 Score: -0.998 Avg: -0.868\t 💯 Done: 0.00% Avg: 27.87%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.368 ← 0.055 ↑ 0.233 → 0.006 ◼ 0.337   \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 1100\t 🏆 Score: -0.092 Avg: -0.804\t 💯 Done: 100.00% Avg: 35.72%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.381 ← 0.190 ↑ 0.321 → 0.012 ◼ 0.095  \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 1200\t 🏆 Score: -0.998 Avg: -0.704\t 💯 Done: 0.00% Avg: 46.83%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.158 ← 0.064 ↑ 0.008 → 0.376 ◼ 0.395    \t✅ Eval: score -0.694 done 36.0%\n",
      "🚂 Episode 1300\t 🏆 Score: -0.538 Avg: -0.677\t 💯 Done: 100.00% Avg: 50.81%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.154 ← 0.190 ↑ 0.087 → 0.112 ◼ 0.456  \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 1400\t 🏆 Score: -0.998 Avg: -0.671\t 💯 Done: 0.00% Avg: 51.10%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.087 ← 0.221 ↑ 0.066 → 0.147 ◼ 0.480    \t✅ Eval: score -0.830 done 20.0%\n",
      "🚂 Episode 1500\t 🏆 Score: -0.998 Avg: -0.705\t 💯 Done: 0.00% Avg: 45.94%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.100 ← 0.470 ↑ 0.130 → 0.010 ◼ 0.290    \t✅ Eval: score -0.660 done 40.0%\n",
      "🚂 Episode 1600\t 🏆 Score: -0.191 Avg: -0.672\t 💯 Done: 100.00% Avg: 51.17%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.218 ← 0.103 ↑ 0.109 → 0.194 ◼ 0.376  \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 1700\t 🏆 Score: -0.998 Avg: -0.654\t 💯 Done: 0.00% Avg: 51.97%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.446 ← 0.037 ↑ 0.294 → 0.028 ◼ 0.194    \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 1800\t 🏆 Score: -0.756 Avg: -0.605\t 💯 Done: 50.00% Avg: 57.95%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.125 ← 0.352 ↑ 0.037 → 0.218 ◼ 0.268   \t✅ Eval: score -0.590 done 48.0%\n",
      "🚂 Episode 1900\t 🏆 Score: -0.494 Avg: -0.632\t 💯 Done: 100.00% Avg: 52.98%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.148 ← 0.323 ↑ 0.053 → 0.182 ◼ 0.294  \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 2000\t 🏆 Score: -0.086 Avg: -0.636\t 💯 Done: 100.00% Avg: 54.25%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.063 ← 0.253 ↑ 0.025 → 0.127 ◼ 0.532  \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 2100\t 🏆 Score: -0.998 Avg: -0.636\t 💯 Done: 0.00% Avg: 56.56%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.167 ← 0.500 ↑ 0.015 → 0.197 ◼ 0.121    \t✅ Eval: score -0.811 done 22.0%\n",
      "🚂 Episode 2200\t 🏆 Score: -0.998 Avg: -0.649\t 💯 Done: 0.00% Avg: 55.54%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.215 ← 0.278 ↑ 0.143 → 0.058 ◼ 0.305    \t✅ Eval: score -0.730 done 32.0%\n",
      "🚂 Episode 2300\t 🏆 Score: -0.624 Avg: -0.617\t 💯 Done: 50.00% Avg: 58.26%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.134 ← 0.433 ↑ 0.021 → 0.212 ◼ 0.200   \t✅ Eval: score -0.998 done 0.0%\n",
      "🚂 Episode 2400\t 🏆 Score: -0.243 Avg: -0.613\t 💯 Done: 100.00% Avg: 59.02%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.237 ← 0.271 ↑ 0.077 → 0.101 ◼ 0.314  \t✅ Eval: score -0.716 done 34.0%\n",
      "🚂 Episode 2500\t 🏆 Score: -0.998 Avg: -0.586\t 💯 Done: 0.00% Avg: 61.59%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.045 ← 0.191 ↑ 0.091 → 0.273 ◼ 0.400    \t✅ Eval: score -0.960 done 4.0%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-18aca0bbbfcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OMP_NUM_THREADS\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtraining_env_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mevaluation_env_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mobs_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-7b44f0ecf854>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(train_params, train_env_params, eval_env_params, obs_params)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0mpickle_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'completion.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scores.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_DIR = '/scratch/prs392/flatland_reinforcement_learning/single_agent/rainbow_dqn/checkpoints'\n",
    "# CHECKPOINT_DIR = '.'\n",
    "CHECKPOINT_DIR = os.path.join(CHECKPOINT_DIR, get_timestamp())\n",
    "\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "training_params = Object()\n",
    "training_params.n_episodes = 2500\n",
    "training_params.training_env_config = 0\n",
    "training_params.evaluation_env_config = 0\n",
    "training_params.n_evaluation_episodes = 25\n",
    "training_params.checkpoint_interval = 100\n",
    "training_params.eps_start = 1.0\n",
    "training_params.eps_end = 0.01\n",
    "training_params.eps_decay = 0.99\n",
    "training_params.buffer_size = int(1e5)\n",
    "training_params.buffer_min_size = 0\n",
    "training_params.restore_replay_buffer = \"\"\n",
    "training_params.save_replay_buffer = False\n",
    "training_params.batch_size = 128\n",
    "training_params.gamma = 0.99\n",
    "training_params.tau = 1e-3\n",
    "training_params.learning_rate = 0.5e-4\n",
    "training_params.hidden_size = 128\n",
    "training_params.update_every = 8\n",
    "training_params.use_gpu = True\n",
    "training_params.num_threads = 1\n",
    "training_params.render = False\n",
    "training_params.Vmin = -10\n",
    "training_params.Vmax = +10\n",
    "\n",
    "env_params = [\n",
    "    {\n",
    "        # Test_0\n",
    "        \"n_agents\": 2,\n",
    "        \"x_dim\": 25,\n",
    "        \"y_dim\": 25,\n",
    "        \"n_cities\": 2,\n",
    "        \"max_rails_between_cities\": 2,\n",
    "        \"max_rails_in_city\": 3,\n",
    "        \"malfunction_rate\": 1 / 50,\n",
    "        \"seed\": 0\n",
    "    },\n",
    "    {\n",
    "        # Test_1\n",
    "        \"n_agents\": 10,\n",
    "        \"x_dim\": 30,\n",
    "        \"y_dim\": 30,\n",
    "        \"n_cities\": 2,\n",
    "        \"max_rails_between_cities\": 2,\n",
    "        \"max_rails_in_city\": 3,\n",
    "        \"malfunction_rate\": 1 / 100,\n",
    "        \"seed\": 0\n",
    "    },\n",
    "    {\n",
    "        # Test_2\n",
    "        \"n_agents\": 20,\n",
    "        \"x_dim\": 30,\n",
    "        \"y_dim\": 30,\n",
    "        \"n_cities\": 3,\n",
    "        \"max_rails_between_cities\": 2,\n",
    "        \"max_rails_in_city\": 3,\n",
    "        \"malfunction_rate\": 1 / 200,\n",
    "        \"seed\": 0\n",
    "    },\n",
    "]\n",
    "\n",
    "obs_params = {\n",
    "    \"observation_tree_depth\": 3,\n",
    "    \"observation_radius\": 30,\n",
    "    \"observation_max_path_depth\": 30\n",
    "}\n",
    "\n",
    "def check_env_config(id):\n",
    "    if id >= len(env_params) or id < 0:\n",
    "        print(\"\\n🛑 Invalid environment configuration, only Test_0 to Test_{} are supported.\".format(len(env_params) - 1))\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "check_env_config(training_params.training_env_config)\n",
    "check_env_config(training_params.evaluation_env_config)\n",
    "\n",
    "training_env_params = env_params[training_params.training_env_config]\n",
    "evaluation_env_params = env_params[training_params.evaluation_env_config]\n",
    "\n",
    "print(\"\\nTraining parameters:\")\n",
    "pprint(vars(training_params))\n",
    "print(\"\\nTraining environment parameters (Test_{}):\".format(training_params.training_env_config))\n",
    "pprint(training_env_params)\n",
    "print(\"\\nEvaluation environment parameters (Test_{}):\".format(training_params.evaluation_env_config))\n",
    "pprint(evaluation_env_params)\n",
    "print(\"\\nObservation parameters:\")\n",
    "pprint(obs_params)\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(training_params.num_threads)\n",
    "train_agent(training_params, Namespace(**training_env_params), Namespace(**evaluation_env_params), Namespace(**obs_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(os.path.join(CHECKPOINT_DIR, 'scores.pkl'), 'rb') as f:\n",
    "    score_list = pickle.load(f)\n",
    "plt.plot(score_list)\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, 'scores.png'))\n",
    "plt.show()\n",
    "\n",
    "with open(os.path.join(CHECKPOINT_DIR, 'completion.pkl'), 'rb') as f:\n",
    "    completion_list = pickle.load(f)\n",
    "plt.plot(completion_list)\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, 'completion.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
