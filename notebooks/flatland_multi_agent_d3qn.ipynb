{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flatland-multi-agent-d3qn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxZBoBUH-9rB",
        "outputId": "082ff381-dbba-4279-9155-6e2e63974ef6"
      },
      "source": [
        "!pip install flatland-rl"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flatland-rl in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pytest-runner>=4.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (5.2)\n",
            "Requirement already satisfied: importlib-resources<2,>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.5.0)\n",
            "Requirement already satisfied: pyarrow>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.14.1)\n",
            "Requirement already satisfied: Pillow>=5.4.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (7.0.0)\n",
            "Requirement already satisfied: msgpack-numpy>=0.4.4.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.4.7.1)\n",
            "Requirement already satisfied: timeout-decorator>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.5.0)\n",
            "Requirement already satisfied: ipycanvas in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.18.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (2.5)\n",
            "Requirement already satisfied: tox>=3.5.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (3.20.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.15.0)\n",
            "Requirement already satisfied: recordtype>=1.3 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.3)\n",
            "Requirement already satisfied: msgpack==0.6.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.6.1)\n",
            "Requirement already satisfied: pytest<5,>=3.8.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (4.6.11)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (20.3.0)\n",
            "Requirement already satisfied: crowdai-api>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.1.22)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (7.1.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.17 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (2.0.0)\n",
            "Requirement already satisfied: svgutils>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.3.1)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.1.4)\n",
            "Requirement already satisfied: gym==0.14.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.14.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (2.4.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.10.1)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources<2,>=1.0.1->flatland-rl) (3.4.0)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.6/dist-packages (from ipycanvas->flatland-rl) (3.4.5)\n",
            "Requirement already satisfied: ipywidgets>=7.5.0 in /usr/local/lib/python3.6/dist-packages (from ipycanvas->flatland-rl) (7.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->flatland-rl) (4.4.2)\n",
            "Requirement already satisfied: packaging>=14 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (20.4)\n",
            "Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (0.10.2)\n",
            "Requirement already satisfied: virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (20.2.1)\n",
            "Requirement already satisfied: pluggy>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (0.13.1)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (3.0.12)\n",
            "Requirement already satisfied: py>=1.4.17 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (1.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (1.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (0.2.5)\n",
            "Requirement already satisfied: more-itertools>=4.0.0; python_version > \"2.7\" in /usr/local/lib/python3.6/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (8.6.0)\n",
            "Requirement already satisfied: python-gitlab>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from crowdai-api>=0.1.21->flatland-rl) (2.5.0)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.6/dist-packages (from crowdai-api>=0.1.21->flatland-rl) (3.5.3)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from crowdai-api>=0.1.21->flatland-rl) (2.23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (0.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from svgutils>=0.3.1->flatland-rl) (4.2.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.1->flatland-rl) (2018.9)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.14.0->flatland-rl) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.14.0->flatland-rl) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.14.0->flatland-rl) (1.2.2)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (3.5.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.10.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.0.8)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.3.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0->tox>=3.5.2->flatland-rl) (0.3.1)\n",
            "Requirement already satisfied: appdirs<2,>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0->tox>=3.5.2->flatland-rl) (1.4.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2020.11.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.14.0->flatland-rl) (0.16.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (50.3.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.8.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.3.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.3.5)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.7.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.6.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.9.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.11.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (20.0.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (3.2.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVd2bxsx_HYs"
      },
      "source": [
        "# from datetime import datetime\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import copy\n",
        "import pickle\n",
        "import datetime\n",
        "\n",
        "from argparse import ArgumentParser, Namespace\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from collections import namedtuple, deque, Iterable\n",
        "\n",
        "import psutil\n",
        "from flatland.utils.rendertools import RenderTool\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from flatland.envs.rail_env import RailEnv, RailEnvActions\n",
        "from flatland.envs.rail_generators import sparse_rail_generator\n",
        "from flatland.envs.schedule_generators import sparse_schedule_generator\n",
        "from flatland.envs.observations import TreeObsForRailEnv\n",
        "\n",
        "from flatland.envs.malfunction_generators import malfunction_from_params, MalfunctionParameters\n",
        "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnJ6eqt6_JHe"
      },
      "source": [
        "\n",
        "def max_lt(seq, val):\n",
        "    \"\"\"\n",
        "    Return greatest item in seq for which item < val applies.\n",
        "    None is returned if seq was empty or all items in seq were >= val.\n",
        "    \"\"\"\n",
        "    max = 0\n",
        "    idx = len(seq) - 1\n",
        "    while idx >= 0:\n",
        "        if seq[idx] < val and seq[idx] >= 0 and seq[idx] > max:\n",
        "            max = seq[idx]\n",
        "        idx -= 1\n",
        "    return max\n",
        "\n",
        "\n",
        "def min_gt(seq, val):\n",
        "    \"\"\"\n",
        "    Return smallest item in seq for which item > val applies.\n",
        "    None is returned if seq was empty or all items in seq were >= val.\n",
        "    \"\"\"\n",
        "    min = np.inf\n",
        "    idx = len(seq) - 1\n",
        "    while idx >= 0:\n",
        "        if seq[idx] >= val and seq[idx] < min:\n",
        "            min = seq[idx]\n",
        "        idx -= 1\n",
        "    return min\n",
        "\n",
        "\n",
        "def norm_obs_clip(obs, clip_min=-1, clip_max=1, fixed_radius=0, normalize_to_range=False):\n",
        "    \"\"\"\n",
        "    This function returns the difference between min and max value of an observation\n",
        "    :param obs: Observation that should be normalized\n",
        "    :param clip_min: min value where observation will be clipped\n",
        "    :param clip_max: max value where observation will be clipped\n",
        "    :return: returnes normalized and clipped observatoin\n",
        "    \"\"\"\n",
        "    if fixed_radius > 0:\n",
        "        max_obs = fixed_radius\n",
        "    else:\n",
        "        max_obs = max(1, max_lt(obs, 1000)) + 1\n",
        "\n",
        "    min_obs = 0  # min(max_obs, min_gt(obs, 0))\n",
        "    if normalize_to_range:\n",
        "        min_obs = min_gt(obs, 0)\n",
        "    if min_obs > max_obs:\n",
        "        min_obs = max_obs\n",
        "    if max_obs == min_obs:\n",
        "        return np.clip(np.array(obs) / max_obs, clip_min, clip_max)\n",
        "    norm = np.abs(max_obs - min_obs)\n",
        "    return np.clip((np.array(obs) - min_obs) / norm, clip_min, clip_max)\n",
        "\n",
        "\n",
        "def _split_node_into_feature_groups(node) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    data = np.zeros(6)\n",
        "    distance = np.zeros(1)\n",
        "    agent_data = np.zeros(4)\n",
        "\n",
        "    data[0] = node.dist_own_target_encountered\n",
        "    data[1] = node.dist_other_target_encountered\n",
        "    data[2] = node.dist_other_agent_encountered\n",
        "    data[3] = node.dist_potential_conflict\n",
        "    data[4] = node.dist_unusable_switch\n",
        "    data[5] = node.dist_to_next_branch\n",
        "\n",
        "    distance[0] = node.dist_min_to_target\n",
        "\n",
        "    agent_data[0] = node.num_agents_same_direction\n",
        "    agent_data[1] = node.num_agents_opposite_direction\n",
        "    agent_data[2] = node.num_agents_malfunctioning\n",
        "    agent_data[3] = node.speed_min_fractional\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def _split_subtree_into_feature_groups(node, current_tree_depth: int, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    if node == -np.inf:\n",
        "        remaining_depth = max_tree_depth - current_tree_depth\n",
        "        # reference: https://stackoverflow.com/questions/515214/total-number-of-nodes-in-a-tree-data-structure\n",
        "        num_remaining_nodes = int((4 ** (remaining_depth + 1) - 1) / (4 - 1))\n",
        "        return [-np.inf] * num_remaining_nodes * 6, [-np.inf] * num_remaining_nodes, [-np.inf] * num_remaining_nodes * 4\n",
        "\n",
        "    data, distance, agent_data = _split_node_into_feature_groups(node)\n",
        "\n",
        "    if not node.childs:\n",
        "        return data, distance, agent_data\n",
        "\n",
        "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
        "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(node.childs[direction], current_tree_depth + 1, max_tree_depth)\n",
        "        data = np.concatenate((data, sub_data))\n",
        "        distance = np.concatenate((distance, sub_distance))\n",
        "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def split_tree_into_feature_groups(tree, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    \"\"\"\n",
        "    This function splits the tree into three difference arrays of values\n",
        "    \"\"\"\n",
        "    data, distance, agent_data = _split_node_into_feature_groups(tree)\n",
        "\n",
        "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
        "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(tree.childs[direction], 1, max_tree_depth)\n",
        "        data = np.concatenate((data, sub_data))\n",
        "        distance = np.concatenate((distance, sub_distance))\n",
        "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def normalize_observation(observation, tree_depth: int, observation_radius=0):\n",
        "    \"\"\"\n",
        "    This function normalizes the observation used by the RL algorithm\n",
        "    \"\"\"\n",
        "    data, distance, agent_data = split_tree_into_feature_groups(observation, tree_depth)\n",
        "\n",
        "    data = norm_obs_clip(data, fixed_radius=observation_radius)\n",
        "    distance = norm_obs_clip(distance, normalize_to_range=True)\n",
        "    agent_data = np.clip(agent_data, -1, 1)\n",
        "    normalized_obs = np.concatenate((np.concatenate((data, distance)), agent_data))\n",
        "    return normalized_obs\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TBhnxgJ_LP0"
      },
      "source": [
        "class DuelingQNetwork(nn.Module):\n",
        "    \"\"\"Dueling Q-network (https://arxiv.org/abs/1511.06581)\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, hidsize1=128, hidsize2=128):\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "\n",
        "        # value network\n",
        "        self.fc1_val = nn.Linear(state_size, hidsize1)\n",
        "        self.fc2_val = nn.Linear(hidsize1, hidsize2)\n",
        "        self.fc4_val = nn.Linear(hidsize2, 1)\n",
        "\n",
        "        # advantage network\n",
        "        self.fc1_adv = nn.Linear(state_size, hidsize1)\n",
        "        self.fc2_adv = nn.Linear(hidsize1, hidsize2)\n",
        "        self.fc4_adv = nn.Linear(hidsize2, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        val = F.relu(self.fc1_val(x))\n",
        "        val = F.relu(self.fc2_val(val))\n",
        "        val = self.fc4_val(val)\n",
        "\n",
        "        # advantage calculation\n",
        "        adv = F.relu(self.fc1_adv(x))\n",
        "        adv = F.relu(self.fc2_adv(adv))\n",
        "        adv = self.fc4_adv(adv)\n",
        "\n",
        "        return val + adv - adv.mean()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e9ebvu7_L_m"
      },
      "source": [
        "class Policy:\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save(self, filename):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def load(self, filename):\n",
        "        raise NotImplementedError\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMUAT2rD_MJr"
      },
      "source": [
        "\n",
        "class DDDQNPolicy(Policy):\n",
        "    \"\"\"Dueling Double DQN policy\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, parameters, evaluation_mode=False):\n",
        "        self.evaluation_mode = evaluation_mode\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.double_dqn = True\n",
        "        self.hidsize = 1\n",
        "\n",
        "        if not evaluation_mode:\n",
        "            self.hidsize = parameters.hidden_size\n",
        "            self.buffer_size = parameters.buffer_size\n",
        "            self.batch_size = parameters.batch_size\n",
        "            self.update_every = parameters.update_every\n",
        "            self.learning_rate = parameters.learning_rate\n",
        "            self.tau = parameters.tau\n",
        "            self.gamma = parameters.gamma\n",
        "            self.buffer_min_size = parameters.buffer_min_size\n",
        "\n",
        "        # Device\n",
        "        if parameters.use_gpu and torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda:0\")\n",
        "            # print(\"ðŸ‡ Using GPU\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "            # print(\"ðŸ¢ Using CPU\")\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = DuelingQNetwork(state_size, action_size, hidsize1=self.hidsize, hidsize2=self.hidsize).to(self.device)\n",
        "\n",
        "        if not evaluation_mode:\n",
        "            self.qnetwork_target = copy.deepcopy(self.qnetwork_local)\n",
        "            self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.learning_rate)\n",
        "            self.memory = ReplayBuffer(action_size, self.buffer_size, self.batch_size, self.device)\n",
        "\n",
        "            self.t_step = 0\n",
        "            self.loss = 0.0\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        assert not self.evaluation_mode, \"Policy has been initialized for evaluation only.\"\n",
        "\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > self.buffer_min_size and len(self.memory) > self.batch_size:\n",
        "                self._learn()\n",
        "\n",
        "    def _learn(self):\n",
        "        experiences = self.memory.sample()\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Get expected Q values from local model\n",
        "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        if self.double_dqn:\n",
        "            # Double DQN\n",
        "            q_best_action = self.qnetwork_local(next_states).max(1)[1]\n",
        "            q_targets_next = self.qnetwork_target(next_states).gather(1, q_best_action.unsqueeze(-1))\n",
        "        else:\n",
        "            # DQN\n",
        "            q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(-1)\n",
        "\n",
        "        # Compute Q targets for current states\n",
        "        q_targets = rewards + (self.gamma * q_targets_next * (1 - dones))\n",
        "\n",
        "        # Compute loss\n",
        "        self.loss = F.mse_loss(q_expected, q_targets)\n",
        "\n",
        "        # Minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        self.loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        self._soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
        "\n",
        "    def _soft_update(self, local_model, target_model, tau):\n",
        "        # Soft update model parameters.\n",
        "        # Î¸_target = Ï„*Î¸_local + (1 - Ï„)*Î¸_target\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "    def save(self, filename):\n",
        "        torch.save(self.qnetwork_local.state_dict(), filename + \".local\")\n",
        "        torch.save(self.qnetwork_target.state_dict(), filename + \".target\")\n",
        "\n",
        "    def load(self, filename):\n",
        "        if os.path.exists(filename + \".local\"):\n",
        "            self.qnetwork_local.load_state_dict(torch.load(filename + \".local\"))\n",
        "        if os.path.exists(filename + \".target\"):\n",
        "            self.qnetwork_target.load_state_dict(torch.load(filename + \".target\"))\n",
        "\n",
        "    def save_replay_buffer(self, filename):\n",
        "        memory = self.memory.memory\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(list(memory)[-500000:], f)\n",
        "\n",
        "    def load_replay_buffer(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.memory.memory = pickle.load(f)\n",
        "\n",
        "    def test(self):\n",
        "        self.act(np.array([[0] * self.state_size]))\n",
        "        self._learn()\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZXdt-HN_MO1"
      },
      "source": [
        "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, device):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = Experience(np.expand_dims(state, 0), action, reward, np.expand_dims(next_state, 0), done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(self.__v_stack_impr([e.state for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        actions = torch.from_numpy(self.__v_stack_impr([e.action for e in experiences if e is not None])) \\\n",
        "            .long().to(self.device)\n",
        "        rewards = torch.from_numpy(self.__v_stack_impr([e.reward for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        next_states = torch.from_numpy(self.__v_stack_impr([e.next_state for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        dones = torch.from_numpy(self.__v_stack_impr([e.done for e in experiences if e is not None]).astype(np.uint8)) \\\n",
        "            .float().to(self.device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "    def __v_stack_impr(self, states):\n",
        "        sub_dim = len(states[0][0]) if isinstance(states[0], Iterable) else 1\n",
        "        np_states = np.reshape(np.array(states), (len(states), sub_dim))\n",
        "        return np_states\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK4XEHFs_MRm"
      },
      "source": [
        "from timeit import default_timer\n",
        "\n",
        "\n",
        "class Timer(object):\n",
        "    \"\"\"\n",
        "    Utility to measure times.\n",
        "\n",
        "    TODO:\n",
        "    - add \"lap\" method to make it easier to measure average time (+std) when measuring the same thing multiple times.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.total_time = 0.0\n",
        "        self.start_time = 0.0\n",
        "        self.end_time = 0.0\n",
        "\n",
        "    def start(self):\n",
        "        self.start_time = default_timer()\n",
        "\n",
        "    def end(self):\n",
        "        self.total_time += default_timer() - self.start_time\n",
        "\n",
        "    def get(self):\n",
        "        return self.total_time\n",
        "\n",
        "    def get_current(self):\n",
        "        return default_timer() - self.start_time\n",
        "\n",
        "    def reset(self):\n",
        "        self.__init__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.get()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2G0J5z_MUO"
      },
      "source": [
        "\n",
        "def create_rail_env(env_params, tree_observation):\n",
        "    n_agents = env_params.n_agents\n",
        "    x_dim = env_params.x_dim\n",
        "    y_dim = env_params.y_dim\n",
        "    n_cities = env_params.n_cities\n",
        "    max_rails_between_cities = env_params.max_rails_between_cities\n",
        "    max_rails_in_city = env_params.max_rails_in_city\n",
        "    seed = env_params.seed\n",
        "\n",
        "    # Break agents from time to time\n",
        "    malfunction_parameters = MalfunctionParameters(\n",
        "        malfunction_rate=env_params.malfunction_rate,\n",
        "        min_duration=20,\n",
        "        max_duration=50\n",
        "    )\n",
        "\n",
        "    return RailEnv(\n",
        "        width=x_dim, height=y_dim,\n",
        "        rail_generator=sparse_rail_generator(\n",
        "            max_num_cities=n_cities,\n",
        "            grid_mode=False,\n",
        "            max_rails_between_cities=max_rails_between_cities,\n",
        "            max_rails_in_city=max_rails_in_city\n",
        "        ),\n",
        "        schedule_generator=sparse_schedule_generator(),\n",
        "        number_of_agents=n_agents,\n",
        "        malfunction_generator_and_process_data=malfunction_from_params(malfunction_parameters),\n",
        "        obs_builder_object=tree_observation,\n",
        "        random_seed=seed\n",
        "    )\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b68V_7NH_MWt"
      },
      "source": [
        "\n",
        "def train_agent(train_params, train_env_params, eval_env_params, obs_params):\n",
        "    # Environment parameters\n",
        "    n_agents = train_env_params.n_agents\n",
        "    x_dim = train_env_params.x_dim\n",
        "    y_dim = train_env_params.y_dim\n",
        "    n_cities = train_env_params.n_cities\n",
        "    max_rails_between_cities = train_env_params.max_rails_between_cities\n",
        "    max_rails_in_city = train_env_params.max_rails_in_city\n",
        "    seed = train_env_params.seed\n",
        "\n",
        "    # Unique ID for this training\n",
        "    now = datetime.datetime.now()\n",
        "    training_id = now.strftime('%y%m%d%H%M%S')\n",
        "\n",
        "    # Observation parameters\n",
        "    observation_tree_depth = obs_params.observation_tree_depth\n",
        "    observation_radius = obs_params.observation_radius\n",
        "    observation_max_path_depth = obs_params.observation_max_path_depth\n",
        "\n",
        "    # Training parameters\n",
        "    eps_start = train_params.eps_start\n",
        "    eps_end = train_params.eps_end\n",
        "    eps_decay = train_params.eps_decay\n",
        "    n_episodes = train_params.n_episodes\n",
        "    checkpoint_interval = train_params.checkpoint_interval\n",
        "    n_eval_episodes = train_params.n_evaluation_episodes\n",
        "    restore_replay_buffer = train_params.restore_replay_buffer\n",
        "    save_replay_buffer = train_params.save_replay_buffer\n",
        "\n",
        "    # Set the seeds\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Observation builder\n",
        "    predictor = ShortestPathPredictorForRailEnv(observation_max_path_depth)\n",
        "    tree_observation = TreeObsForRailEnv(max_depth=observation_tree_depth, predictor=predictor)\n",
        "\n",
        "    # Setup the environments\n",
        "    train_env = create_rail_env(train_env_params, tree_observation)\n",
        "    train_env.reset(regenerate_schedule=True, regenerate_rail=True)\n",
        "    eval_env = create_rail_env(eval_env_params, tree_observation)\n",
        "    eval_env.reset(regenerate_schedule=True, regenerate_rail=True)\n",
        "\n",
        "    # Setup renderer\n",
        "    if train_params.render:\n",
        "        env_renderer = RenderTool(train_env, gl=\"PGL\")\n",
        "\n",
        "    # Calculate the state size given the depth of the tree observation and the number of features\n",
        "    n_features_per_node = train_env.obs_builder.observation_dim\n",
        "    n_nodes = sum([np.power(4, i) for i in range(observation_tree_depth + 1)])\n",
        "    state_size = n_features_per_node * n_nodes\n",
        "\n",
        "    # The action space of flatland is 5 discrete actions\n",
        "    action_size = 5\n",
        "\n",
        "    # Max number of steps per episode\n",
        "    # This is the official formula used during evaluations\n",
        "    # See details in flatland.envs.schedule_generators.sparse_schedule_generator\n",
        "    # max_steps = int(4 * 2 * (env.height + env.width + (n_agents / n_cities)))\n",
        "    max_steps = train_env._max_episode_steps\n",
        "\n",
        "    action_count = [0] * action_size\n",
        "    action_dict = dict()\n",
        "    agent_obs = [None] * n_agents\n",
        "    agent_prev_obs = [None] * n_agents\n",
        "    agent_prev_action = [2] * n_agents\n",
        "    update_values = [False] * n_agents\n",
        "\n",
        "    # Smoothed values used as target for hyperparameter tuning\n",
        "    smoothed_normalized_score = -1.0\n",
        "    smoothed_eval_normalized_score = -1.0\n",
        "    smoothed_completion = 0.0\n",
        "    smoothed_eval_completion = 0.0\n",
        "\n",
        "    # Double Dueling DQN policy\n",
        "    policy = DDDQNPolicy(state_size, action_size, train_params)\n",
        "\n",
        "    # Loads existing replay buffer\n",
        "    if restore_replay_buffer:\n",
        "        try:\n",
        "            policy.load_replay_buffer(restore_replay_buffer)\n",
        "            policy.test()\n",
        "        except RuntimeError as e:\n",
        "            print(\"\\nðŸ›‘ Could't load replay buffer, were the experiences generated using the same tree depth?\")\n",
        "            print(e)\n",
        "            exit(1)\n",
        "\n",
        "    print(\"\\nðŸ’¾ Replay buffer status: {}/{} experiences\".format(len(policy.memory.memory), train_params.buffer_size))\n",
        "\n",
        "    hdd = psutil.disk_usage('/')\n",
        "    if save_replay_buffer and (hdd.free / (2 ** 30)) < 500.0:\n",
        "        print(\"âš ï¸  Careful! Saving replay buffers will quickly consume a lot of disk space. You have {:.2f}gb left.\".format(hdd.free / (2 ** 30)))\n",
        "\n",
        "    # TensorBoard writer\n",
        "    writer = SummaryWriter()\n",
        "    writer.add_hparams(vars(train_params), {})\n",
        "    writer.add_hparams(vars(train_env_params), {})\n",
        "    writer.add_hparams(vars(obs_params), {})\n",
        "\n",
        "    training_timer = Timer()\n",
        "    training_timer.start()\n",
        "\n",
        "    print(\"\\nðŸš‰ Training {} trains on {}x{} grid for {} episodes, evaluating on {} episodes every {} episodes. Training id '{}'.\\n\".format(\n",
        "        train_env.get_num_agents(),\n",
        "        x_dim, y_dim,\n",
        "        n_episodes,\n",
        "        n_eval_episodes,\n",
        "        checkpoint_interval,\n",
        "        training_id\n",
        "    ))\n",
        "\n",
        "    make_dir(CHECKPOINT_DIR)\n",
        "    params_file = os.path.join(CHECKPOINT_DIR, 'params.txt')\n",
        "    write_params_to_file(train_params, train_env_params, obs_params, params_file)\n",
        "\n",
        "    score_list = []\n",
        "    completion_list = []\n",
        "\n",
        "    for episode_idx in range(n_episodes + 1):\n",
        "        step_timer = Timer()\n",
        "        reset_timer = Timer()\n",
        "        learn_timer = Timer()\n",
        "        preproc_timer = Timer()\n",
        "        inference_timer = Timer()\n",
        "\n",
        "        # Reset environment\n",
        "        reset_timer.start()\n",
        "        obs, info = train_env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "        reset_timer.end()\n",
        "\n",
        "        if train_params.render:\n",
        "            env_renderer.set_new_rail()\n",
        "\n",
        "        score = 0\n",
        "        nb_steps = 0\n",
        "        actions_taken = []\n",
        "\n",
        "        # Build initial agent-specific observations\n",
        "        for agent in train_env.get_agent_handles():\n",
        "            if obs[agent]:\n",
        "                agent_obs[agent] = normalize_observation(obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
        "                agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "\n",
        "        # Run episode\n",
        "        for step in range(max_steps - 1):\n",
        "            inference_timer.start()\n",
        "            for agent in train_env.get_agent_handles():\n",
        "                if info['action_required'][agent]:\n",
        "                    update_values[agent] = True\n",
        "                    action = policy.act(agent_obs[agent], eps=eps_start)\n",
        "\n",
        "                    action_count[action] += 1\n",
        "                    actions_taken.append(action)\n",
        "                else:\n",
        "                    # An action is not required if the train hasn't joined the railway network,\n",
        "                    # if it already reached its target, or if is currently malfunctioning.\n",
        "                    update_values[agent] = False\n",
        "                    action = 0\n",
        "                action_dict.update({agent: action})\n",
        "            inference_timer.end()\n",
        "\n",
        "            # Environment step\n",
        "            step_timer.start()\n",
        "            next_obs, all_rewards, done, info = train_env.step(action_dict)\n",
        "            step_timer.end()\n",
        "\n",
        "            # Render an episode at some interval\n",
        "            if train_params.render and episode_idx % checkpoint_interval == 0:\n",
        "                env_renderer.render_env(\n",
        "                    show=True,\n",
        "                    frames=False,\n",
        "                    show_observations=False,\n",
        "                    show_predictions=False\n",
        "                )\n",
        "\n",
        "            # Update replay buffer and train agent\n",
        "            for agent in train_env.get_agent_handles():\n",
        "                if update_values[agent] or done['__all__']:\n",
        "                    # Only learn from timesteps where somethings happened\n",
        "                    learn_timer.start()\n",
        "                    policy.step(agent_prev_obs[agent], agent_prev_action[agent], all_rewards[agent], agent_obs[agent], done[agent])\n",
        "                    learn_timer.end()\n",
        "\n",
        "                    agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "                    agent_prev_action[agent] = action_dict[agent]\n",
        "\n",
        "                # Preprocess the new observations\n",
        "                if next_obs[agent]:\n",
        "                    preproc_timer.start()\n",
        "                    agent_obs[agent] = normalize_observation(next_obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
        "                    preproc_timer.end()\n",
        "\n",
        "                score += all_rewards[agent]\n",
        "\n",
        "            nb_steps = step\n",
        "\n",
        "            if done['__all__']:\n",
        "                break\n",
        "\n",
        "        # Epsilon decay\n",
        "        eps_start = max(eps_end, eps_decay * eps_start)\n",
        "\n",
        "        # Collect information about training\n",
        "        tasks_finished = sum(done[idx] for idx in train_env.get_agent_handles())\n",
        "        completion = tasks_finished / max(1, train_env.get_num_agents())\n",
        "        normalized_score = score / (max_steps * train_env.get_num_agents())\n",
        "        action_probs = action_count / np.sum(action_count)\n",
        "        action_count = [1] * action_size\n",
        "\n",
        "        smoothing = 0.99\n",
        "        smoothed_normalized_score = smoothed_normalized_score * smoothing + normalized_score * (1.0 - smoothing)\n",
        "        smoothed_completion = smoothed_completion * smoothing + completion * (1.0 - smoothing)\n",
        "\n",
        "        score_list.append(smoothed_normalized_score)\n",
        "        completion_list.append(smoothed_completion)\n",
        "\n",
        "        # Print logs\n",
        "        if episode_idx % checkpoint_interval == 0:\n",
        "            torch.save(policy.qnetwork_local, os.path.join(CHECKPOINT_DIR, str(episode_idx) + '.pth'))\n",
        "\n",
        "            if save_replay_buffer:\n",
        "                policy.save_replay_buffer('replay_buffers/' + training_id + '-' + str(episode_idx) + '.pkl')\n",
        "\n",
        "            if train_params.render:\n",
        "                env_renderer.close_window()\n",
        "\n",
        "        print(\n",
        "            '\\rðŸš‚ Episode {}'\n",
        "            '\\t ðŸ† Score: {:.3f}'\n",
        "            ' Avg: {:.3f}'\n",
        "            '\\t ðŸ’¯ Done: {:.2f}%'\n",
        "            ' Avg: {:.2f}%'\n",
        "            '\\t ðŸŽ² Epsilon: {:.3f} '\n",
        "            '\\t ðŸ”€ Action Probs: {}'.format(\n",
        "                episode_idx,\n",
        "                normalized_score,\n",
        "                smoothed_normalized_score,\n",
        "                100 * completion,\n",
        "                100 * smoothed_completion,\n",
        "                eps_start,\n",
        "                format_action_prob(action_probs)\n",
        "            ), end=\" \")\n",
        "\n",
        "        # Evaluate policy and log results at some interval\n",
        "        if episode_idx % checkpoint_interval == 0 and n_eval_episodes > 0:\n",
        "            scores, completions, nb_steps_eval = eval_policy(eval_env, policy, train_params, obs_params)\n",
        "\n",
        "            writer.add_scalar(\"evaluation/scores_min\", np.min(scores), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/scores_max\", np.max(scores), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/scores_mean\", np.mean(scores), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/scores_std\", np.std(scores), episode_idx)\n",
        "            writer.add_histogram(\"evaluation/scores\", np.array(scores), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/completions_min\", np.min(completions), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/completions_max\", np.max(completions), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/completions_mean\", np.mean(completions), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/completions_std\", np.std(completions), episode_idx)\n",
        "            writer.add_histogram(\"evaluation/completions\", np.array(completions), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/nb_steps_min\", np.min(nb_steps_eval), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/nb_steps_max\", np.max(nb_steps_eval), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/nb_steps_mean\", np.mean(nb_steps_eval), episode_idx)\n",
        "            writer.add_scalar(\"evaluation/nb_steps_std\", np.std(nb_steps_eval), episode_idx)\n",
        "            writer.add_histogram(\"evaluation/nb_steps\", np.array(nb_steps_eval), episode_idx)\n",
        "\n",
        "            smoothing = 0.9\n",
        "            smoothed_eval_normalized_score = smoothed_eval_normalized_score * smoothing + np.mean(scores) * (1.0 - smoothing)\n",
        "            smoothed_eval_completion = smoothed_eval_completion * smoothing + np.mean(completions) * (1.0 - smoothing)\n",
        "            writer.add_scalar(\"evaluation/smoothed_score\", smoothed_eval_normalized_score, episode_idx)\n",
        "            writer.add_scalar(\"evaluation/smoothed_completion\", smoothed_eval_completion, episode_idx)\n",
        "\n",
        "        # Save logs to tensorboard\n",
        "        writer.add_scalar(\"training/score\", normalized_score, episode_idx)\n",
        "        writer.add_scalar(\"training/smoothed_score\", smoothed_normalized_score, episode_idx)\n",
        "        writer.add_scalar(\"training/completion\", np.mean(completion), episode_idx)\n",
        "        writer.add_scalar(\"training/smoothed_completion\", np.mean(smoothed_completion), episode_idx)\n",
        "        writer.add_scalar(\"training/nb_steps\", nb_steps, episode_idx)\n",
        "        writer.add_histogram(\"actions/distribution\", np.array(actions_taken), episode_idx)\n",
        "        writer.add_scalar(\"actions/nothing\", action_probs[RailEnvActions.DO_NOTHING], episode_idx)\n",
        "        writer.add_scalar(\"actions/left\", action_probs[RailEnvActions.MOVE_LEFT], episode_idx)\n",
        "        writer.add_scalar(\"actions/forward\", action_probs[RailEnvActions.MOVE_FORWARD], episode_idx)\n",
        "        writer.add_scalar(\"actions/right\", action_probs[RailEnvActions.MOVE_RIGHT], episode_idx)\n",
        "        writer.add_scalar(\"actions/stop\", action_probs[RailEnvActions.STOP_MOVING], episode_idx)\n",
        "        writer.add_scalar(\"training/epsilon\", eps_start, episode_idx)\n",
        "        writer.add_scalar(\"training/buffer_size\", len(policy.memory), episode_idx)\n",
        "        writer.add_scalar(\"training/loss\", policy.loss, episode_idx)\n",
        "        writer.add_scalar(\"timer/reset\", reset_timer.get(), episode_idx)\n",
        "        writer.add_scalar(\"timer/step\", step_timer.get(), episode_idx)\n",
        "        writer.add_scalar(\"timer/learn\", learn_timer.get(), episode_idx)\n",
        "        writer.add_scalar(\"timer/preproc\", preproc_timer.get(), episode_idx)\n",
        "        writer.add_scalar(\"timer/total\", training_timer.get_current(), episode_idx)\n",
        "\n",
        "    pickle_list(score_list, os.path.join(CHECKPOINT_DIR, 'scores.pkl'))\n",
        "    pickle_list(completion_list, os.path.join(CHECKPOINT_DIR, 'completion.pkl'))\n",
        "\n",
        "    plt.plot(score_list)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'scores.png'))\n",
        "    \n",
        "    plt.plot(completion_list)\n",
        "    plt.show()\n",
        "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'completion.png'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFckIZRM_MZe"
      },
      "source": [
        "\n",
        "def format_action_prob(action_probs):\n",
        "    action_probs = np.round(action_probs, 3)\n",
        "    actions = [\"â†»\", \"â†\", \"â†‘\", \"â†’\", \"â—¼\"]\n",
        "\n",
        "    buffer = \"\"\n",
        "    for action, action_prob in zip(actions, action_probs):\n",
        "        buffer += action + \" \" + \"{:.3f}\".format(action_prob) + \" \"\n",
        "\n",
        "    return buffer\n",
        "\n",
        "\n",
        "def eval_policy(env, policy, train_params, obs_params):\n",
        "    n_eval_episodes = train_params.n_evaluation_episodes\n",
        "    max_steps = env._max_episode_steps\n",
        "    tree_depth = obs_params.observation_tree_depth\n",
        "    observation_radius = obs_params.observation_radius\n",
        "\n",
        "    action_dict = dict()\n",
        "    scores = []\n",
        "    completions = []\n",
        "    nb_steps = []\n",
        "\n",
        "    for episode_idx in range(n_eval_episodes):\n",
        "        agent_obs = [None] * env.get_num_agents()\n",
        "        score = 0.0\n",
        "\n",
        "        obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "\n",
        "        final_step = 0\n",
        "\n",
        "        for step in range(max_steps - 1):\n",
        "            for agent in env.get_agent_handles():\n",
        "                if obs[agent]:\n",
        "                    agent_obs[agent] = normalize_observation(obs[agent], tree_depth=tree_depth, observation_radius=observation_radius)\n",
        "\n",
        "                action = 0\n",
        "                if info['action_required'][agent]:\n",
        "                    action = policy.act(agent_obs[agent], eps=0.0)\n",
        "                action_dict.update({agent: action})\n",
        "\n",
        "            obs, all_rewards, done, info = env.step(action_dict)\n",
        "\n",
        "            for agent in env.get_agent_handles():\n",
        "                score += all_rewards[agent]\n",
        "\n",
        "            final_step = step\n",
        "\n",
        "            if done['__all__']:\n",
        "                break\n",
        "\n",
        "        normalized_score = score / (max_steps * env.get_num_agents())\n",
        "        scores.append(normalized_score)\n",
        "\n",
        "        tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "        completion = tasks_finished / max(1, env.get_num_agents())\n",
        "        completions.append(completion)\n",
        "\n",
        "        nb_steps.append(final_step)\n",
        "\n",
        "    print(\"\\tâœ… Eval: score {:.3f} done {:.1f}%\".format(np.mean(scores), np.mean(completions) * 100.0))\n",
        "\n",
        "    return scores, completions, nb_steps\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVkPmdmK_1Ae"
      },
      "source": [
        "def make_dir(dir_path):\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "def get_timestamp():\n",
        "    ct = datetime.datetime.now()\n",
        "    return str(ct).split('.')[0].replace(' ', '').replace('-', '').replace(':', '')\n",
        "\n",
        "def pickle_list(l, file_path):\n",
        "    with open(file_path, 'wb') as f:\n",
        "        pickle.dump(l, f)\n",
        "\n",
        "def write_params_to_file(train_params, train_env_params, obs_params, params_file):\n",
        "    with open(params_file, \"w\") as file1:\n",
        "        file1.write(f'n_episodes={train_params.n_episodes}' + '\\n')\n",
        "        file1.write(f'training_env_config={train_params.training_env_config}' + '\\n')\n",
        "        file1.write(f'evaluation_env_config={train_params.evaluation_env_config}' + '\\n')\n",
        "        file1.write(f'n_evaluation_episodes={train_params.n_evaluation_episodes}' + '\\n')\n",
        "        file1.write(f'checkpoint_interval={train_params.checkpoint_interval}' + '\\n')\n",
        "        file1.write(f'eps_start={train_params.eps_start}' + '\\n')\n",
        "        file1.write(f'eps_end={train_params.eps_end}' + '\\n')\n",
        "        file1.write(f'eps_decay={train_params.eps_decay}' + '\\n')\n",
        "        file1.write(f'buffer_size={train_params.buffer_size}' + '\\n')\n",
        "        file1.write(f'buffer_min_size={train_params.buffer_min_size}' + '\\n')\n",
        "        file1.write(f'restore_replay_buffer={train_params.restore_replay_buffer}' + '\\n')\n",
        "        file1.write(f'save_replay_buffer={train_params.save_replay_buffer}' + '\\n')\n",
        "        file1.write(f'batch_size={train_params.batch_size}' + '\\n')\n",
        "        file1.write(f'gamma={train_params.gamma}' + '\\n')\n",
        "        file1.write(f'tau={train_params.tau}' + '\\n')\n",
        "        file1.write(f'learning_rate={train_params.learning_rate}' + '\\n')\n",
        "        file1.write(f'hidden_size={train_params.hidden_size}' + '\\n')\n",
        "        file1.write(f'update_every={train_params.update_every}' + '\\n')\n",
        "        file1.write(f'use_gpu={train_params.use_gpu}' + '\\n')\n",
        "        file1.write(f'num_threads={train_params.num_threads}' + '\\n')\n",
        "        file1.write(f'render={train_params.render}' + '\\n')\n",
        "        file1.write(f'n_agents={train_env_params.n_agents}' + '\\n')\n",
        "        file1.write(f'x_dim={train_env_params.x_dim}' + '\\n')\n",
        "        file1.write(f'y_dim={train_env_params.y_dim}' + '\\n')\n",
        "        file1.write(f'n_cities={train_env_params.n_cities}' + '\\n')\n",
        "        file1.write(f'max_rails_between_cities={train_env_params.max_rails_between_cities}' + '\\n')\n",
        "        file1.write(f'max_rails_in_city={train_env_params.max_rails_in_city}' + '\\n')\n",
        "        file1.write(f'malfunction_rate={train_env_params.malfunction_rate}' + '\\n')\n",
        "        file1.write(f'seed={train_env_params.seed}' + '\\n')\n",
        "        file1.write(f'observation_tree_depth={obs_params.observation_tree_depth}' + '\\n')\n",
        "        file1.write(f'observation_radius={obs_params.observation_radius}' + '\\n')\n",
        "        file1.write(f'observation_max_path_depth={obs_params.observation_max_path_depth}' + '\\n')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXZfkYrU_Mcz",
        "outputId": "45914e72-272f-422f-9f12-53ce73fe5240"
      },
      "source": [
        "# CHECKPOINT_DIR = '/scratch/ns4486/flatland-reinforcement-learning/single-agent/checkpoints'\n",
        "CHECKPOINT_DIR = '.'\n",
        "CHECKPOINT_DIR = os.path.join(CHECKPOINT_DIR, get_timestamp())\n",
        "\n",
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "training_params = Object()\n",
        "training_params.n_episodes = 2500\n",
        "training_params.training_env_config = 0\n",
        "training_params.evaluation_env_config = 0\n",
        "training_params.n_evaluation_episodes = 25\n",
        "training_params.checkpoint_interval = 100\n",
        "training_params.eps_start = 1.0\n",
        "training_params.eps_end = 0.01\n",
        "training_params.eps_decay = 0.99\n",
        "training_params.buffer_size = int(1e5)\n",
        "training_params.buffer_min_size = 0\n",
        "training_params.restore_replay_buffer = \"\"\n",
        "training_params.save_replay_buffer = False\n",
        "training_params.batch_size = 128\n",
        "training_params.gamma = 0.99\n",
        "training_params.tau = 1e-3\n",
        "training_params.learning_rate = 0.5e-4\n",
        "training_params.hidden_size = 128\n",
        "training_params.update_every = 8\n",
        "training_params.use_gpu = True\n",
        "training_params.num_threads = 1\n",
        "training_params.render = False\n",
        "\n",
        "\n",
        "env_params = [\n",
        "    {\n",
        "        # Test_0\n",
        "        \"n_agents\": 2,\n",
        "        \"x_dim\": 25,\n",
        "        \"y_dim\": 25,\n",
        "        \"n_cities\": 2,\n",
        "        \"max_rails_between_cities\": 2,\n",
        "        \"max_rails_in_city\": 3,\n",
        "        \"malfunction_rate\": 1 / 50,\n",
        "        \"seed\": 0\n",
        "    },\n",
        "    {\n",
        "        # Test_1\n",
        "        \"n_agents\": 10,\n",
        "        \"x_dim\": 30,\n",
        "        \"y_dim\": 30,\n",
        "        \"n_cities\": 2,\n",
        "        \"max_rails_between_cities\": 2,\n",
        "        \"max_rails_in_city\": 3,\n",
        "        \"malfunction_rate\": 1 / 100,\n",
        "        \"seed\": 0\n",
        "    },\n",
        "    {\n",
        "        # Test_2\n",
        "        \"n_agents\": 20,\n",
        "        \"x_dim\": 30,\n",
        "        \"y_dim\": 30,\n",
        "        \"n_cities\": 3,\n",
        "        \"max_rails_between_cities\": 2,\n",
        "        \"max_rails_in_city\": 3,\n",
        "        \"malfunction_rate\": 1 / 200,\n",
        "        \"seed\": 0\n",
        "    },\n",
        "]\n",
        "\n",
        "obs_params = {\n",
        "    \"observation_tree_depth\": 2,\n",
        "    \"observation_radius\": 10,\n",
        "    \"observation_max_path_depth\": 30\n",
        "}\n",
        "\n",
        "def check_env_config(id):\n",
        "    if id >= len(env_params) or id < 0:\n",
        "        print(\"\\nðŸ›‘ Invalid environment configuration, only Test_0 to Test_{} are supported.\".format(len(env_params) - 1))\n",
        "        exit(1)\n",
        "\n",
        "\n",
        "check_env_config(training_params.training_env_config)\n",
        "check_env_config(training_params.evaluation_env_config)\n",
        "\n",
        "training_env_params = env_params[training_params.training_env_config]\n",
        "evaluation_env_params = env_params[training_params.evaluation_env_config]\n",
        "\n",
        "print(\"\\nTraining parameters:\")\n",
        "pprint(vars(training_params))\n",
        "print(\"\\nTraining environment parameters (Test_{}):\".format(training_params.training_env_config))\n",
        "pprint(training_env_params)\n",
        "print(\"\\nEvaluation environment parameters (Test_{}):\".format(training_params.evaluation_env_config))\n",
        "pprint(evaluation_env_params)\n",
        "print(\"\\nObservation parameters:\")\n",
        "pprint(obs_params)\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(training_params.num_threads)\n",
        "train_agent(training_params, Namespace(**training_env_params), Namespace(**evaluation_env_params), Namespace(**obs_params))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training parameters:\n",
            "{'batch_size': 128,\n",
            " 'buffer_min_size': 0,\n",
            " 'buffer_size': 100000,\n",
            " 'checkpoint_interval': 100,\n",
            " 'eps_decay': 0.99,\n",
            " 'eps_end': 0.01,\n",
            " 'eps_start': 1.0,\n",
            " 'evaluation_env_config': 0,\n",
            " 'gamma': 0.99,\n",
            " 'hidden_size': 128,\n",
            " 'learning_rate': 5e-05,\n",
            " 'n_episodes': 2500,\n",
            " 'n_evaluation_episodes': 25,\n",
            " 'num_threads': 1,\n",
            " 'render': False,\n",
            " 'restore_replay_buffer': '',\n",
            " 'save_replay_buffer': False,\n",
            " 'tau': 0.001,\n",
            " 'training_env_config': 0,\n",
            " 'update_every': 8,\n",
            " 'use_gpu': True}\n",
            "\n",
            "Training environment parameters (Test_0):\n",
            "{'malfunction_rate': 0.02,\n",
            " 'max_rails_between_cities': 2,\n",
            " 'max_rails_in_city': 3,\n",
            " 'n_agents': 2,\n",
            " 'n_cities': 2,\n",
            " 'seed': 0,\n",
            " 'x_dim': 25,\n",
            " 'y_dim': 25}\n",
            "\n",
            "Evaluation environment parameters (Test_0):\n",
            "{'malfunction_rate': 0.02,\n",
            " 'max_rails_between_cities': 2,\n",
            " 'max_rails_in_city': 3,\n",
            " 'n_agents': 2,\n",
            " 'n_cities': 2,\n",
            " 'seed': 0,\n",
            " 'x_dim': 25,\n",
            " 'y_dim': 25}\n",
            "\n",
            "Observation parameters:\n",
            "{'observation_max_path_depth': 30,\n",
            " 'observation_radius': 10,\n",
            " 'observation_tree_depth': 2}\n",
            "DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator\n",
            "DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator\n",
            "\n",
            "ðŸ’¾ Replay buffer status: 0/100000 experiences\n",
            "\n",
            "ðŸš‰ Training 2 trains on 25x25 grid for 2500 episodes, evaluating on 25 episodes every 100 episodes. Training id '201207110222'.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/rail_generators.py:781: UserWarning: Could not set all required cities!\n",
            "  \"Could not set all required cities!\")\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/rail_generators.py:703: UserWarning: [WARNING] Changing to Grid mode to place at least 2 cities.\n",
            "  warnings.warn(\"[WARNING] Changing to Grid mode to place at least 2 cities.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ðŸš‚ Episode 0\t ðŸ† Score: -0.998 Avg: -1.000\t ðŸ’¯ Done: 0.00% Avg: 0.00%\t ðŸŽ² Epsilon: 0.990 \t ðŸ”€ Action Probs: â†» 0.167 â† 0.194 â†‘ 0.250 â†’ 0.181 â—¼ 0.208  \tâœ… Eval: score -0.998 done 0.0%\n",
            "ðŸš‚ Episode 88\t ðŸ† Score: -0.998 Avg: -0.810\t ðŸ’¯ Done: 0.00% Avg: 28.31%\t ðŸŽ² Epsilon: 0.409 \t ðŸ”€ Action Probs: â†» 0.127 â† 0.190 â†‘ 0.113 â†’ 0.458 â—¼ 0.113  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Pggg8K_Me-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSMgX6io_Mhv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f566eBg_MkR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbv-f5W_Mne"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFrIjCox_Mp9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}