{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flatland-multi-agent-maac.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxZBoBUH-9rB",
        "outputId": "4b81afc1-5292-4a3c-d5a0-75024cc42da9"
      },
      "source": [
        "!pip install flatland-rl"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flatland-rl in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.18.5)\n",
            "Requirement already satisfied: pyarrow>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.14.1)\n",
            "Requirement already satisfied: pytest-runner>=4.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (5.2)\n",
            "Requirement already satisfied: gym==0.14.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.14.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (20.3.0)\n",
            "Requirement already satisfied: ipycanvas in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.7.0)\n",
            "Requirement already satisfied: crowdai-api>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.1.22)\n",
            "Requirement already satisfied: importlib-metadata>=0.17 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.15.0)\n",
            "Requirement already satisfied: Pillow>=5.4.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (7.0.0)\n",
            "Requirement already satisfied: importlib-resources<2,>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.5.0)\n",
            "Requirement already satisfied: msgpack-numpy>=0.4.4.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.4.7.1)\n",
            "Requirement already satisfied: recordtype>=1.3 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.3)\n",
            "Requirement already satisfied: tox>=3.5.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (3.20.1)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (1.1.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.10.1)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (7.1.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (3.2.2)\n",
            "Requirement already satisfied: timeout-decorator>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.5.0)\n",
            "Requirement already satisfied: svgutils>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.3.1)\n",
            "Requirement already satisfied: pytest<5,>=3.8.2 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (4.6.11)\n",
            "Requirement already satisfied: msgpack==0.6.1 in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (0.6.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (2.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from flatland-rl) (2.5)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.14.0->flatland-rl) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.14.0->flatland-rl) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.14.0->flatland-rl) (1.4.1)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.6/dist-packages (from ipycanvas->flatland-rl) (3.4.6)\n",
            "Requirement already satisfied: ipywidgets>=7.5.0 in /usr/local/lib/python3.6/dist-packages (from ipycanvas->flatland-rl) (7.5.1)\n",
            "Requirement already satisfied: python-gitlab>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from crowdai-api>=0.1.21->flatland-rl) (2.5.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from crowdai-api>=0.1.21->flatland-rl) (2.23.0)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.6/dist-packages (from crowdai-api>=0.1.21->flatland-rl) (3.5.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.17->flatland-rl) (3.4.0)\n",
            "Requirement already satisfied: packaging>=14 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (20.4)\n",
            "Requirement already satisfied: py>=1.4.17 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (1.9.0)\n",
            "Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (0.10.2)\n",
            "Requirement already satisfied: virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (20.2.2)\n",
            "Requirement already satisfied: pluggy>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (0.13.1)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from tox>=3.5.2->flatland-rl) (3.0.12)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.1->flatland-rl) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25.1->flatland-rl) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.2->flatland-rl) (0.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from svgutils>=0.3.1->flatland-rl) (4.2.6)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0; python_version > \"2.7\" in /usr/local/lib/python3.6/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (8.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest<5,>=3.8.2->flatland-rl) (0.2.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->flatland-rl) (4.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.14.0->flatland-rl) (0.16.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.0.8)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.10.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.3.3)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->crowdai-api>=0.1.21->flatland-rl) (1.24.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0->tox>=3.5.2->flatland-rl) (0.3.1)\n",
            "Requirement already satisfied: appdirs<2,>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0->tox>=3.5.2->flatland-rl) (1.4.4)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.3.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.1.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (50.3.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.0.18)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (2.11.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.9.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (20.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.1.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (3.2.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->ipycanvas->flatland-rl) (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVd2bxsx_HYs"
      },
      "source": [
        "# from datetime import datetime\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import copy\n",
        "import pickle\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from argparse import ArgumentParser, Namespace\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from collections import namedtuple, deque, Iterable\n",
        "from itertools import chain\n",
        "\n",
        "import psutil\n",
        "from flatland.utils.rendertools import RenderTool\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from flatland.envs.rail_env import RailEnv, RailEnvActions\n",
        "from flatland.envs.rail_generators import sparse_rail_generator, complex_rail_generator\n",
        "from flatland.envs.schedule_generators import sparse_schedule_generator\n",
        "from flatland.envs.observations import TreeObsForRailEnv\n",
        "\n",
        "from flatland.envs.malfunction_generators import malfunction_from_params, MalfunctionParameters\n",
        "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnJ6eqt6_JHe"
      },
      "source": [
        "\n",
        "def max_lt(seq, val):\n",
        "    \"\"\"\n",
        "    Return greatest item in seq for which item < val applies.\n",
        "    None is returned if seq was empty or all items in seq were >= val.\n",
        "    \"\"\"\n",
        "    max = 0\n",
        "    idx = len(seq) - 1\n",
        "    while idx >= 0:\n",
        "        if seq[idx] < val and seq[idx] >= 0 and seq[idx] > max:\n",
        "            max = seq[idx]\n",
        "        idx -= 1\n",
        "    return max\n",
        "\n",
        "\n",
        "def min_gt(seq, val):\n",
        "    \"\"\"\n",
        "    Return smallest item in seq for which item > val applies.\n",
        "    None is returned if seq was empty or all items in seq were >= val.\n",
        "    \"\"\"\n",
        "    min = np.inf\n",
        "    idx = len(seq) - 1\n",
        "    while idx >= 0:\n",
        "        if seq[idx] >= val and seq[idx] < min:\n",
        "            min = seq[idx]\n",
        "        idx -= 1\n",
        "    return min\n",
        "\n",
        "\n",
        "def norm_obs_clip(obs, clip_min=-1, clip_max=1, fixed_radius=0, normalize_to_range=False):\n",
        "    \"\"\"\n",
        "    This function returns the difference between min and max value of an observation\n",
        "    :param obs: Observation that should be normalized\n",
        "    :param clip_min: min value where observation will be clipped\n",
        "    :param clip_max: max value where observation will be clipped\n",
        "    :return: returnes normalized and clipped observatoin\n",
        "    \"\"\"\n",
        "    if fixed_radius > 0:\n",
        "        max_obs = fixed_radius\n",
        "    else:\n",
        "        max_obs = max(1, max_lt(obs, 1000)) + 1\n",
        "\n",
        "    min_obs = 0  # min(max_obs, min_gt(obs, 0))\n",
        "    if normalize_to_range:\n",
        "        min_obs = min_gt(obs, 0)\n",
        "    if min_obs > max_obs:\n",
        "        min_obs = max_obs\n",
        "    if max_obs == min_obs:\n",
        "        return np.clip(np.array(obs) / max_obs, clip_min, clip_max)\n",
        "    norm = np.abs(max_obs - min_obs)\n",
        "    return np.clip((np.array(obs) - min_obs) / norm, clip_min, clip_max)\n",
        "\n",
        "\n",
        "def _split_node_into_feature_groups(node) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    data = np.zeros(6)\n",
        "    distance = np.zeros(1)\n",
        "    agent_data = np.zeros(4)\n",
        "\n",
        "    data[0] = node.dist_own_target_encountered\n",
        "    data[1] = node.dist_other_target_encountered\n",
        "    data[2] = node.dist_other_agent_encountered\n",
        "    data[3] = node.dist_potential_conflict\n",
        "    data[4] = node.dist_unusable_switch\n",
        "    data[5] = node.dist_to_next_branch\n",
        "\n",
        "    distance[0] = node.dist_min_to_target\n",
        "\n",
        "    agent_data[0] = node.num_agents_same_direction\n",
        "    agent_data[1] = node.num_agents_opposite_direction\n",
        "    agent_data[2] = node.num_agents_malfunctioning\n",
        "    agent_data[3] = node.speed_min_fractional\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def _split_subtree_into_feature_groups(node, current_tree_depth: int, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    if node == -np.inf:\n",
        "        remaining_depth = max_tree_depth - current_tree_depth\n",
        "        # reference: https://stackoverflow.com/questions/515214/total-number-of-nodes-in-a-tree-data-structure\n",
        "        num_remaining_nodes = int((4 ** (remaining_depth + 1) - 1) / (4 - 1))\n",
        "        return [-np.inf] * num_remaining_nodes * 6, [-np.inf] * num_remaining_nodes, [-np.inf] * num_remaining_nodes * 4\n",
        "\n",
        "    data, distance, agent_data = _split_node_into_feature_groups(node)\n",
        "\n",
        "    if not node.childs:\n",
        "        return data, distance, agent_data\n",
        "\n",
        "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
        "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(node.childs[direction], current_tree_depth + 1, max_tree_depth)\n",
        "        data = np.concatenate((data, sub_data))\n",
        "        distance = np.concatenate((distance, sub_distance))\n",
        "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def split_tree_into_feature_groups(tree, max_tree_depth: int) -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "    \"\"\"\n",
        "    This function splits the tree into three difference arrays of values\n",
        "    \"\"\"\n",
        "    data, distance, agent_data = _split_node_into_feature_groups(tree)\n",
        "\n",
        "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
        "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(tree.childs[direction], 1, max_tree_depth)\n",
        "        data = np.concatenate((data, sub_data))\n",
        "        distance = np.concatenate((distance, sub_distance))\n",
        "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
        "\n",
        "    return data, distance, agent_data\n",
        "\n",
        "\n",
        "def normalize_observation(observation, tree_depth: int, observation_radius=0):\n",
        "    \"\"\"\n",
        "    This function normalizes the observation used by the RL algorithm\n",
        "    \"\"\"\n",
        "    data, distance, agent_data = split_tree_into_feature_groups(observation, tree_depth)\n",
        "\n",
        "    data = norm_obs_clip(data, fixed_radius=observation_radius)\n",
        "    distance = norm_obs_clip(distance, normalize_to_range=True)\n",
        "    agent_data = np.clip(agent_data, -1, 1)\n",
        "    normalized_obs = np.concatenate((np.concatenate((data, distance)), agent_data))\n",
        "    return normalized_obs\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TBhnxgJ_LP0"
      },
      "source": [
        "# https://github.com/ikostrikov/pytorch-ddpg-naf/blob/master/ddpg.py#L11\n",
        "def soft_update(target, source, tau):\n",
        "    \"\"\"\n",
        "    Perform DDPG soft update (move target params toward source based on weight\n",
        "    factor tau)\n",
        "    Inputs:\n",
        "        target (torch.nn.Module): Net to copy parameters to\n",
        "        source (torch.nn.Module): Net whose parameters to copy\n",
        "        tau (float, 0 < x < 1): Weight factor for update\n",
        "    \"\"\"\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "\n",
        "# https://github.com/ikostrikov/pytorch-ddpg-naf/blob/master/ddpg.py#L15\n",
        "def hard_update(target, source):\n",
        "    \"\"\"\n",
        "    Copy network parameters from source to target\n",
        "    Inputs:\n",
        "        target (torch.nn.Module): Net to copy parameters to\n",
        "        source (torch.nn.Module): Net whose parameters to copy\n",
        "    \"\"\"\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(param.data)\n",
        "\n",
        "# https://github.com/seba-1511/dist_tuto.pth/blob/gh-pages/train_dist.py\n",
        "def average_gradients(model):\n",
        "    \"\"\" Gradient averaging. \"\"\"\n",
        "    size = float(dist.get_world_size())\n",
        "    for param in model.parameters():\n",
        "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=0)\n",
        "        param.grad.data /= size\n",
        "\n",
        "# https://github.com/seba-1511/dist_tuto.pth/blob/gh-pages/train_dist.py\n",
        "def init_processes(rank, size, fn, backend='gloo'):\n",
        "    \"\"\" Initialize the distributed environment. \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
        "    fn(rank, size)\n",
        "\n",
        "def onehot_from_logits(logits, eps=0.0, dim=1):\n",
        "    \"\"\"\n",
        "    Given batch of logits, return one-hot sample using epsilon greedy strategy\n",
        "    (based on given epsilon)\n",
        "    \"\"\"\n",
        "    # get best (according to current policy) actions in one-hot form\n",
        "    argmax_acs = (logits == logits.max(dim, keepdim=True)[0]).float()\n",
        "    if eps == 0.0:\n",
        "        return argmax_acs\n",
        "    # get random actions in one-hot form\n",
        "    rand_acs = Variable(torch.eye(logits.shape[1])[[np.random.choice(\n",
        "        range(logits.shape[1]), size=logits.shape[0])]], requires_grad=False)\n",
        "    # chooses between best and random actions using epsilon greedy\n",
        "    return torch.stack([argmax_acs[i] if r > eps else rand_acs[i] for i, r in\n",
        "                        enumerate(torch.rand(logits.shape[0]))])\n",
        "\n",
        "# modified for PyTorch from https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb\n",
        "def sample_gumbel(shape, eps=1e-20, tens_type=torch.FloatTensor):\n",
        "    \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
        "    U = Variable(tens_type(*shape).uniform_(), requires_grad=False)\n",
        "    return -torch.log(-torch.log(U + eps) + eps)\n",
        "\n",
        "# modified for PyTorch from https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb\n",
        "def gumbel_softmax_sample(logits, temperature, dim=1):\n",
        "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
        "    y = logits + sample_gumbel(logits.shape, tens_type=type(logits.data))\n",
        "    return F.softmax(y / temperature, dim=dim)\n",
        "\n",
        "# modified for PyTorch from https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb\n",
        "def gumbel_softmax(logits, temperature=1.0, hard=False, dim=1):\n",
        "    \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
        "    Args:\n",
        "      logits: [batch_size, n_class] unnormalized log-probs\n",
        "      temperature: non-negative scalar\n",
        "      hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
        "    Returns:\n",
        "      [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
        "      If hard=True, then the returned sample will be one-hot, otherwise it will\n",
        "      be a probabilitiy distribution that sums to 1 across classes\n",
        "    \"\"\"\n",
        "    y = gumbel_softmax_sample(logits, temperature, dim=dim)\n",
        "    if hard:\n",
        "        y_hard = onehot_from_logits(y, dim=dim)\n",
        "        y = (y_hard - y).detach() + y\n",
        "    return y\n",
        "\n",
        "def firmmax_sample(logits, temperature, dim=1):\n",
        "    if temperature == 0:\n",
        "        return F.softmax(logits, dim=dim)\n",
        "    y = logits + sample_gumbel(logits.shape, tens_type=type(logits.data)) / temperature\n",
        "    return F.softmax(y, dim=dim)\n",
        "\n",
        "def categorical_sample(probs, use_cuda=False):\n",
        "    int_acs = torch.multinomial(probs, 1)\n",
        "    if use_cuda:\n",
        "        tensor_type = torch.cuda.FloatTensor\n",
        "    else:\n",
        "        tensor_type = torch.FloatTensor\n",
        "    acs = Variable(tensor_type(*probs.shape).fill_(0)).scatter_(1, int_acs, 1)\n",
        "    return int_acs, acs\n",
        "\n",
        "def disable_gradients(module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def enable_gradients(module):\n",
        "    for p in module.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def sep_clip_grad_norm(parameters, max_norm, norm_type=2):\n",
        "    \"\"\"\n",
        "    Clips gradient norms calculated on a per-parameter basis, rather than over\n",
        "    the whole list of parameters as in torch.nn.utils.clip_grad_norm.\n",
        "    Code based on torch.nn.utils.clip_grad_norm\n",
        "    \"\"\"\n",
        "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
        "    max_norm = float(max_norm)\n",
        "    norm_type = float(norm_type)\n",
        "    for p in parameters:\n",
        "        if norm_type == float('inf'):\n",
        "            p_norm = p.grad.data.abs().max()\n",
        "        else:\n",
        "            p_norm = p.grad.data.norm(norm_type)\n",
        "        clip_coef = max_norm / (p_norm + 1e-6)\n",
        "        if clip_coef < 1:\n",
        "            p.grad.data.mul_(clip_coef)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrVVPp9l0V_L"
      },
      "source": [
        "class ReplayMemory(object):\n",
        "    \"\"\"\n",
        "    Replay Memory for multi-agent RL with parallel rollouts\n",
        "    \"\"\"\n",
        "    def __init__(self, max_steps, num_agents, obs_dims, ac_dims):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            max_steps (int): Maximum number of timepoints to store in buffer\n",
        "            num_agents (int): Number of agents in environment\n",
        "            obs_dims (list of ints): number of obervation dimensions for each\n",
        "                                     agent\n",
        "            ac_dims (list of ints): number of action dimensions for each agent\n",
        "        \"\"\"\n",
        "        self.max_steps = max_steps\n",
        "        self.num_agents = num_agents\n",
        "        self.obs_buffs = []\n",
        "        self.ac_buffs = []\n",
        "        self.rew_buffs = []\n",
        "        self.next_obs_buffs = []\n",
        "        self.done_buffs = []\n",
        "        for odim, adim in zip(obs_dims, ac_dims):\n",
        "            self.obs_buffs.append(np.zeros((max_steps, odim), dtype=np.float32))\n",
        "            self.ac_buffs.append(np.zeros((max_steps, adim), dtype=np.float32))\n",
        "            self.rew_buffs.append(np.zeros(max_steps, dtype=np.float32))\n",
        "            self.next_obs_buffs.append(np.zeros((max_steps, odim), dtype=np.float32))\n",
        "            self.done_buffs.append(np.zeros(max_steps, dtype=np.uint8))\n",
        "\n",
        "\n",
        "        self.filled_i = 0  # index of first empty location in buffer (last index when full)\n",
        "        self.curr_i = 0  # current index to write to (ovewrite oldest data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.filled_i\n",
        "\n",
        "    def push(self, observations, actions, rewards, next_observations, dones):\n",
        "        nentries = observations.shape[0]  # handle multiple parallel environments\n",
        "        if self.curr_i + nentries > self.max_steps:\n",
        "            rollover = self.max_steps - self.curr_i # num of indices to roll over\n",
        "            for agent_i in range(self.num_agents):\n",
        "                self.obs_buffs[agent_i] = np.roll(self.obs_buffs[agent_i],\n",
        "                                                  rollover, axis=0)\n",
        "                self.ac_buffs[agent_i] = np.roll(self.ac_buffs[agent_i],\n",
        "                                                 rollover, axis=0)\n",
        "                self.rew_buffs[agent_i] = np.roll(self.rew_buffs[agent_i],\n",
        "                                                  rollover)\n",
        "                self.next_obs_buffs[agent_i] = np.roll(\n",
        "                    self.next_obs_buffs[agent_i], rollover, axis=0)\n",
        "                self.done_buffs[agent_i] = np.roll(self.done_buffs[agent_i],\n",
        "                                                   rollover)\n",
        "            self.curr_i = 0\n",
        "            self.filled_i = self.max_steps\n",
        "        for agent_i in range(self.num_agents):\n",
        "            self.obs_buffs[agent_i][self.curr_i:self.curr_i + nentries] = np.vstack(\n",
        "                observations[:, agent_i])\n",
        "            # actions are already batched by agent, so they are indexed differently\n",
        "            self.ac_buffs[agent_i][self.curr_i:self.curr_i + nentries] = actions[agent_i]\n",
        "            self.rew_buffs[agent_i][self.curr_i:self.curr_i + nentries] = rewards[:, agent_i]\n",
        "            self.next_obs_buffs[agent_i][self.curr_i:self.curr_i + nentries] = np.vstack(\n",
        "                next_observations[:, agent_i])\n",
        "            self.done_buffs[agent_i][self.curr_i:self.curr_i + nentries] = dones[:, agent_i]\n",
        "        self.curr_i += nentries\n",
        "        if self.filled_i < self.max_steps:\n",
        "            self.filled_i += nentries\n",
        "        if self.curr_i == self.max_steps:\n",
        "            self.curr_i = 0\n",
        "\n",
        "    def sample(self, N, to_gpu=False, norm_rews=True):\n",
        "        inds = np.random.choice(np.arange(self.filled_i), size=N,\n",
        "                                replace=True)\n",
        "        if to_gpu:\n",
        "            cast = lambda x: Variable(Tensor(x), requires_grad=False).cuda()\n",
        "        else:\n",
        "            cast = lambda x: Variable(Tensor(x), requires_grad=False)\n",
        "        if norm_rews:\n",
        "            ret_rews = [cast((self.rew_buffs[i][inds] -\n",
        "                              self.rew_buffs[i][:self.filled_i].mean()) /\n",
        "                             self.rew_buffs[i][:self.filled_i].std())\n",
        "                        for i in range(self.num_agents)]\n",
        "        else:\n",
        "            ret_rews = [cast(self.rew_buffs[i][inds]) for i in range(self.num_agents)]\n",
        "        return ([cast(self.obs_buffs[i][inds]) for i in range(self.num_agents)],\n",
        "                [cast(self.ac_buffs[i][inds]) for i in range(self.num_agents)],\n",
        "                ret_rews,\n",
        "                [cast(self.next_obs_buffs[i][inds]) for i in range(self.num_agents)],\n",
        "                [cast(self.done_buffs[i][inds]) for i in range(self.num_agents)])\n",
        "\n",
        "    def get_average_rewards(self, N):\n",
        "        if self.filled_i == self.max_steps:\n",
        "            inds = np.arange(self.curr_i - N, self.curr_i)  # allow for negative indexing\n",
        "        else:\n",
        "            inds = np.arange(max(0, self.curr_i - N), self.curr_i)\n",
        "        return [self.rew_buffs[i][inds].mean() for i in range(self.num_agents)]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFsMmfoyyZ5R"
      },
      "source": [
        "class BasePolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    Base policy network\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, out_dim, hidden_dim=64, nonlin=F.leaky_relu,\n",
        "                 norm_in=True, onehot_dim=0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim (int): Number of dimensions in input\n",
        "            out_dim (int): Number of dimensions in output\n",
        "            hidden_dim (int): Number of hidden dimensions\n",
        "            nonlin (PyTorch function): Nonlinearity to apply to hidden layers\n",
        "        \"\"\"\n",
        "        super(BasePolicy, self).__init__()\n",
        "\n",
        "        if norm_in:  # normalize inputs\n",
        "            self.in_fn = nn.BatchNorm1d(input_dim, affine=False)\n",
        "        else:\n",
        "            self.in_fn = lambda x: x\n",
        "        self.fc1 = nn.Linear(input_dim + onehot_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, out_dim)\n",
        "        self.nonlin = nonlin\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            X (PyTorch Matrix): Batch of observations (optionally a tuple that\n",
        "                                additionally includes a onehot label)\n",
        "        Outputs:\n",
        "            out (PyTorch Matrix): Actions\n",
        "        \"\"\"\n",
        "        onehot = None\n",
        "        if type(X) is tuple:\n",
        "            X, onehot = X\n",
        "        inp = self.in_fn(X)  # don't batchnorm onehot\n",
        "        if onehot is not None:\n",
        "            inp = torch.cat((onehot, inp), dim=1)\n",
        "        h1 = self.nonlin(self.fc1(inp))\n",
        "        h2 = self.nonlin(self.fc2(h1))\n",
        "        out = self.fc3(h2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DiscretePolicy(BasePolicy):\n",
        "    \"\"\"\n",
        "    Policy Network for discrete action spaces\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(DiscretePolicy, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, obs, sample=True, return_all_probs=False,\n",
        "                return_log_pi=False, regularize=False,\n",
        "                return_entropy=False):\n",
        "        out = super(DiscretePolicy, self).forward(obs)\n",
        "        probs = F.softmax(out, dim=1)\n",
        "        on_gpu = next(self.parameters()).is_cuda\n",
        "        if sample:\n",
        "            int_act, act = categorical_sample(probs, use_cuda=on_gpu)\n",
        "        else:\n",
        "            act = onehot_from_logits(probs)\n",
        "        rets = [act]\n",
        "        if return_log_pi or return_entropy:\n",
        "            log_probs = F.log_softmax(out, dim=1)\n",
        "        if return_all_probs:\n",
        "            rets.append(probs)\n",
        "        if return_log_pi:\n",
        "            # return log probability of selected action\n",
        "            rets.append(log_probs.gather(1, int_act))\n",
        "        if regularize:\n",
        "            rets.append([(out**2).mean()])\n",
        "        if return_entropy:\n",
        "            rets.append(-(log_probs * probs).sum(1).mean())\n",
        "        if len(rets) == 1:\n",
        "            return rets[0]\n",
        "        return rets\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2JXEjjEyvpf"
      },
      "source": [
        "class AttentionCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention network, used as critic for all agents. Each agent gets its own\n",
        "    observation and action, and can also attend over the other agents' encoded\n",
        "    observations and actions.\n",
        "    \"\"\"\n",
        "    def __init__(self, sa_sizes, hidden_dim=32, norm_in=True, attend_heads=1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            sa_sizes (list of (int, int)): Size of state and action spaces per\n",
        "                                          agent\n",
        "            hidden_dim (int): Number of hidden dimensions\n",
        "            norm_in (bool): Whether to apply BatchNorm to input\n",
        "            attend_heads (int): Number of attention heads to use (use a number\n",
        "                                that hidden_dim is divisible by)\n",
        "        \"\"\"\n",
        "        super(AttentionCritic, self).__init__()\n",
        "        assert (hidden_dim % attend_heads) == 0\n",
        "        self.sa_sizes = sa_sizes\n",
        "        self.nagents = len(sa_sizes)\n",
        "        self.attend_heads = attend_heads\n",
        "\n",
        "        self.critic_encoders = nn.ModuleList()\n",
        "        self.critics = nn.ModuleList()\n",
        "\n",
        "        self.state_encoders = nn.ModuleList()\n",
        "        # iterate over agents\n",
        "        for sdim, adim in sa_sizes:\n",
        "            idim = sdim + adim\n",
        "            odim = adim\n",
        "            encoder = nn.Sequential()\n",
        "            if norm_in:\n",
        "                encoder.add_module('enc_bn', nn.BatchNorm1d(idim,\n",
        "                                                            affine=False))\n",
        "            encoder.add_module('enc_fc1', nn.Linear(idim, hidden_dim))\n",
        "            encoder.add_module('enc_nl', nn.LeakyReLU())\n",
        "            self.critic_encoders.append(encoder)\n",
        "            critic = nn.Sequential()\n",
        "            critic.add_module('critic_fc1', nn.Linear(2 * hidden_dim,\n",
        "                                                      hidden_dim))\n",
        "            critic.add_module('critic_nl', nn.LeakyReLU())\n",
        "            critic.add_module('critic_fc2', nn.Linear(hidden_dim, odim))\n",
        "            self.critics.append(critic)\n",
        "\n",
        "            state_encoder = nn.Sequential()\n",
        "            if norm_in:\n",
        "                state_encoder.add_module('s_enc_bn', nn.BatchNorm1d(\n",
        "                                            sdim, affine=False))\n",
        "            state_encoder.add_module('s_enc_fc1', nn.Linear(sdim,\n",
        "                                                            hidden_dim))\n",
        "            state_encoder.add_module('s_enc_nl', nn.LeakyReLU())\n",
        "            self.state_encoders.append(state_encoder)\n",
        "\n",
        "        attend_dim = hidden_dim // attend_heads\n",
        "        self.key_extractors = nn.ModuleList()\n",
        "        self.selector_extractors = nn.ModuleList()\n",
        "        self.value_extractors = nn.ModuleList()\n",
        "        for i in range(attend_heads):\n",
        "            self.key_extractors.append(nn.Linear(hidden_dim, attend_dim, bias=False))\n",
        "            self.selector_extractors.append(nn.Linear(hidden_dim, attend_dim, bias=False))\n",
        "            self.value_extractors.append(nn.Sequential(nn.Linear(hidden_dim,\n",
        "                                                                attend_dim),\n",
        "                                                       nn.LeakyReLU()))\n",
        "\n",
        "        self.shared_modules = [self.key_extractors, self.selector_extractors,\n",
        "                               self.value_extractors, self.critic_encoders]\n",
        "\n",
        "    def shared_parameters(self):\n",
        "        \"\"\"\n",
        "        Parameters shared across agents and reward heads\n",
        "        \"\"\"\n",
        "        return chain(*[m.parameters() for m in self.shared_modules])\n",
        "\n",
        "    def scale_shared_grads(self):\n",
        "        \"\"\"\n",
        "        Scale gradients for parameters that are shared since they accumulate\n",
        "        gradients from the critic loss function multiple times\n",
        "        \"\"\"\n",
        "        for p in self.shared_parameters():\n",
        "            p.grad.data.mul_(1. / self.nagents)\n",
        "\n",
        "    def forward(self, inps, agents=None, return_q=True, return_all_q=False,\n",
        "                regularize=False, return_attend=False, logger=None, niter=0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            inps (list of PyTorch Matrices): Inputs to each agents' encoder\n",
        "                                             (batch of obs + ac)\n",
        "            agents (int): indices of agents to return Q for\n",
        "            return_q (bool): return Q-value\n",
        "            return_all_q (bool): return Q-value for all actions\n",
        "            regularize (bool): returns values to add to loss function for\n",
        "                               regularization\n",
        "            return_attend (bool): return attention weights per agent\n",
        "            logger (TensorboardX SummaryWriter): If passed in, important values\n",
        "                                                 are logged\n",
        "        \"\"\"\n",
        "        if agents is None:\n",
        "            agents = range(len(self.critic_encoders))\n",
        "        states = [s for s, a in inps]\n",
        "        actions = [a for s, a in inps]\n",
        "        inps = [torch.cat((s, a), dim=1) for s, a in inps]\n",
        "\n",
        "        # extract state-action encoding for each agent\n",
        "        sa_encodings = [encoder(inp) for encoder, inp in zip(self.critic_encoders, inps)]\n",
        "        # extract state encoding for each agent that we're returning Q for\n",
        "        s_encodings = [self.state_encoders[a_i](states[a_i]) for a_i in agents]\n",
        "        # extract keys for each head for each agent\n",
        "        all_head_keys = [[k_ext(enc) for enc in sa_encodings] for k_ext in self.key_extractors]\n",
        "        # extract sa values for each head for each agent\n",
        "        all_head_values = [[v_ext(enc) for enc in sa_encodings] for v_ext in self.value_extractors]\n",
        "        # extract selectors for each head for each agent that we're returning Q for\n",
        "        all_head_selectors = [[sel_ext(enc) for i, enc in enumerate(s_encodings) if i in agents]\n",
        "                              for sel_ext in self.selector_extractors]\n",
        "\n",
        "        other_all_values = [[] for _ in range(len(agents))]\n",
        "        all_attend_logits = [[] for _ in range(len(agents))]\n",
        "        all_attend_probs = [[] for _ in range(len(agents))]\n",
        "        # calculate attention per head\n",
        "        for curr_head_keys, curr_head_values, curr_head_selectors in zip(\n",
        "                all_head_keys, all_head_values, all_head_selectors):\n",
        "            # iterate over agents\n",
        "            for i, a_i, selector in zip(range(len(agents)), agents, curr_head_selectors):\n",
        "                keys = [k for j, k in enumerate(curr_head_keys) if j != a_i]\n",
        "                values = [v for j, v in enumerate(curr_head_values) if j != a_i]\n",
        "                # calculate attention across agents\n",
        "                attend_logits = torch.matmul(selector.view(selector.shape[0], 1, -1),\n",
        "                                             torch.stack(keys).permute(1, 2, 0))\n",
        "                # scale dot-products by size of key (from Attention is All You Need)\n",
        "                scaled_attend_logits = attend_logits / np.sqrt(keys[0].shape[1])\n",
        "                attend_weights = F.softmax(scaled_attend_logits, dim=2)\n",
        "                other_values = (torch.stack(values).permute(1, 2, 0) *\n",
        "                                attend_weights).sum(dim=2)\n",
        "                other_all_values[i].append(other_values)\n",
        "                all_attend_logits[i].append(attend_logits)\n",
        "                all_attend_probs[i].append(attend_weights)\n",
        "        # calculate Q per agent\n",
        "        all_rets = []\n",
        "        for i, a_i in enumerate(agents):\n",
        "            # head_entropies = [(-((probs + 1e-8).log() * probs).squeeze().sum(1)\n",
        "            #                    .mean()) for probs in all_attend_probs[i]]\n",
        "            head_entropies = [(-((probs + 1e-8).log() * probs).sum(1)\n",
        "                               .mean()) for probs in all_attend_probs[i]]\n",
        "            agent_rets = []\n",
        "            critic_in = torch.cat((s_encodings[i], *other_all_values[i]), dim=1)\n",
        "            all_q = self.critics[a_i](critic_in)\n",
        "            int_acs = actions[a_i].max(dim=1, keepdim=True)[1]\n",
        "            q = all_q.gather(1, int_acs)\n",
        "            if return_q:\n",
        "                agent_rets.append(q)\n",
        "            if return_all_q:\n",
        "                agent_rets.append(all_q)\n",
        "            if regularize:\n",
        "                # regularize magnitude of attention logits\n",
        "                attend_mag_reg = 1e-3 * sum((logit**2).mean() for logit in\n",
        "                                            all_attend_logits[i])\n",
        "                regs = (attend_mag_reg,)\n",
        "                agent_rets.append(regs)\n",
        "            if return_attend:\n",
        "                agent_rets.append(np.array(all_attend_probs[i]))\n",
        "            if logger is not None:\n",
        "                logger.add_scalars('agent%i/attention' % a_i,\n",
        "                                   dict(('head%i_entropy' % h_i, ent) for h_i, ent\n",
        "                                        in enumerate(head_entropies)),\n",
        "                                   niter)\n",
        "            if len(agent_rets) == 1:\n",
        "                all_rets.append(agent_rets[0])\n",
        "            else:\n",
        "                all_rets.append(agent_rets)\n",
        "        if len(all_rets) == 1:\n",
        "            return all_rets[0]\n",
        "        else:\n",
        "            return all_rets\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5comYNWczTj7"
      },
      "source": [
        "class AttentionAgent(object):\n",
        "    \"\"\"\n",
        "    General class for Attention agents (policy, target policy)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_in_pol, num_out_pol, hidden_dim=64,\n",
        "                 lr=0.01, onehot_dim=0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            num_in_pol (int): number of dimensions for policy input\n",
        "            num_out_pol (int): number of dimensions for policy output\n",
        "        \"\"\"\n",
        "        self.policy = DiscretePolicy(num_in_pol, num_out_pol,\n",
        "                                     hidden_dim=hidden_dim,\n",
        "                                     onehot_dim=onehot_dim)\n",
        "        self.target_policy = DiscretePolicy(num_in_pol,\n",
        "                                            num_out_pol,\n",
        "                                            hidden_dim=hidden_dim,\n",
        "                                            onehot_dim=onehot_dim)\n",
        "\n",
        "        hard_update(self.target_policy, self.policy)\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "    def step(self, obs, explore=False):\n",
        "        \"\"\"\n",
        "        Take a step forward in environment for a minibatch of observations\n",
        "        Inputs:\n",
        "            obs (PyTorch Variable): Observations for this agent\n",
        "            explore (boolean): Whether or not to sample\n",
        "        Outputs:\n",
        "            action (PyTorch Variable): Actions for this agent\n",
        "        \"\"\"\n",
        "        return self.policy(obs, sample=explore)\n",
        "\n",
        "    def get_params(self):\n",
        "        return {'policy': self.policy.state_dict(),\n",
        "                'target_policy': self.target_policy.state_dict(),\n",
        "                'policy_optimizer': self.policy_optimizer.state_dict()}\n",
        "\n",
        "    def load_params(self, params):\n",
        "        self.policy.load_state_dict(params['policy'])\n",
        "        self.target_policy.load_state_dict(params['target_policy'])\n",
        "        self.policy_optimizer.load_state_dict(params['policy_optimizer'])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e9ebvu7_L_m"
      },
      "source": [
        "class Policy:\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        raise NotImplementedError\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_2DX5m7A0WB"
      },
      "source": [
        "class AttentionSACPolicy(Policy):\n",
        "    def __init__(self, n_agents, state_size, action_size, parameters):\n",
        "        self.n_agents = n_agents\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        sa_sizes = [(state_size, action_size)] * n_agents\n",
        "\n",
        "        self.hidsize = parameters.hidden_size\n",
        "        self.buffer_size = parameters.buffer_size\n",
        "        self.batch_size = parameters.batch_size\n",
        "        self.update_every = parameters.update_every\n",
        "        self.learning_rate = parameters.learning_rate\n",
        "        self.tau = parameters.tau\n",
        "        self.gamma = parameters.gamma\n",
        "        self.buffer_min_size = parameters.buffer_min_size\n",
        "        self.use_gpu = parameters.use_gpu\n",
        "\n",
        "        self.t_step = 0\n",
        "        self.pol_dev = 'cpu'\n",
        "        self.critic_dev = 'cpu'\n",
        "        self.trgt_pol_dev = 'cpu'\n",
        "        self.trgt_critic_dev = 'cpu'\n",
        "\n",
        "        self.q_lr = 0.001\n",
        "        self.niter = 0\n",
        "        self.reward_scale = 10.\n",
        "\n",
        "        if parameters.use_gpu and torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda:0\")\n",
        "            print(\"🐇 Using GPU\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "            print(\"🐢 Using CPU\")\n",
        "\n",
        "\n",
        "        self.memory = MultiAgentReplayBuffer(self.action_size, self.buffer_size, self.batch_size, self.device, self.n_agents)\n",
        "\n",
        "        self.agents = [AttentionAgent(\n",
        "                           num_in_pol = self.state_size,\n",
        "                           num_out_pol = self.action_size,\n",
        "                           hidden_dim = 256,\n",
        "                           lr = 0.001\n",
        "                        ) for _ in range(self.n_agents)]\n",
        "        self.critic = AttentionCritic(\n",
        "            sa_sizes, \n",
        "            hidden_dim = 128, \n",
        "            norm_in = True, \n",
        "            attend_heads = 8\n",
        "        )\n",
        "        self.target_critic = AttentionCritic(\n",
        "            sa_sizes, \n",
        "            hidden_dim = 128, \n",
        "            norm_in = True, \n",
        "            attend_heads = 8\n",
        "        )\n",
        "        hard_update(self.target_critic, self.critic)\n",
        "        self.critic_optimizer = optim.Adam(\n",
        "            self.critic.parameters(), \n",
        "            lr = self.q_lr,\n",
        "            weight_decay = 1e-3\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def policies(self):\n",
        "        return [a.policy for a in self.agents]\n",
        "\n",
        "    @property\n",
        "    def target_policies(self):\n",
        "        return [a.target_policy for a in self.agents]\n",
        "\n",
        "    def act(self, state, agent_id):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "        self.agents[agent_id].policy.eval()\n",
        "        action_values = self.agents[agent_id].step(state, explore = True)\n",
        "        self.agents[agent_id].policy.train()\n",
        "        return np.argmax(action_values.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > self.buffer_min_size and len(self.memory) > self.batch_size:\n",
        "                self._learn()\n",
        "\n",
        "    def _learn(self):\n",
        "        self.prep_training('gpu' if self.use_gpu else 'cpu')\n",
        "\n",
        "        # sample = self.replay_buffer.sample(config.batch_size, to_gpu = config.use_gpu)\n",
        "        sample = self.memory.sample()\n",
        "\n",
        "        self.update_critic(sample)\n",
        "        self.update_policies(sample)\n",
        "        self.update_all_targets()\n",
        "        self.prep_rollouts(device='cpu')\n",
        "\n",
        "    \n",
        "    def update_critic(self, sample, soft=True, logger=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Update central critic for all agents\n",
        "        \"\"\"\n",
        "        obs, acs, rews, next_obs, dones = sample\n",
        "        # Q loss\n",
        "        next_acs = []\n",
        "        next_log_pis = []\n",
        "        for pi, ob in zip(self.target_policies, next_obs):\n",
        "            curr_next_ac, curr_next_log_pi = pi(ob, return_log_pi=True)\n",
        "            next_acs.append(curr_next_ac)\n",
        "            next_log_pis.append(curr_next_log_pi)\n",
        "        trgt_critic_in = list(zip(next_obs, next_acs))\n",
        "        critic_in = list(zip(obs, acs))\n",
        "        next_qs = self.target_critic(trgt_critic_in)\n",
        "        critic_rets = self.critic(critic_in, regularize=True,\n",
        "                                  logger=logger, niter=self.niter)\n",
        "        q_loss = 0\n",
        "        for a_i, nq, log_pi, (pq, regs) in zip(range(self.n_agents), next_qs,\n",
        "                                               next_log_pis, critic_rets):\n",
        "            target_q = (rews[a_i].view(-1, 1) +\n",
        "                        self.gamma * nq *\n",
        "                        (1 - dones[a_i].view(-1, 1)))\n",
        "            if soft:\n",
        "                target_q -= log_pi / self.reward_scale\n",
        "            q_loss += MSELoss(pq, target_q.detach())\n",
        "            for reg in regs:\n",
        "                q_loss += reg  # regularizing attention\n",
        "        q_loss.backward()\n",
        "        self.critic.scale_shared_grads()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm(\n",
        "            self.critic.parameters(), 10 * self.n_agents)\n",
        "        self.critic_optimizer.step()\n",
        "        self.critic_optimizer.zero_grad()\n",
        "\n",
        "        if logger is not None:\n",
        "            logger.add_scalar('losses/q_loss', q_loss, self.niter)\n",
        "            logger.add_scalar('grad_norms/q', grad_norm, self.niter)\n",
        "        self.niter += 1\n",
        "\n",
        "    def update_policies(self, sample, soft=True, logger=None, **kwargs):\n",
        "        obs, acs, rews, next_obs, dones = sample\n",
        "        samp_acs = []\n",
        "        all_probs = []\n",
        "        all_log_pis = []\n",
        "        all_pol_regs = []\n",
        "\n",
        "        for a_i, pi, ob in zip(range(self.n_agents), self.policies, obs):\n",
        "            curr_ac, probs, log_pi, pol_regs, ent = pi(\n",
        "                ob, return_all_probs=True, return_log_pi=True,\n",
        "                regularize=True, return_entropy=True)\n",
        "            # logger.add_scalar('agent%i/policy_entropy' % a_i, ent,\n",
        "            #                   self.niter)\n",
        "            samp_acs.append(curr_ac)\n",
        "            all_probs.append(probs)\n",
        "            all_log_pis.append(log_pi)\n",
        "            all_pol_regs.append(pol_regs)\n",
        "\n",
        "        critic_in = list(zip(obs, samp_acs))\n",
        "        critic_rets = self.critic(critic_in, return_all_q=True)\n",
        "        for a_i, probs, log_pi, pol_regs, (q, all_q) in zip(range(self.n_agents), all_probs,\n",
        "                                                            all_log_pis, all_pol_regs,\n",
        "                                                            critic_rets):\n",
        "            curr_agent = self.agents[a_i]\n",
        "            v = (all_q * probs).sum(dim=1, keepdim=True)\n",
        "            pol_target = q - v\n",
        "            if soft:\n",
        "                pol_loss = (log_pi * (log_pi / self.reward_scale - pol_target).detach()).mean()\n",
        "            else:\n",
        "                pol_loss = (log_pi * (-pol_target).detach()).mean()\n",
        "            for reg in pol_regs:\n",
        "                pol_loss += 1e-3 * reg  # policy regularization\n",
        "            # don't want critic to accumulate gradients from policy loss\n",
        "            disable_gradients(self.critic)\n",
        "            pol_loss.backward()\n",
        "            enable_gradients(self.critic)\n",
        "\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm(\n",
        "                curr_agent.policy.parameters(), 0.5)\n",
        "            curr_agent.policy_optimizer.step()\n",
        "            curr_agent.policy_optimizer.zero_grad()\n",
        "\n",
        "            # if logger is not None:\n",
        "            #     logger.add_scalar('agent%i/losses/pol_loss' % a_i,\n",
        "            #                       pol_loss, self.niter)\n",
        "            #     logger.add_scalar('agent%i/grad_norms/pi' % a_i,\n",
        "            #                       grad_norm, self.niter)\n",
        "\n",
        "    def prep_training(self, device='gpu'):\n",
        "        self.critic.train()\n",
        "        self.target_critic.train()\n",
        "        for a in self.agents:\n",
        "            a.policy.train()\n",
        "            a.target_policy.train()\n",
        "        if device == 'gpu':\n",
        "            fn = lambda x: x.cuda()\n",
        "        else:\n",
        "            fn = lambda x: x.cpu()\n",
        "        if not self.pol_dev == device:\n",
        "            for a in self.agents:\n",
        "                a.policy = fn(a.policy)\n",
        "            self.pol_dev = device\n",
        "        if not self.critic_dev == device:\n",
        "            self.critic = fn(self.critic)\n",
        "            self.critic_dev = device\n",
        "        if not self.trgt_pol_dev == device:\n",
        "            for a in self.agents:\n",
        "                a.target_policy = fn(a.target_policy)\n",
        "            self.trgt_pol_dev = device\n",
        "        if not self.trgt_critic_dev == device:\n",
        "            self.target_critic = fn(self.target_critic)\n",
        "            self.trgt_critic_dev = device\n",
        "\n",
        "    def prep_rollouts(self, device='cpu'):\n",
        "        for a in self.agents:\n",
        "            a.policy.eval()\n",
        "        if device == 'gpu':\n",
        "            fn = lambda x: x.cuda()\n",
        "        else:\n",
        "            fn = lambda x: x.cpu()\n",
        "        # only need main policy for rollouts\n",
        "        if not self.pol_dev == device:\n",
        "            for a in self.agents:\n",
        "                a.policy = fn(a.policy)\n",
        "            self.pol_dev = device\n",
        "\n",
        "    def update_all_targets(self):\n",
        "        soft_update(self.target_critic, self.critic, self.tau)\n",
        "        for a in self.agents:\n",
        "            soft_update(a.target_policy, a.policy, self.tau)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZXdt-HN_MO1"
      },
      "source": [
        "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, device):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = Experience(np.expand_dims(state, 0), action, reward, np.expand_dims(next_state, 0), done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(self.__v_stack_impr([e.state for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        actions = torch.from_numpy(self.__v_stack_impr([e.action for e in experiences if e is not None])) \\\n",
        "            .long().to(self.device)\n",
        "        rewards = torch.from_numpy(self.__v_stack_impr([e.reward for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        next_states = torch.from_numpy(self.__v_stack_impr([e.next_state for e in experiences if e is not None])) \\\n",
        "            .float().to(self.device)\n",
        "        dones = torch.from_numpy(self.__v_stack_impr([e.done for e in experiences if e is not None]).astype(np.uint8)) \\\n",
        "            .float().to(self.device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "    def __v_stack_impr(self, states):\n",
        "        sub_dim = len(states[0][0]) if isinstance(states[0], Iterable) else 1\n",
        "        np_states = np.reshape(np.array(states), (len(states), sub_dim))\n",
        "        return np_states\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSVxleuMz4zO"
      },
      "source": [
        "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "\n",
        "class MultiAgentReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, device, n_agents):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.n_agents = n_agents\n",
        "\n",
        "    def add(self, states, actions, rewards, next_states, dones):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = Experience(\n",
        "            [np.expand_dims(state, 0) for state in states], \n",
        "            [np.expand_dims(action, 0) for action in actions], \n",
        "            [reward for reward in rewards], \n",
        "            [np.expand_dims(next_state, 0) for next_state in next_states], \n",
        "            [done for done in dones]\n",
        "        )\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        next_states = []\n",
        "        dones = []\n",
        "\n",
        "        for agent in range(self.n_agents):\n",
        "            states.append(torch.from_numpy(self.__v_stack_impr([e.state[agent] for e in experiences if e is not None])) \\\n",
        "                .float().to(self.device))\n",
        "            actions.append(torch.from_numpy(self.__v_stack_impr([e.action[agent] for e in experiences if e is not None])) \\\n",
        "                .long().to(self.device))\n",
        "            rewards.append(torch.from_numpy(self.__v_stack_impr([e.reward[agent] for e in experiences if e is not None])) \\\n",
        "                .float().to(self.device))\n",
        "            next_states.append(torch.from_numpy(self.__v_stack_impr([e.next_state[agent] for e in experiences if e is not None])) \\\n",
        "                .float().to(self.device))\n",
        "            dones.append(torch.from_numpy(self.__v_stack_impr([e.done[agent] for e in experiences if e is not None]).astype(np.uint8)) \\\n",
        "                .float().to(self.device))\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n",
        "\n",
        "    def __v_stack_impr(self, states):\n",
        "        sub_dim = len(states[0][0]) if isinstance(states[0], Iterable) else 1\n",
        "        np_states = np.reshape(np.array(states), (len(states), sub_dim))\n",
        "        return np_states\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK4XEHFs_MRm"
      },
      "source": [
        "from timeit import default_timer\n",
        "\n",
        "\n",
        "class Timer(object):\n",
        "    \"\"\"\n",
        "    Utility to measure times.\n",
        "\n",
        "    TODO:\n",
        "    - add \"lap\" method to make it easier to measure average time (+std) when measuring the same thing multiple times.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.total_time = 0.0\n",
        "        self.start_time = 0.0\n",
        "        self.end_time = 0.0\n",
        "\n",
        "    def start(self):\n",
        "        self.start_time = default_timer()\n",
        "\n",
        "    def end(self):\n",
        "        self.total_time += default_timer() - self.start_time\n",
        "\n",
        "    def get(self):\n",
        "        return self.total_time\n",
        "\n",
        "    def get_current(self):\n",
        "        return default_timer() - self.start_time\n",
        "\n",
        "    def reset(self):\n",
        "        self.__init__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.get()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2G0J5z_MUO"
      },
      "source": [
        "\n",
        "def create_rail_env(env_params, tree_observation):\n",
        "    n_agents = env_params.n_agents\n",
        "    x_dim = env_params.x_dim\n",
        "    y_dim = env_params.y_dim\n",
        "    n_cities = env_params.n_cities\n",
        "    max_rails_between_cities = env_params.max_rails_between_cities\n",
        "    max_rails_in_city = env_params.max_rails_in_city\n",
        "    seed = env_params.seed\n",
        "\n",
        "    # Break agents from time to time\n",
        "    malfunction_parameters = MalfunctionParameters(\n",
        "        malfunction_rate=env_params.malfunction_rate,\n",
        "        min_duration=20,\n",
        "        max_duration=50\n",
        "    )\n",
        "\n",
        "    return RailEnv(\n",
        "        width=x_dim, height=y_dim,\n",
        "        # rail_generator=sparse_rail_generator(\n",
        "        #     max_num_cities=n_cities,\n",
        "        #     grid_mode=False,\n",
        "        #     max_rails_between_cities=max_rails_between_cities,\n",
        "        #     max_rails_in_city=max_rails_in_city\n",
        "        # ),\n",
        "        # schedule_generator=sparse_schedule_generator(),\n",
        "        rail_generator = complex_rail_generator(\n",
        "            nr_start_goal=10,\n",
        "            nr_extra=10,\n",
        "            min_dist=10,\n",
        "            max_dist=99999,\n",
        "            seed=1\n",
        "        ),\n",
        "        number_of_agents=n_agents,\n",
        "        malfunction_generator_and_process_data=malfunction_from_params(malfunction_parameters),\n",
        "        obs_builder_object=tree_observation,\n",
        "        random_seed=seed\n",
        "    )\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b68V_7NH_MWt"
      },
      "source": [
        "\n",
        "def train_agent(train_params, train_env_params, eval_env_params, obs_params):\n",
        "    # Environment parameters\n",
        "    n_agents = train_env_params.n_agents\n",
        "    x_dim = train_env_params.x_dim\n",
        "    y_dim = train_env_params.y_dim\n",
        "    n_cities = train_env_params.n_cities\n",
        "    max_rails_between_cities = train_env_params.max_rails_between_cities\n",
        "    max_rails_in_city = train_env_params.max_rails_in_city\n",
        "    seed = train_env_params.seed\n",
        "\n",
        "    # Unique ID for this training\n",
        "    now = datetime.datetime.now()\n",
        "    training_id = now.strftime('%y%m%d%H%M%S')\n",
        "\n",
        "    # Observation parameters\n",
        "    observation_tree_depth = obs_params.observation_tree_depth\n",
        "    observation_radius = obs_params.observation_radius\n",
        "    observation_max_path_depth = obs_params.observation_max_path_depth\n",
        "\n",
        "    # Training parameters\n",
        "    eps_start = train_params.eps_start\n",
        "    eps_end = train_params.eps_end\n",
        "    eps_decay = train_params.eps_decay\n",
        "    n_episodes = train_params.n_episodes\n",
        "    checkpoint_interval = train_params.checkpoint_interval\n",
        "    n_eval_episodes = train_params.n_evaluation_episodes\n",
        "    restore_replay_buffer = train_params.restore_replay_buffer\n",
        "    save_replay_buffer = train_params.save_replay_buffer\n",
        "\n",
        "    # Set the seeds\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Observation builder\n",
        "    predictor = ShortestPathPredictorForRailEnv(observation_max_path_depth)\n",
        "    tree_observation = TreeObsForRailEnv(max_depth=observation_tree_depth, predictor=predictor)\n",
        "\n",
        "    # Setup the environments\n",
        "    train_env = create_rail_env(train_env_params, tree_observation)\n",
        "    train_env.reset(regenerate_schedule=True, regenerate_rail=True)\n",
        "    eval_env = create_rail_env(eval_env_params, tree_observation)\n",
        "    eval_env.reset(regenerate_schedule=True, regenerate_rail=True)\n",
        "\n",
        "    # Setup renderer\n",
        "    if train_params.render:\n",
        "        env_renderer = RenderTool(train_env, gl=\"PGL\")\n",
        "\n",
        "    # Calculate the state size given the depth of the tree observation and the number of features\n",
        "    n_features_per_node = train_env.obs_builder.observation_dim\n",
        "    n_nodes = sum([np.power(4, i) for i in range(observation_tree_depth + 1)])\n",
        "    state_size = n_features_per_node * n_nodes\n",
        "\n",
        "    # The action space of flatland is 5 discrete actions\n",
        "    action_size = 5\n",
        "\n",
        "    # Max number of steps per episode\n",
        "    # This is the official formula used during evaluations\n",
        "    # See details in flatland.envs.schedule_generators.sparse_schedule_generator\n",
        "    # max_steps = int(4 * 2 * (env.height + env.width + (n_agents / n_cities)))\n",
        "    max_steps = train_env._max_episode_steps\n",
        "\n",
        "    action_count = [0] * action_size\n",
        "    action_dict = dict()\n",
        "    agent_obs = [None] * n_agents\n",
        "    agent_prev_obs = [None] * n_agents\n",
        "    # agent_prev_action = [2] * n_agents\n",
        "    agent_prev_action = [np.array([0., 0., 1., 0., 0.])] * n_agents\n",
        "    update_values = [False] * n_agents\n",
        "\n",
        "    # Smoothed values used as target for hyperparameter tuning\n",
        "    smoothed_normalized_score = -1.0\n",
        "    smoothed_eval_normalized_score = -1.0\n",
        "    smoothed_completion = 0.0\n",
        "    smoothed_eval_completion = 0.0\n",
        "\n",
        "    policy = AttentionSACPolicy(n_agents, state_size, action_size, train_params)\n",
        "\n",
        "    # Loads existing replay buffer\n",
        "    if restore_replay_buffer:\n",
        "        try:\n",
        "            policy.load_replay_buffer(restore_replay_buffer)\n",
        "            policy.test()\n",
        "        except RuntimeError as e:\n",
        "            print(\"\\n🛑 Could't load replay buffer, were the experiences generated using the same tree depth?\")\n",
        "            print(e)\n",
        "            exit(1)\n",
        "\n",
        "    print(\"\\n💾 Replay buffer status: {}/{} experiences\".format(len(policy.memory.memory), train_params.buffer_size))\n",
        "\n",
        "    hdd = psutil.disk_usage('/')\n",
        "    if save_replay_buffer and (hdd.free / (2 ** 30)) < 500.0:\n",
        "        print(\"⚠️  Careful! Saving replay buffers will quickly consume a lot of disk space. You have {:.2f}gb left.\".format(hdd.free / (2 ** 30)))\n",
        "\n",
        "    # TensorBoard writer\n",
        "    writer = SummaryWriter()\n",
        "    writer.add_hparams(vars(train_params), {})\n",
        "    writer.add_hparams(vars(train_env_params), {})\n",
        "    writer.add_hparams(vars(obs_params), {})\n",
        "\n",
        "    training_timer = Timer()\n",
        "    training_timer.start()\n",
        "\n",
        "    print(\"\\n🚉 Training {} trains on {}x{} grid for {} episodes, evaluating on {} episodes every {} episodes. Training id '{}'.\\n\".format(\n",
        "        train_env.get_num_agents(),\n",
        "        x_dim, y_dim,\n",
        "        n_episodes,\n",
        "        n_eval_episodes,\n",
        "        checkpoint_interval,\n",
        "        training_id\n",
        "    ))\n",
        "\n",
        "    make_dir(CHECKPOINT_DIR)\n",
        "    params_file = os.path.join(CHECKPOINT_DIR, 'params.txt')\n",
        "    write_params_to_file(train_params, train_env_params, obs_params, params_file)\n",
        "\n",
        "    score_list = []\n",
        "    completion_list = []\n",
        "\n",
        "    for episode_idx in range(n_episodes + 1):\n",
        "        step_timer = Timer()\n",
        "        reset_timer = Timer()\n",
        "        learn_timer = Timer()\n",
        "        preproc_timer = Timer()\n",
        "        inference_timer = Timer()\n",
        "\n",
        "        # Reset environment\n",
        "        reset_timer.start()\n",
        "        obs, info = train_env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "        reset_timer.end()\n",
        "\n",
        "        if train_params.render:\n",
        "            env_renderer.set_new_rail()\n",
        "\n",
        "        score = 0\n",
        "        nb_steps = 0\n",
        "        actions_taken = []\n",
        "\n",
        "        # Build initial agent-specific observations\n",
        "        for agent in train_env.get_agent_handles():\n",
        "            if obs[agent]:\n",
        "                agent_obs[agent] = normalize_observation(obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
        "                agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "\n",
        "        # Run episode\n",
        "        for step in range(max_steps - 1):\n",
        "            inference_timer.start()\n",
        "            for agent in train_env.get_agent_handles():\n",
        "                if info['action_required'][agent]:\n",
        "                    update_values[agent] = True\n",
        "                    action = policy.act(agent_obs[agent], agent)\n",
        "\n",
        "                    action_count[action] += 1\n",
        "                    actions_taken.append(action)\n",
        "                else:\n",
        "                    # An action is not required if the train hasn't joined the railway network,\n",
        "                    # if it already reached its target, or if is currently malfunctioning.\n",
        "                    update_values[agent] = False\n",
        "                    action = 0\n",
        "                action_dict.update({agent: action})\n",
        "            inference_timer.end()\n",
        "\n",
        "            # Environment step\n",
        "            step_timer.start()\n",
        "            next_obs, all_rewards, done, info = train_env.step(action_dict)\n",
        "            step_timer.end()\n",
        "\n",
        "            # Render an episode at some interval\n",
        "            if train_params.render and episode_idx % checkpoint_interval == 0:\n",
        "                env_renderer.render_env(\n",
        "                    show=True,\n",
        "                    frames=False,\n",
        "                    show_observations=False,\n",
        "                    show_predictions=False\n",
        "                )\n",
        "\n",
        "            policy.step(agent_prev_obs, agent_prev_action, all_rewards, agent_obs, done)\n",
        "\n",
        "            for agent in train_env.get_agent_handles():\n",
        "                if update_values[agent] or done['__all__']:\n",
        "                    agent_prev_obs[agent] = agent_obs[agent].copy()\n",
        "                    action_id = action_dict[agent]\n",
        "                    agent_prev_action[agent] = np.array([0., 0., 0., 0., 0.])\n",
        "                    agent_prev_action[agent][action_id] = 1\n",
        "\n",
        "                # Preprocess the new observations\n",
        "                if next_obs[agent]:\n",
        "                    preproc_timer.start()\n",
        "                    agent_obs[agent] = normalize_observation(next_obs[agent], observation_tree_depth, observation_radius=observation_radius)\n",
        "                    preproc_timer.end()\n",
        "\n",
        "                score += all_rewards[agent]\n",
        "\n",
        "            nb_steps = step\n",
        "\n",
        "            if done['__all__']:\n",
        "                break\n",
        "\n",
        "        # Epsilon decay\n",
        "        eps_start = max(eps_end, eps_decay * eps_start)\n",
        "\n",
        "        # Collect information about training\n",
        "        tasks_finished = sum(done[idx] for idx in train_env.get_agent_handles())\n",
        "        completion = tasks_finished / max(1, train_env.get_num_agents())\n",
        "        normalized_score = score / (max_steps * train_env.get_num_agents())\n",
        "        action_probs = action_count / np.sum(action_count)\n",
        "        action_count = [1] * action_size\n",
        "\n",
        "        smoothing = 0.99\n",
        "        smoothed_normalized_score = smoothed_normalized_score * smoothing + normalized_score * (1.0 - smoothing)\n",
        "        smoothed_completion = smoothed_completion * smoothing + completion * (1.0 - smoothing)\n",
        "\n",
        "        score_list.append(smoothed_normalized_score)\n",
        "        completion_list.append(smoothed_completion)\n",
        "\n",
        "        # Print logs\n",
        "        if episode_idx % checkpoint_interval == 0:\n",
        "            # torch.save(policy.qnetwork_local, os.path.join(CHECKPOINT_DIR, str(episode_idx) + '.pth'))\n",
        "            for agent_id in range(len(policy.agents)):\n",
        "                torch.save(policy.agents[agent_id].policy, os.path.join(CHECKPOINT_DIR, str(episode_idx) + '_agent' + str(agent_id) + '.pth'))\n",
        "\n",
        "            if save_replay_buffer:\n",
        "                policy.save_replay_buffer('replay_buffers/' + training_id + '-' + str(episode_idx) + '.pkl')\n",
        "\n",
        "            if train_params.render:\n",
        "                env_renderer.close_window()\n",
        "\n",
        "        print(\n",
        "            '\\r🚂 Episode {}'\n",
        "            '\\t 🏆 Score: {:.3f}'\n",
        "            ' Avg: {:.3f}'\n",
        "            '\\t 💯 Done: {:.2f}%'\n",
        "            ' Avg: {:.2f}%'\n",
        "            '\\t 🎲 Epsilon: {:.3f} '\n",
        "            '\\t 🔀 Action Probs: {}'.format(\n",
        "                episode_idx,\n",
        "                normalized_score,\n",
        "                smoothed_normalized_score,\n",
        "                100 * completion,\n",
        "                100 * smoothed_completion,\n",
        "                eps_start,\n",
        "                format_action_prob(action_probs)\n",
        "            ), end=\" \")\n",
        "\n",
        "        # Evaluate policy and log results at some interval\n",
        "        if episode_idx % checkpoint_interval == 0 and n_eval_episodes > 0:\n",
        "            scores, completions, nb_steps_eval = eval_policy(eval_env, policy, train_params, obs_params)\n",
        "\n",
        "            # writer.add_scalar(\"evaluation/scores_min\", np.min(scores), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/scores_max\", np.max(scores), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/scores_mean\", np.mean(scores), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/scores_std\", np.std(scores), episode_idx)\n",
        "            # writer.add_histogram(\"evaluation/scores\", np.array(scores), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/completions_min\", np.min(completions), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/completions_max\", np.max(completions), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/completions_mean\", np.mean(completions), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/completions_std\", np.std(completions), episode_idx)\n",
        "            # writer.add_histogram(\"evaluation/completions\", np.array(completions), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/nb_steps_min\", np.min(nb_steps_eval), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/nb_steps_max\", np.max(nb_steps_eval), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/nb_steps_mean\", np.mean(nb_steps_eval), episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/nb_steps_std\", np.std(nb_steps_eval), episode_idx)\n",
        "            # writer.add_histogram(\"evaluation/nb_steps\", np.array(nb_steps_eval), episode_idx)\n",
        "\n",
        "            smoothing = 0.9\n",
        "            smoothed_eval_normalized_score = smoothed_eval_normalized_score * smoothing + np.mean(scores) * (1.0 - smoothing)\n",
        "            smoothed_eval_completion = smoothed_eval_completion * smoothing + np.mean(completions) * (1.0 - smoothing)\n",
        "            # writer.add_scalar(\"evaluation/smoothed_score\", smoothed_eval_normalized_score, episode_idx)\n",
        "            # writer.add_scalar(\"evaluation/smoothed_completion\", smoothed_eval_completion, episode_idx)\n",
        "\n",
        "        # Save logs to tensorboard\n",
        "        # writer.add_scalar(\"training/score\", normalized_score, episode_idx)\n",
        "        # writer.add_scalar(\"training/smoothed_score\", smoothed_normalized_score, episode_idx)\n",
        "        # writer.add_scalar(\"training/completion\", np.mean(completion), episode_idx)\n",
        "        # writer.add_scalar(\"training/smoothed_completion\", np.mean(smoothed_completion), episode_idx)\n",
        "        # writer.add_scalar(\"training/nb_steps\", nb_steps, episode_idx)\n",
        "        # writer.add_histogram(\"actions/distribution\", np.array(actions_taken), episode_idx)\n",
        "        # writer.add_scalar(\"actions/nothing\", action_probs[RailEnvActions.DO_NOTHING], episode_idx)\n",
        "        # writer.add_scalar(\"actions/left\", action_probs[RailEnvActions.MOVE_LEFT], episode_idx)\n",
        "        # writer.add_scalar(\"actions/forward\", action_probs[RailEnvActions.MOVE_FORWARD], episode_idx)\n",
        "        # writer.add_scalar(\"actions/right\", action_probs[RailEnvActions.MOVE_RIGHT], episode_idx)\n",
        "        # writer.add_scalar(\"actions/stop\", action_probs[RailEnvActions.STOP_MOVING], episode_idx)\n",
        "        # writer.add_scalar(\"training/epsilon\", eps_start, episode_idx)\n",
        "        # writer.add_scalar(\"training/buffer_size\", len(policy.memory), episode_idx)\n",
        "        # writer.add_scalar(\"training/loss\", policy.loss, episode_idx)\n",
        "        # writer.add_scalar(\"timer/reset\", reset_timer.get(), episode_idx)\n",
        "        # writer.add_scalar(\"timer/step\", step_timer.get(), episode_idx)\n",
        "        # writer.add_scalar(\"timer/learn\", learn_timer.get(), episode_idx)\n",
        "        # writer.add_scalar(\"timer/preproc\", preproc_timer.get(), episode_idx)\n",
        "        # writer.add_scalar(\"timer/total\", training_timer.get_current(), episode_idx)\n",
        "\n",
        "    pickle_list(score_list, os.path.join(CHECKPOINT_DIR, 'scores.pkl'))\n",
        "    pickle_list(completion_list, os.path.join(CHECKPOINT_DIR, 'completion.pkl'))\n",
        "\n",
        "    plt.plot(score_list)\n",
        "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'scores.png'))\n",
        "    plt.show()\n",
        "    \n",
        "    plt.plot(completion_list)\n",
        "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'completion.png'))\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFckIZRM_MZe"
      },
      "source": [
        "\n",
        "def format_action_prob(action_probs):\n",
        "    action_probs = np.round(action_probs, 3)\n",
        "    actions = [\"↻\", \"←\", \"↑\", \"→\", \"◼\"]\n",
        "\n",
        "    buffer = \"\"\n",
        "    for action, action_prob in zip(actions, action_probs):\n",
        "        buffer += action + \" \" + \"{:.3f}\".format(action_prob) + \" \"\n",
        "\n",
        "    return buffer\n",
        "\n",
        "\n",
        "def eval_policy(env, policy, train_params, obs_params):\n",
        "    n_eval_episodes = train_params.n_evaluation_episodes\n",
        "    max_steps = env._max_episode_steps\n",
        "    tree_depth = obs_params.observation_tree_depth\n",
        "    observation_radius = obs_params.observation_radius\n",
        "\n",
        "    action_dict = dict()\n",
        "    scores = []\n",
        "    completions = []\n",
        "    nb_steps = []\n",
        "\n",
        "    for episode_idx in range(n_eval_episodes):\n",
        "        agent_obs = [None] * env.get_num_agents()\n",
        "        score = 0.0\n",
        "\n",
        "        obs, info = env.reset(regenerate_rail=True, regenerate_schedule=True)\n",
        "\n",
        "        final_step = 0\n",
        "\n",
        "        for step in range(max_steps - 1):\n",
        "            for agent in env.get_agent_handles():\n",
        "                if obs[agent]:\n",
        "                    agent_obs[agent] = normalize_observation(obs[agent], tree_depth=tree_depth, observation_radius=observation_radius)\n",
        "\n",
        "                action = 0\n",
        "                if info['action_required'][agent]:\n",
        "                    action = policy.act(agent_obs[agent], agent)\n",
        "                action_dict.update({agent: action})\n",
        "\n",
        "            obs, all_rewards, done, info = env.step(action_dict)\n",
        "\n",
        "            for agent in env.get_agent_handles():\n",
        "                score += all_rewards[agent]\n",
        "\n",
        "            final_step = step\n",
        "\n",
        "            if done['__all__']:\n",
        "                break\n",
        "\n",
        "        normalized_score = score / (max_steps * env.get_num_agents())\n",
        "        scores.append(normalized_score)\n",
        "\n",
        "        tasks_finished = sum(done[idx] for idx in env.get_agent_handles())\n",
        "        completion = tasks_finished / max(1, env.get_num_agents())\n",
        "        completions.append(completion)\n",
        "\n",
        "        nb_steps.append(final_step)\n",
        "\n",
        "    print(\"\\t✅ Eval: score {:.3f} done {:.1f}%\".format(np.mean(scores), np.mean(completions) * 100.0))\n",
        "\n",
        "    return scores, completions, nb_steps\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVkPmdmK_1Ae"
      },
      "source": [
        "def make_dir(dir_path):\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "def get_timestamp():\n",
        "    ct = datetime.datetime.now()\n",
        "    return str(ct).split('.')[0].replace(' ', '').replace('-', '').replace(':', '')\n",
        "\n",
        "def pickle_list(l, file_path):\n",
        "    with open(file_path, 'wb') as f:\n",
        "        pickle.dump(l, f)\n",
        "\n",
        "def write_params_to_file(train_params, train_env_params, obs_params, params_file):\n",
        "    with open(params_file, \"w\") as file1:\n",
        "        file1.write(f'n_episodes={train_params.n_episodes}' + '\\n')\n",
        "        file1.write(f'training_env_config={train_params.training_env_config}' + '\\n')\n",
        "        file1.write(f'evaluation_env_config={train_params.evaluation_env_config}' + '\\n')\n",
        "        file1.write(f'n_evaluation_episodes={train_params.n_evaluation_episodes}' + '\\n')\n",
        "        file1.write(f'checkpoint_interval={train_params.checkpoint_interval}' + '\\n')\n",
        "        file1.write(f'eps_start={train_params.eps_start}' + '\\n')\n",
        "        file1.write(f'eps_end={train_params.eps_end}' + '\\n')\n",
        "        file1.write(f'eps_decay={train_params.eps_decay}' + '\\n')\n",
        "        file1.write(f'buffer_size={train_params.buffer_size}' + '\\n')\n",
        "        file1.write(f'buffer_min_size={train_params.buffer_min_size}' + '\\n')\n",
        "        file1.write(f'restore_replay_buffer={train_params.restore_replay_buffer}' + '\\n')\n",
        "        file1.write(f'save_replay_buffer={train_params.save_replay_buffer}' + '\\n')\n",
        "        file1.write(f'batch_size={train_params.batch_size}' + '\\n')\n",
        "        file1.write(f'gamma={train_params.gamma}' + '\\n')\n",
        "        file1.write(f'tau={train_params.tau}' + '\\n')\n",
        "        file1.write(f'learning_rate={train_params.learning_rate}' + '\\n')\n",
        "        file1.write(f'hidden_size={train_params.hidden_size}' + '\\n')\n",
        "        file1.write(f'update_every={train_params.update_every}' + '\\n')\n",
        "        file1.write(f'use_gpu={train_params.use_gpu}' + '\\n')\n",
        "        file1.write(f'num_threads={train_params.num_threads}' + '\\n')\n",
        "        file1.write(f'render={train_params.render}' + '\\n')\n",
        "        file1.write(f'n_agents={train_env_params.n_agents}' + '\\n')\n",
        "        file1.write(f'x_dim={train_env_params.x_dim}' + '\\n')\n",
        "        file1.write(f'y_dim={train_env_params.y_dim}' + '\\n')\n",
        "        file1.write(f'n_cities={train_env_params.n_cities}' + '\\n')\n",
        "        file1.write(f'max_rails_between_cities={train_env_params.max_rails_between_cities}' + '\\n')\n",
        "        file1.write(f'max_rails_in_city={train_env_params.max_rails_in_city}' + '\\n')\n",
        "        file1.write(f'malfunction_rate={train_env_params.malfunction_rate}' + '\\n')\n",
        "        file1.write(f'seed={train_env_params.seed}' + '\\n')\n",
        "        file1.write(f'observation_tree_depth={obs_params.observation_tree_depth}' + '\\n')\n",
        "        file1.write(f'observation_radius={obs_params.observation_radius}' + '\\n')\n",
        "        file1.write(f'observation_max_path_depth={obs_params.observation_max_path_depth}' + '\\n')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXZfkYrU_Mcz",
        "outputId": "466c6fcc-0e93-482f-e13e-00c076715654"
      },
      "source": [
        "MSELoss = torch.nn.MSELoss()\n",
        "\n",
        "# CHECKPOINT_DIR = '/scratch/ns4486/flatland-reinforcement-learning/single-agent/checkpoints'\n",
        "CHECKPOINT_DIR = '.'\n",
        "CHECKPOINT_DIR = os.path.join(CHECKPOINT_DIR, get_timestamp())\n",
        "\n",
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "training_params = Object()\n",
        "training_params.n_episodes = 2500\n",
        "training_params.training_env_config = 0\n",
        "training_params.evaluation_env_config = 0\n",
        "training_params.n_evaluation_episodes = 25\n",
        "training_params.checkpoint_interval = 100\n",
        "training_params.eps_start = 1.0\n",
        "training_params.eps_end = 0.01\n",
        "training_params.eps_decay = 0.99\n",
        "training_params.buffer_size = int(1e5)\n",
        "training_params.buffer_min_size = 0\n",
        "training_params.restore_replay_buffer = \"\"\n",
        "training_params.save_replay_buffer = False\n",
        "training_params.batch_size = 128\n",
        "training_params.gamma = 0.99\n",
        "training_params.tau = 1e-3\n",
        "training_params.learning_rate = 0.5e-4\n",
        "training_params.hidden_size = 128\n",
        "training_params.update_every = 8\n",
        "training_params.use_gpu = False\n",
        "training_params.num_threads = 1\n",
        "training_params.render = False\n",
        "\n",
        "\n",
        "env_params = [\n",
        "    {\n",
        "        # Test_0\n",
        "        \"n_agents\": 2,\n",
        "        \"x_dim\": 25,\n",
        "        \"y_dim\": 25,\n",
        "        \"n_cities\": 2,\n",
        "        \"max_rails_between_cities\": 2,\n",
        "        \"max_rails_in_city\": 3,\n",
        "        \"malfunction_rate\": 1 / 50,\n",
        "        \"seed\": 0\n",
        "    },\n",
        "    {\n",
        "        # Test_1\n",
        "        \"n_agents\": 10,\n",
        "        \"x_dim\": 30,\n",
        "        \"y_dim\": 30,\n",
        "        \"n_cities\": 2,\n",
        "        \"max_rails_between_cities\": 2,\n",
        "        \"max_rails_in_city\": 3,\n",
        "        \"malfunction_rate\": 1 / 100,\n",
        "        \"seed\": 0\n",
        "    },\n",
        "    {\n",
        "        # Test_2\n",
        "        \"n_agents\": 20,\n",
        "        \"x_dim\": 30,\n",
        "        \"y_dim\": 30,\n",
        "        \"n_cities\": 3,\n",
        "        \"max_rails_between_cities\": 2,\n",
        "        \"max_rails_in_city\": 3,\n",
        "        \"malfunction_rate\": 1 / 200,\n",
        "        \"seed\": 0\n",
        "    },\n",
        "]\n",
        "\n",
        "obs_params = {\n",
        "    \"observation_tree_depth\": 2,\n",
        "    \"observation_radius\": 10,\n",
        "    \"observation_max_path_depth\": 30\n",
        "}\n",
        "\n",
        "def check_env_config(id):\n",
        "    if id >= len(env_params) or id < 0:\n",
        "        print(\"\\n🛑 Invalid environment configuration, only Test_0 to Test_{} are supported.\".format(len(env_params) - 1))\n",
        "        exit(1)\n",
        "\n",
        "\n",
        "check_env_config(training_params.training_env_config)\n",
        "check_env_config(training_params.evaluation_env_config)\n",
        "\n",
        "training_env_params = env_params[training_params.training_env_config]\n",
        "evaluation_env_params = env_params[training_params.evaluation_env_config]\n",
        "\n",
        "print(\"\\nTraining parameters:\")\n",
        "pprint(vars(training_params))\n",
        "print(\"\\nTraining environment parameters (Test_{}):\".format(training_params.training_env_config))\n",
        "pprint(training_env_params)\n",
        "print(\"\\nEvaluation environment parameters (Test_{}):\".format(training_params.evaluation_env_config))\n",
        "pprint(evaluation_env_params)\n",
        "print(\"\\nObservation parameters:\")\n",
        "pprint(obs_params)\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(training_params.num_threads)\n",
        "train_agent(training_params, Namespace(**training_env_params), Namespace(**evaluation_env_params), Namespace(**obs_params))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training parameters:\n",
            "{'batch_size': 128,\n",
            " 'buffer_min_size': 0,\n",
            " 'buffer_size': 100000,\n",
            " 'checkpoint_interval': 100,\n",
            " 'eps_decay': 0.99,\n",
            " 'eps_end': 0.01,\n",
            " 'eps_start': 1.0,\n",
            " 'evaluation_env_config': 0,\n",
            " 'gamma': 0.99,\n",
            " 'hidden_size': 128,\n",
            " 'learning_rate': 5e-05,\n",
            " 'n_episodes': 2500,\n",
            " 'n_evaluation_episodes': 25,\n",
            " 'num_threads': 1,\n",
            " 'render': False,\n",
            " 'restore_replay_buffer': '',\n",
            " 'save_replay_buffer': False,\n",
            " 'tau': 0.001,\n",
            " 'training_env_config': 0,\n",
            " 'update_every': 8,\n",
            " 'use_gpu': False}\n",
            "\n",
            "Training environment parameters (Test_0):\n",
            "{'malfunction_rate': 0.02,\n",
            " 'max_rails_between_cities': 2,\n",
            " 'max_rails_in_city': 3,\n",
            " 'n_agents': 2,\n",
            " 'n_cities': 2,\n",
            " 'seed': 0,\n",
            " 'x_dim': 25,\n",
            " 'y_dim': 25}\n",
            "\n",
            "Evaluation environment parameters (Test_0):\n",
            "{'malfunction_rate': 0.02,\n",
            " 'max_rails_between_cities': 2,\n",
            " 'max_rails_in_city': 3,\n",
            " 'n_agents': 2,\n",
            " 'n_cities': 2,\n",
            " 'seed': 0,\n",
            " 'x_dim': 25,\n",
            " 'y_dim': 25}\n",
            "\n",
            "Observation parameters:\n",
            "{'observation_max_path_depth': 30,\n",
            " 'observation_radius': 10,\n",
            " 'observation_tree_depth': 2}\n",
            "DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator\n",
            "DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator\n",
            "🐢 Using CPU\n",
            "\n",
            "💾 Replay buffer status: 0/100000 experiences\n",
            "\n",
            "🚉 Training 2 trains on 25x25 grid for 2500 episodes, evaluating on 25 episodes every 100 episodes. Training id '201209194300'.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:131: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:178: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r🚂 Episode 0\t 🏆 Score: -0.632 Avg: -0.996\t 💯 Done: 50.00% Avg: 0.50%\t 🎲 Epsilon: 0.990 \t 🔀 Action Probs: ↻ 0.229 ← 0.174 ↑ 0.242 → 0.193 ◼ 0.162  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (10, 15) -> (15, 22)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "\t✅ Eval: score -0.765 done 38.0%\n",
            "🚂 Episode 10\t 🏆 Score: -0.372 Avg: -0.960\t 💯 Done: 100.00% Avg: 6.24%\t 🎲 Epsilon: 0.895 \t 🔀 Action Probs: ↻ 0.208 ← 0.208 ↑ 0.176 → 0.187 ◼ 0.221  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (6, 7) -> (15, 5)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 48\t 🏆 Score: -0.541 Avg: -0.871\t 💯 Done: 50.00% Avg: 19.15%\t 🎲 Epsilon: 0.611 \t 🔀 Action Probs: ↻ 0.193 ← 0.203 ↑ 0.217 → 0.203 ◼ 0.184  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (2, 23) -> (18, 1)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 99\t 🏆 Score: -0.378 Avg: -0.803\t 💯 Done: 100.00% Avg: 28.91%\t 🎲 Epsilon: 0.366 \t 🔀 Action Probs: ↻ 0.202 ← 0.187 ↑ 0.213 → 0.199 ◼ 0.198  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (4, 19) -> (17, 19)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 100\t 🏆 Score: -0.606 Avg: -0.801\t 💯 Done: 50.00% Avg: 29.12%\t 🎲 Epsilon: 0.362 \t 🔀 Action Probs: ↻ 0.189 ← 0.189 ↑ 0.199 → 0.204 ◼ 0.219  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (5, 24) -> (4, 16)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (10, 24) -> (11, 17)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "re_generate cnt=3\n",
            "\t✅ Eval: score -0.778 done 30.0%\n",
            "🚂 Episode 125\t 🏆 Score: -0.318 Avg: -0.794\t 💯 Done: 100.00% Avg: 29.40%\t 🎲 Epsilon: 0.282 \t 🔀 Action Probs: ↻ 0.188 ← 0.213 ↑ 0.207 → 0.179 ◼ 0.213  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (19, 2) -> (3, 16)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (17, 0) -> (20, 7)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "re_generate cnt=3\n",
            "🚂 Episode 129\t 🏆 Score: -0.330 Avg: -0.791\t 💯 Done: 100.00% Avg: 30.22%\t 🎲 Epsilon: 0.271 \t 🔀 Action Probs: ↻ 0.193 ← 0.212 ↑ 0.170 → 0.209 ◼ 0.217  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (14, 2) -> (12, 19)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 135\t 🏆 Score: -0.509 Avg: -0.784\t 💯 Done: 100.00% Avg: 30.89%\t 🎲 Epsilon: 0.255 \t 🔀 Action Probs: ↻ 0.190 ← 0.207 ↑ 0.202 → 0.213 ◼ 0.188  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (19, 3) -> (13, 17)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 157\t 🏆 Score: -0.469 Avg: -0.770\t 💯 Done: 100.00% Avg: 33.44%\t 🎲 Epsilon: 0.204 \t 🔀 Action Probs: ↻ 0.203 ← 0.239 ↑ 0.199 → 0.199 ◼ 0.160  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (15, 6) -> (22, 23)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 200\t 🏆 Score: -0.497 Avg: -0.741\t 💯 Done: 100.00% Avg: 37.27%\t 🎲 Epsilon: 0.133 \t 🔀 Action Probs: ↻ 0.221 ← 0.203 ↑ 0.191 → 0.199 ◼ 0.186  \t✅ Eval: score -0.762 done 38.0%\n",
            "🚂 Episode 215\t 🏆 Score: -0.999 Avg: -0.733\t 💯 Done: 0.00% Avg: 38.08%\t 🎲 Epsilon: 0.114 \t 🔀 Action Probs: ↻ 0.151 ← 0.198 ↑ 0.236 → 0.217 ◼ 0.198  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (0, 16) -> (19, 13)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 240\t 🏆 Score: -0.624 Avg: -0.729\t 💯 Done: 50.00% Avg: 38.52%\t 🎲 Epsilon: 0.089 \t 🔀 Action Probs: ↻ 0.189 ← 0.201 ↑ 0.195 → 0.199 ◼ 0.216  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (17, 22) -> (11, 5)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 287\t 🏆 Score: -0.584 Avg: -0.715\t 💯 Done: 50.00% Avg: 39.89%\t 🎲 Epsilon: 0.055 \t 🔀 Action Probs: ↻ 0.205 ← 0.201 ↑ 0.213 → 0.208 ◼ 0.173  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (14, 10) -> (7, 0)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 300\t 🏆 Score: -0.999 Avg: -0.730\t 💯 Done: 0.00% Avg: 38.29%\t 🎲 Epsilon: 0.049 \t 🔀 Action Probs: ↻ 0.204 ← 0.183 ↑ 0.194 → 0.140 ◼ 0.280  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (14, 3) -> (16, 15)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "\t✅ Eval: score -0.684 done 50.0%\n",
            "🚂 Episode 325\t 🏆 Score: -0.538 Avg: -0.741\t 💯 Done: 100.00% Avg: 37.26%\t 🎲 Epsilon: 0.038 \t 🔀 Action Probs: ↻ 0.200 ← 0.207 ↑ 0.193 → 0.216 ◼ 0.184  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (4, 18) -> (14, 7)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 327\t 🏆 Score: -0.570 Avg: -0.742\t 💯 Done: 50.00% Avg: 37.02%\t 🎲 Epsilon: 0.037 \t 🔀 Action Probs: ↻ 0.188 ← 0.193 ↑ 0.215 → 0.209 ◼ 0.196  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (13, 15) -> (10, 20)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 398\t 🏆 Score: -0.999 Avg: -0.748\t 💯 Done: 0.00% Avg: 35.57%\t 🎲 Epsilon: 0.018 \t 🔀 Action Probs: ↻ 0.203 ← 0.241 ↑ 0.182 → 0.211 ◼ 0.163  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (6, 5) -> (15, 17)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 400\t 🏆 Score: -0.092 Avg: -0.739\t 💯 Done: 100.00% Avg: 36.36%\t 🎲 Epsilon: 0.018 \t 🔀 Action Probs: ↻ 0.192 ← 0.242 ↑ 0.198 → 0.176 ◼ 0.192  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (9, 1) -> (14, 5)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (19, 20) -> (13, 1)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (23, 16) -> (8, 7)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "re_generate cnt=3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (18, 12) -> (16, 23)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (18, 3) -> (15, 23)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "\t✅ Eval: score -0.659 done 52.0%\n",
            "🚂 Episode 462\t 🏆 Score: -0.999 Avg: -0.730\t 💯 Done: 0.00% Avg: 37.11%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.212 ← 0.184 ↑ 0.195 → 0.202 ◼ 0.207  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (8, 11) -> (18, 6)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 500\t 🏆 Score: -0.779 Avg: -0.746\t 💯 Done: 50.00% Avg: 35.90%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.204 ← 0.193 ↑ 0.203 → 0.207 ◼ 0.192  \t✅ Eval: score -0.644 done 54.0%\n",
            "🚂 Episode 502\t 🏆 Score: -0.739 Avg: -0.748\t 💯 Done: 50.00% Avg: 35.69%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.188 ← 0.207 ↑ 0.188 → 0.208 ◼ 0.208  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (15, 6) -> (6, 19)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 510\t 🏆 Score: -0.999 Avg: -0.754\t 💯 Done: 0.00% Avg: 34.83%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.172 ← 0.207 ↑ 0.214 → 0.192 ◼ 0.214  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (22, 14) -> (6, 21)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 559\t 🏆 Score: -0.999 Avg: -0.743\t 💯 Done: 0.00% Avg: 35.11%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.188 ← 0.187 ↑ 0.212 → 0.222 ◼ 0.191  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (9, 11) -> (17, 1)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 600\t 🏆 Score: -0.502 Avg: -0.743\t 💯 Done: 50.00% Avg: 35.29%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.181 ← 0.191 ↑ 0.226 → 0.187 ◼ 0.215  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (14, 7) -> (10, 14)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "\t✅ Eval: score -0.809 done 28.0%\n",
            "🚂 Episode 604\t 🏆 Score: -0.999 Avg: -0.747\t 💯 Done: 0.00% Avg: 34.87%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.191 ← 0.207 ↑ 0.183 → 0.207 ◼ 0.213  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (2, 16) -> (18, 24)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 633\t 🏆 Score: -0.999 Avg: -0.704\t 💯 Done: 0.00% Avg: 39.56%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.208 ← 0.167 ↑ 0.212 → 0.283 ◼ 0.129  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (15, 24) -> (6, 13)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 659\t 🏆 Score: -0.999 Avg: -0.715\t 💯 Done: 0.00% Avg: 37.84%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.212 ← 0.201 ↑ 0.185 → 0.218 ◼ 0.184  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (15, 5) -> (4, 11)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 677\t 🏆 Score: -0.176 Avg: -0.705\t 💯 Done: 100.00% Avg: 39.50%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.213 ← 0.183 ↑ 0.240 → 0.169 ◼ 0.195  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (2, 14) -> (19, 17)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 700\t 🏆 Score: -0.227 Avg: -0.695\t 💯 Done: 100.00% Avg: 41.30%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.234 ← 0.175 ↑ 0.193 → 0.198 ◼ 0.200  \t✅ Eval: score -0.636 done 54.0%\n",
            "🚂 Episode 756\t 🏆 Score: -0.999 Avg: -0.712\t 💯 Done: 0.00% Avg: 39.93%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.198 ← 0.161 ↑ 0.211 → 0.215 ◼ 0.215  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 9) -> (21, 9)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 794\t 🏆 Score: -0.999 Avg: -0.722\t 💯 Done: 0.00% Avg: 38.73%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.199 ← 0.192 ↑ 0.152 → 0.227 ◼ 0.230  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (11, 5) -> (21, 10)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 800\t 🏆 Score: -0.999 Avg: -0.725\t 💯 Done: 0.00% Avg: 38.42%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.205 ← 0.204 ↑ 0.189 → 0.198 ◼ 0.205  \t✅ Eval: score -0.782 done 26.0%\n",
            "🚂 Episode 816\t 🏆 Score: -0.830 Avg: -0.720\t 💯 Done: 50.00% Avg: 40.22%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.156 ← 0.188 ↑ 0.224 → 0.227 ◼ 0.205  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (21, 6) -> (12, 16)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 861\t 🏆 Score: -0.588 Avg: -0.697\t 💯 Done: 50.00% Avg: 42.85%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.199 ← 0.203 ↑ 0.202 → 0.195 ◼ 0.201  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (2, 6) -> (5, 17)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 868\t 🏆 Score: -0.617 Avg: -0.700\t 💯 Done: 50.00% Avg: 42.40%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.190 ← 0.202 ↑ 0.201 → 0.218 ◼ 0.189  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (11, 8) -> (15, 24)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 889\t 🏆 Score: -0.999 Avg: -0.704\t 💯 Done: 0.00% Avg: 41.45%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.211 ← 0.195 ↑ 0.223 → 0.170 ◼ 0.202  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (21, 10) -> (14, 5)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 900\t 🏆 Score: -0.999 Avg: -0.716\t 💯 Done: 0.00% Avg: 39.43%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.222 ← 0.194 ↑ 0.189 → 0.200 ◼ 0.194  \t✅ Eval: score -0.794 done 26.0%\n",
            "🚂 Episode 908\t 🏆 Score: -0.999 Avg: -0.717\t 💯 Done: 0.00% Avg: 39.25%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.211 ← 0.194 ↑ 0.205 → 0.181 ◼ 0.209  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (2, 5) -> (8, 4)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 957\t 🏆 Score: -0.556 Avg: -0.716\t 💯 Done: 100.00% Avg: 39.39%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.207 ← 0.210 ↑ 0.204 → 0.189 ◼ 0.189  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (15, 3) -> (18, 23)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (13, 12) -> (20, 22)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "re_generate cnt=3\n",
            "🚂 Episode 971\t 🏆 Score: -0.999 Avg: -0.728\t 💯 Done: 0.00% Avg: 38.30%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.165 ← 0.165 ↑ 0.231 → 0.220 ◼ 0.220  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 10) -> (14, 2)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 978\t 🏆 Score: -0.999 Avg: -0.736\t 💯 Done: 0.00% Avg: 37.61%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.197 ← 0.184 ↑ 0.203 → 0.217 ◼ 0.200  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (13, 3) -> (6, 2)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (10, 6) -> (21, 20)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "re_generate cnt=3\n",
            "🚂 Episode 1000\t 🏆 Score: -0.575 Avg: -0.725\t 💯 Done: 50.00% Avg: 37.83%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.195 ← 0.186 ↑ 0.186 → 0.220 ◼ 0.213  \t✅ Eval: score -0.653 done 52.0%\n",
            "🚂 Episode 1060\t 🏆 Score: -0.862 Avg: -0.733\t 💯 Done: 50.00% Avg: 37.63%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.205 ← 0.207 ↑ 0.218 → 0.175 ◼ 0.195  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (23, 14) -> (6, 14)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (6, 5) -> (23, 15)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "re_generate cnt=3\n",
            "🚂 Episode 1080\t 🏆 Score: -0.474 Avg: -0.732\t 💯 Done: 100.00% Avg: 37.62%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.212 ← 0.192 ↑ 0.198 → 0.206 ◼ 0.192  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (10, 20) -> (23, 17)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (7, 11) -> (14, 9)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (13, 0) -> (7, 10)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "re_generate cnt=3\n",
            "re_generate cnt=4\n",
            "🚂 Episode 1085\t 🏆 Score: -0.552 Avg: -0.720\t 💯 Done: 50.00% Avg: 39.19%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.223 ← 0.198 ↑ 0.176 → 0.194 ◼ 0.209  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (4, 7) -> (8, 3)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1100\t 🏆 Score: -0.240 Avg: -0.719\t 💯 Done: 100.00% Avg: 38.92%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.183 ← 0.190 ↑ 0.190 → 0.232 ◼ 0.205  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (9, 22) -> (5, 10)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "\t✅ Eval: score -0.720 done 36.0%\n",
            "🚂 Episode 1200\t 🏆 Score: -0.409 Avg: -0.714\t 💯 Done: 100.00% Avg: 40.97%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.212 ← 0.208 ↑ 0.212 → 0.212 ◼ 0.157  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (22, 13) -> (15, 16)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "\t✅ Eval: score -0.734 done 36.0%\n",
            "🚂 Episode 1205\t 🏆 Score: -0.261 Avg: -0.718\t 💯 Done: 100.00% Avg: 40.45%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.183 ← 0.213 ↑ 0.217 → 0.181 ◼ 0.207  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (4, 11) -> (21, 21)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1210\t 🏆 Score: -0.506 Avg: -0.720\t 💯 Done: 50.00% Avg: 39.94%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.197 ← 0.219 ↑ 0.182 → 0.209 ◼ 0.193  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (9, 2) -> (15, 4)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1222\t 🏆 Score: -0.999 Avg: -0.727\t 💯 Done: 0.00% Avg: 39.61%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.209 ← 0.191 ↑ 0.207 → 0.195 ◼ 0.198  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (11, 4) -> (17, 14)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (23, 19) -> (22, 10)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "re_generate cnt=3\n",
            "🚂 Episode 1298\t 🏆 Score: -0.999 Avg: -0.726\t 💯 Done: 0.00% Avg: 38.75%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.200 ← 0.205 ↑ 0.193 → 0.200 ◼ 0.201  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (24, 6) -> (11, 11)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1300\t 🏆 Score: -0.999 Avg: -0.723\t 💯 Done: 0.00% Avg: 38.97%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.167 ← 0.212 ↑ 0.136 → 0.242 ◼ 0.242  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (9, 16) -> (6, 20)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "\t✅ Eval: score -0.715 done 40.0%\n",
            "🚂 Episode 1326\t 🏆 Score: -0.999 Avg: -0.713\t 💯 Done: 0.00% Avg: 41.69%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.185 ← 0.238 ↑ 0.172 → 0.202 ◼ 0.202  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (10, 14) -> (17, 17)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1330\t 🏆 Score: -0.523 Avg: -0.712\t 💯 Done: 50.00% Avg: 41.51%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.174 ← 0.207 ↑ 0.229 → 0.180 ◼ 0.210  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (16, 12) -> (0, 15)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1356\t 🏆 Score: -0.389 Avg: -0.722\t 💯 Done: 100.00% Avg: 39.78%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.190 ← 0.201 ↑ 0.199 → 0.206 ◼ 0.205  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (16, 13) -> (5, 4)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1400\t 🏆 Score: -0.728 Avg: -0.733\t 💯 Done: 50.00% Avg: 38.83%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.191 ← 0.195 ↑ 0.211 → 0.211 ◼ 0.192  \t✅ Eval: score -0.754 done 36.0%\n",
            "🚂 Episode 1421\t 🏆 Score: -0.999 Avg: -0.722\t 💯 Done: 0.00% Avg: 40.47%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.202 ← 0.188 ↑ 0.218 → 0.197 ◼ 0.195  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (8, 4) -> (8, 19)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (11, 7) -> (17, 13)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "re_generate cnt=3\n",
            "🚂 Episode 1455\t 🏆 Score: -0.999 Avg: -0.715\t 💯 Done: 0.00% Avg: 40.16%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.187 ← 0.189 ↑ 0.195 → 0.215 ◼ 0.213  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (17, 10) -> (14, 24)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1463\t 🏆 Score: -0.999 Avg: -0.722\t 💯 Done: 0.00% Avg: 38.97%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.189 ← 0.206 ↑ 0.272 → 0.139 ◼ 0.194  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (17, 3) -> (14, 22)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1500\t 🏆 Score: -0.999 Avg: -0.726\t 💯 Done: 0.00% Avg: 39.27%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.193 ← 0.188 ↑ 0.178 → 0.203 ◼ 0.239  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[0]: (6, 21) -> (23, 8)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "\t✅ Eval: score -0.729 done 36.0%\n",
            "🚂 Episode 1504\t 🏆 Score: -0.999 Avg: -0.716\t 💯 Done: 0.00% Avg: 40.18%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.233 ← 0.171 ↑ 0.197 → 0.197 ◼ 0.202  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (9, 10) -> (22, 17)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1519\t 🏆 Score: -0.999 Avg: -0.728\t 💯 Done: 0.00% Avg: 38.21%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.206 ← 0.214 ↑ 0.206 → 0.195 ◼ 0.179  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (16, 5) -> (5, 18)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1600\t 🏆 Score: -0.210 Avg: -0.740\t 💯 Done: 100.00% Avg: 35.34%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.192 ← 0.236 ↑ 0.221 → 0.174 ◼ 0.177  \t✅ Eval: score -0.755 done 40.0%\n",
            "🚂 Episode 1616\t 🏆 Score: -0.999 Avg: -0.731\t 💯 Done: 0.00% Avg: 36.06%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.212 ← 0.204 ↑ 0.191 → 0.188 ◼ 0.204  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/flatland/envs/schedule_generators.py:286: UserWarning: reset position for agent[1]: (22, 21) -> (10, 3)\n",
            "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "re_generate cnt=2\n",
            "🚂 Episode 1644\t 🏆 Score: -0.999 Avg: -0.757\t 💯 Done: 0.00% Avg: 32.96%\t 🎲 Epsilon: 0.010 \t 🔀 Action Probs: ↻ 0.216 ← 0.223 ↑ 0.230 → 0.155 ◼ 0.176  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Pggg8K_Me-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSMgX6io_Mhv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f566eBg_MkR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbv-f5W_Mne"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFrIjCox_Mp9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}